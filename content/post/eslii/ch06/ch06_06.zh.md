+++
title = "ESL-6.6 Kernel Density Estimation and Classification"
summary = """
统计学习基础（译注）第六章第六节，第 208-211 页。
"""

date = 2018-11-21T22:45:00+08:00
lastmod = 2018-11-21T22:45:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Kernel density estimation is an unsupervised learning procedure, which
historically precedes kernel regression. It also leads naturally to a simple
family of procedures for nonparametric classification.

### 6.6.1 Kernel Density Estimation

Suppose we have a random sample x 1 , . . . , x N drawn from a probability
density f X (x), and we wish to estimate f X at a point x 0 . For simplicity we
assume for now that X ∈ IR. Arguing as before, a natural local estimate
has the form

$$\hat{f}\_X(x\_0) = \frac{\\# x\_i \in \mathcal{N}(x\_0)}{N\lambda}\tag{6.21}$$

where N (x 0 ) is a small metric neighborhood around x 0 of width λ. This
estimate is bumpy, and the smooth Parzen estimate is preferred

$$\hat{f}\_X(x\_0) = \frac{1}{N\lambda}
\sum\_{i=1}^N K\_\lambda(x\_0, x\_i)\tag{6.22}$$

because it counts observations close to x 0 with weights that decrease with
distance from x 0 . In this case a popular choice for K λ is the Gaussian kernel
K λ (x 0 , x) = φ(|x − x 0 |/λ). Figure 6.13 shows a Gaussian kernel density fit
to the sample values for systolic blood pressure for the CHD group. Letting
φ λ denote the Gaussian density with mean zero and standard-deviation λ,
then (6.22) has the form

$$\begin{align}
\hat{f}\_X(x) &=
\frac{1}{N} \sum\_{i=1}^N \phi\_\lambda(x-x\_i) \\\\ &=
(\hat{F} \star \phi\_\lambda)(x)
\tag{6.23}\end{align}$$

the convolution of the sample empirical distribution F̂ with φ λ . The dis-
tribution F̂ (x) puts mass 1/N at each of the observed x i , and is jumpy; in
f ˆ X (x) we have smoothed F̂ by adding independent Gaussian noise to each
observation x i .

The Parzen density estimate is the equivalent of the local average, and
improvements have been proposed along the lines of local regression [on the
log scale for densities; see Loader (1999)]. We will not pursue these here.
In IR p the natural generalization of the Gaussian density estimate amounts
to using the Gaussian product kernel in (6.23),

$$\hat{f}\_X(x\_0) = \frac{1}{N(2\lambda^2\pi)^{\frac{p}{2}}}
\sum\_{i=1}^N e^{-\frac{1}{2} (\\|x\_i-x\_0\\|/\lambda)^2}
\tag{6.24}$$

### 6.6.2 Kernel Density Classification

One can use nonparametric density estimates for classification in a straight-
forward fashion using Bayes’ theorem. Suppose for a J class problem we fit
nonparametric density estimates f ˆ j (X), j = 1, . . . , J separately in each of
the classes, and we also have estimates of the class priors π̂ j (usually the
sample proportions). Then

$$\hat{\text{Pr}}(G=j|X=x\_0) = \frac
{\hat{\pi}\_j \hat{f}\_j(x\_0)}
{\sum\_{k=1}^J {\hat{\pi}\_k \hat{f}\_k(x\_0)}}
\tag{6.25}$$

Figure 6.14 uses this method to estimate the prevalence of CHD for the
heart risk factor study, and should be compared with the left panel of Fig-
ure 6.12. The main difference occurs in the region of high SBP in the right
panel of Figure 6.14. In this region the data are sparse for both classes, and
since the Gaussian kernel density estimates use metric kernels, the density
estimates are low and of poor quality (high variance) in these regions. The
local logistic regression method (6.20) uses the tri-cube kernel with k-NN
bandwidth; this effectively widens the kernel in this region, and makes use
of the local linear assumption to smooth out the estimate (on the logit
scale).

If classification is the ultimate goal, then learning the separate class den-
sities well may be unnecessary, and can in fact be misleading. Figure 6.15
shows an example where the densities are both multimodal, but the pos-
terior ratio is quite smooth. In learning the separate densities from data,
one might decide to settle for a rougher, high-variance fit to capture these
features, which are irrelevant for the purposes of estimating the posterior
probabilities. In fact, if classification is the ultimate goal, then we need only
to estimate the posterior well near the decision boundary (for two classes,
this is the set {x|Pr(G = 1|X = x) = 12 }).

### 6.6.3 The Naive Bayes Classifier

This is a technique that has remained popular over the years, despite its
name (also known as “Idiot’s Bayes”!) It is especially appropriate when
the dimension p of the feature space is high, making density estimation
unattractive. The naive Bayes model assumes that given a class G = j, the
features X k are independent:

$$f\_j(X) = \prod\_{k=1}^p f\_{jk} X\_k \tag{6.26}$$

While this assumption is generally not true, it does simplify the estimation
dramatically:

* The individual class-conditional marginal densities f jk can each be
estimated separately using one-dimensional kernel density estimates.
This is in fact a generalization of the original naive Bayes procedures,
which used univariate Gaussians to represent these marginals.
* If a component X j of X is discrete, then an appropriate histogram
estimate can be used. This provides a seamless way of mixing variable
types in a feature vector.

Despite these rather optimistic assumptions, naive Bayes classifiers often
outperform far more sophisticated alternatives. The reasons are related to
Figure 6.15: although the individual class density estimates may be biased,
this bias might not hurt the posterior probabilities as much, especially
near the decision regions. In fact, the problem may be able to withstand
considerable bias for the savings in variance such a “naive” assumption
earns.

Starting from (6.26) we can derive the logit-transform (using class J as
the base):

$$\begin{align}
\log\frac{\text{Pr}(G=l|X)}{\text{Pr}(G=J|X)} &=
\log\frac{\pi\_l f\_l(X)}{\pi\_J f\_J(X)} \\\\ &=
\log\frac{\pi\_l \prod\_{k=1}^p f\_{lk}(X\_k)}
         {\pi\_J \prod\_{k=1}^p f\_{Jk}(X\_k)} \\\\ &=
\log\frac{pi\_l}{\pi\_J} + \sum\_{k=1}^p
  \log\frac{f\_{lk}(X\_k)}{f\_{Jk}(X\_k)} \\\\ &=
\alpha\_l + \sum\_{k=1}^p g\_{lk}X\_k
\end{align}\tag{6.27}$$

This has the form of a generalized additive model, which is described in more
detail in Chapter 9. The models are fit in quite different ways though; their
differences are explored in Exercise 6.9. The relationship between naive
Bayes and generalized additive models is analogous to that between linear
discriminant analysis and logistic regression (Section 4.4.5).