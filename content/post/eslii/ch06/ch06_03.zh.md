+++
title = "ESL-6.3 Local Regression in ℝᵖ"
summary = """
统计学习基础（译注）第六章第三节，第 200-201 页。
"""

date = 2018-11-21T17:00:00+08:00
lastmod = 2018-11-21T17:00:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Kernel smoothing and local regression generalize very naturally to two or
more dimensions. The Nadaraya–Watson kernel smoother fits a constant
locally with weights supplied by a p-dimensional kernel. Local linear re-
gression will fit a hyperplane locally in X, by weighted least squares, with
weights supplied by a p-dimensional kernel. It is simple to implement and
is generally preferred to the local constant fit for its superior performance
on the boundaries.

Let b(X) be a vector of polynomial terms in X of maximum degree d.
For example, with d = 1 and p = 2 we get b(X) = (1,X1,X2); with d = 2
we get b(X) = (1,X1,X2,X21 ,X2 2 ,X1X2); and trivially with d = 0 we get
b(X) = 1. At each x0 ∈ IRp solve

$$\min\_{\beta(x\_0)} \sum\_{i=1}^N
K\_\lambda(x\_0, x\_i)(y\_i-b(x\_i)^T\beta(x\_0))^2
\tag{6.12}$$

to produce the fit ˆ f(x0) = b(x0)T ˆ β(x0). Typically the kernel will be a radial
function, such as the radial Epanechnikov or tri-cube kernel

$$K\_\lambda(x\_0, x) = D \left(
\frac{\\|x-x\_0\\|}{\lambda}
\right)\tag{6.13}$$

where ||·|| is the Euclidean norm. Since the Euclidean norm depends on the
units in each coordinate, it makes most sense to standardize each predictor,
for example, to unit standard deviation, prior to smoothing.

While boundary effects are a problem in one-dimensional smoothing,
they are a much bigger problem in two or higher dimensions, since the
fraction of points on the boundary is larger. In fact, one of the manifesta-
tions of the curse of dimensionality is that the fraction of points close to the
boundary increases to one as the dimension grows. Directly modifying the
kernel to accommodate two-dimensional boundaries becomes very messy,
especially for irregular boundaries. Local polynomial regression seamlessly
performs boundary correction to the desired order in any dimensions. Fig-
ure 6.8 illustrates local linear regression on some measurements from an
astronomical study with an unusual predictor design (star-shaped). Here
the boundary is extremely irregular, and the fitted surface must also inter-
polate over regions of increasing data sparsity as we approach the boundary.

Local regression becomes less useful in dimensions much higher than two
or three. We have discussed in some detail the problems of dimensional-
ity, for example, in Chapter 2. It is impossible to simultaneously main-
tain localness (⇒ low bias) and a sizable sample in the neighborhood (⇒ low variance) as the dimension increases, without the total sample size in-
creasing exponentially in p. Visualization of ˆ f(X) also becomes difficult in
higher dimensions, and this is often one of the primary goals of smoothing.
Although the scatter-cloud and wire-frame pictures in Figure 6.8 look at-
tractive, it is quite difficult to interpret the results except at a gross level.
From a data analysis perspective, conditional plots are far more useful.

Figure 6.9 shows an analysis of some environmental data with three pre-
dictors. The trellis display here shows ozone as a function of radiation,
conditioned on the other two variables, temperature and wind speed. How-
ever, conditioning on the value of a variable really implies local to that
value (as in local regression). Above each of the panels in Figure 6.9 is an
indication of the range of values present in that panel for each of the condi-
tioning values. In the panel itself the data subsets are displayed (response
versus remaining variable), and a one-dimensional local linear regression is
fit to the data. Although this is not quite the same as looking at slices of
a fitted three-dimensional surface, it is probably more useful in terms of
understanding the joint behavior of the data.