+++
title = "ESL-6.4 Structured Local Regression Models in ℝᵖ"
summary = """
统计学习基础（译注）第六章第四节，第 201-205 页。
"""

date = 2018-11-21T20:45:00+08:00
lastmod = 2018-11-21T20:45:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

When the dimension to sample-size ratio is unfavorable, local regression
does not help us much, unless we are willing to make some structural as-
sumptions about the model. Much of this book is about structured regres-
sion and classification models. Here we focus on some approaches directly
related to kernel methods.

### 6.4.1 Structured Kernels

One line of approach is to modify the kernel. The default spherical ker-
nel (6.13) gives equal weight to each coordinate, and so a natural default
strategy is to standardize each variable to unit standard deviation. A more
general approach is to use a positive semidefinite matrix A to weigh the
different coordinates:

$$K\_{\lambda, \mathbf{A}} = D \left(
  \frac{(x-x\_0)^T \mathbf{A} (x-x\_0)}{\lambda}
\right)\tag{6.14}$$

Entire coordinates or directions can be downgraded or omitted by imposing
appropriate restrictions on A. For example, if A is diagonal, then we can
increase or decrease the influence of individual predictors X j by increasing
or decreasing A jj . Often the predictors are many and highly correlated,
such as those arising from digitized analog signals or images. The covariance
function of the predictors can be used to tailor a metric A that focuses less,
say, on high-frequency contrasts (Exercise 6.4). Proposals have been made
for learning the parameters for multidimensional kernels. For example, the
projection-pursuit regression model discussed in Chapter 11 is of this flavor,
where low-rank versions of A imply ridge functions for f ˆ (X). More general
models for A are cumbersome, and we favor instead the structured forms
for the regression function discussed next.

### 6.4.2 Structured Regression Functions

We are trying to fit a regression function E(Y |X) = f (X 1 , X 2 , . . . , X p ) in
IR p , in which every level of interaction is potentially present. It is natural
to consider analysis-of-variance (ANOVA) decompositions of the form

$$f(X\_1, X\_2, \dots, X\_p) =
\alpha + \sum\_j g\_j(X\_j) + \sum\_{k<l} g\_{kl}(X\_k, X\_l) + \cdots
\tag{6.15}$$

and then introduce structure by eliminating some of the higher-order terms.
Additive models assume only main effect terms: f (X) = α + j=1 g j (X j );
second-order models will have terms with interactions of order at most
two, and so on. In Chapter 9, we describe iterative backfitting algorithms
for fitting such low-order interaction models. In the additive model, for
example, if all but the kth P term is assumed known, then we can estimate g k
by local regression of Y − j6 = k g j (X j ) on X k . This is done for each function
in turn, repeatedly, until convergence. The important detail is that at any
stage, one-dimensional local regression is all that is needed. The same ideas
can be used to fit low-dimensional ANOVA decompositions.

An important special case of these structured models are the class of
varying coefficient models. Suppose, for example, that we divide the p pre-
dictors in X into a set (X 1 , X 2 , . . . , X q ) with q < p, and the remainder of
the variables we collect in the vector Z. We then assume the conditionally
linear model

$$f(X) = \alpha(Z) + \beta\_1(Z)X\_1 + \cdots + \beta\_q(Z)X\_q
\tag{6.16}$$

For given Z, this is a linear model, but each of the coefficients can vary
with Z. It is natural to fit such a model by locally weighted least squares:

$$\min\_{\substack{\alpha(z\_0)\\\\\beta(z\_0)}} \sum\_{i=1}^N
K\_\lambda(z\_0, z\_i)
(y\_i - \alpha(z\_0) - x\_{1i}\beta\_1(z\_0) - \cdots - x\_{qi}\beta\_q(z\_0))^2
\tag{6.17}$$

Figure 6.10 illustrates the idea on measurements of the human aorta.
A longstanding claim has been that the aorta thickens with age . Here we
model the diameter of the aorta as a linear function of age , but allow the
coefficients to vary with gender and depth down the aorta. We used a local
regression model separately for males and females. While the aorta clearly
does thicken with age at the higher regions of the aorta, the relationship
fades with distance down the aorta. Figure 6.11 shows the intercept and
slope as a function of depth.

Figure 6.10 illustrates the idea on measurements of the human aorta.
A longstanding claim has been that the aorta thickens with age . Here we
model the diameter of the aorta as a linear function of age , but allow the
coefficients to vary with gender and depth down the aorta. We used a local
regression model separately for males and females. While the aorta clearly
does thicken with age at the higher regions of the aorta, the relationship
fades with distance down the aorta. Figure 6.11 shows the intercept and
slope as a function of depth.
