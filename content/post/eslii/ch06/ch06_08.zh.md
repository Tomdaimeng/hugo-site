+++
title = "ESL-6.8 Mixture Models for Density Estimation and Classification"
summary = """
统计学习基础（译注）第六章第八节，第 214-215 页。
"""

date = 2018-11-21T23:55:00+08:00
lastmod = 2018-11-21T23:55:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The mixture model is a useful tool for density estimation, and can be viewed
as a kind of kernel method. The Gaussian mixture model has the form

$$f(x) = \sum\_{m=1}^M \alpha\_m \phi(x; \mu\_m, \mathbf{\Sigma}\_m)\tag{6.32}$$

with mixing proportions α m ,m α m = 1, and each Gaussian density has
a mean μ m and covariance matrix Σ m . In general, mixture models can use
any component densities in place of the Gaussian in (6.32): the Gaussian
mixture model is by far the most popular.

The parameters are usually fit by maximum likelihood, using the EM
algorithm as described in Chapter 8. Some special cases arise:

* If the covariance matrices are constrained to be scalar: Σ m = σ m I,
then (6.32) has the form of a radial basis expansion.
* If in addition σ m = σ > 0 is fixed, and M ↑ N , then the max-
imum likelihood estimate for (6.32) approaches the kernel density
estimate (6.22) where α̂ m = 1/N and μ̂ m = x m .

Using Bayes’ theorem, separate mixture densities in each class lead to flex-
ible models for Pr(G|X); this is taken up in some detail in Chapter 12.

Figure 6.17 shows an application of mixtures to the heart disease risk-
factor study. In the top row are histograms of Age for the no CHD and CHD
groups separately, and then combined on the right. Using the combined
data, we fit a two-component mixture of the form (6.32) with the (scalars)
Σ 1 and Σ 2 not constrained to be equal. Fitting was done via the EM
algorithm (Chapter 8): note that the procedure does not use knowledge of
the CHD labels. The resulting estimates were

$$\begin{matrix}
\hat{\mu}\_1 = 36.4 & \hat{\Sigma}\_1 = 157.7 &
\hat{\alpha}\_1 = 0.7 \\\\ \hat{\mu}\_2 = 58.0 &
\hat{\Sigma}\_2 = 15.6 & \hat{\alpha}\_2 =0.3
\end{matrix}$$

The component densities φ(μ̂ 1 , Σ̂ 1 ) and φ(μ̂ 2 , Σ̂ 2 ) are shown in the lower-
left and middle panels. The lower-right panel shows these component den-
sities (orange and blue) along with the estimated mixture density (green).

The mixture model also provides an estimate of the probability that
observation i belongs to component m,

$$\hat{r}\_{im} = \frac
{\hat{\alpha}\_m \phi(x\_i; \hat{\mu}\_m, \hat{\Sigma}\_m)}
{\sum\_{k=1}^M \hat{\alpha}\_k \phi(x\_i; \hat{\mu}\_k, \hat{\Sigma}\_k)}
\tag{6.33}$$

where x i is Age in our example. Suppose we threshold each value r̂ i2 and
hence define δ̂ i = I(r̂ i2 > 0.5). Then we can compare the classification of
each observation by CHD and the mixture model:

|     |     | $\hat{\delta}=0$  |  $\hat{\delta}=1$ |
|-----|-----|-------------------|-------------------|
| CHD | No  | 232               | 70                |
|     | Yes |  76               | 84                |

Although the mixture model did not use the CHD labels, it has done a fair
job in discovering the two CHD subpopulations. Linear logistic regression,
using the CHD as a response, achieves the same error rate (32%) when fit to
these data using maximum-likelihood (Section 4.4).