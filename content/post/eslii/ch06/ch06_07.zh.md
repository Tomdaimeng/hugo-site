+++
title = "ESL-6.7 Radial Basis Functions and Kernels"
summary = """
统计学习基础（译注）第六章第七节，第 212-214 页。
"""

date = 2018-11-21T23:45:00+08:00
lastmod = 2018-11-21T23:45:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In Chapter 5, functions are represented as expansions in basis functions:
f (x) = j=1 β j h j (x). The art of flexible modeling using basis expansions
consists of picking an appropriate family of basis functions, and then con-
trolling the complexity of the representation by selection, regularization, or
both. Some of the families of basis functions have elements that are defined
locally; for example, B-splines are defined locally in IR. If more flexibility
is desired in a particular region, then that region needs to be represented
by more basis functions (which in the case of B-splines translates to more
knots). Tensor products of IR-local basis functions deliver basis functions
local in IR p . Not all basis functions are local—for example, the truncated
power bases for splines, or the sigmoidal basis functions σ(α 0 + αx) used
in neural-networks (see Chapter 11). The composed function f (x) can nev-
ertheless show local behavior, because of the particular signs and values
of the coefficients causing cancellations of global effects. For example, the
truncated power basis has an equivalent B-spline basis for the same space
of functions; the cancellation is exact in this case.

Kernel methods achieve flexibility by fitting simple models in a region
local to the target point x 0 . Localization is achieved via a weighting kernel
K λ , and individual observations receive weights K λ (x 0 , x i ).

Radial basis functions combine these ideas, by treating the kernel func-
tions K λ (ξ, x) as basis functions. This leads to the model

$$\begin{align}
f(x) &= \sum\_{j=1}^M K\_{\lambda\_j}(\xi\_j, x) \beta\_j \\\\ &=
\sum\_{j=1}^M D\left(\frac{\\|x-\xi\_j\\|}{\lambda\_j}\right) \beta\_j
\tag{6.28}\end{align}$$

where each basis element is indexed by a location or prototype parameter ξ j
and a scale parameter λ j . A popular choice for D is the standard Gaussian
density function. There are several approaches to learning the parameters
{λ j , ξ j , β j }, j = 1, . . . , M . For simplicity we will focus on least squares
methods for regression, and use the Gaussian kernel.

* Optimize the sum-of-squares with respect to all the parameters:
$$\begin{align}
\min\_{\\{\lambda\_j, \xi\_j, \beta\_j\\}\_1^M}
\sum\_{i=1}^N \Bigg( & y\_i - \beta\_0 - \\\\ & \sum\_{j=1}^M
\beta\_j \exp\bigg\\{
  -\frac{(x\_i-\xi\_j)^T(x\_i-\xi\_j)}{\lambda\_j^2}
\bigg\\}\Bigg)
\tag{6.29}\end{align}$$
This model is commonly referred to as an RBF network, an alterna-
tive to the sigmoidal neural network discussed in Chapter 11; the ξ j
and λ j playing the role of the weights. This criterion is nonconvex
with multiple local minima, and the algorithms for optimization are
similar to those used for neural networks.
* Estimate the {λ j , ξ j } separately from the β j . Given the former, the
estimation of the latter is a simple least squares problem. Often the
kernel parameters λ j and ξ j are chosen in an unsupervised way using
the X distribution alone. One of the methods is to fit a Gaussian
mixture density model to the training x i , which provides both the
centers ξ j and the scales λ j . Other even more adhoc approaches use
clustering methods to locate the prototypes ξ j , and treat λ j = λ
as a hyper-parameter. The obvious drawback of these approaches is
that the conditional distribution Pr(Y |X) and in particular E(Y |X)
is having no say in where the action is concentrated. On the positive
side, they are much simpler to implement.

While it would seem attractive to reduce the parameter set and assume
a constant value for λ j = λ, this can have an undesirable side effect of
creating holes—regions of IR p where none of the kernels has appreciable
support, as illustrated in Figure 6.16 (upper panel). Renormalized radial
basis functions,

$$h\_j(x) = \frac
{D(\\|x-\xi\_j\\|/\lambda)}{\sum\_{k=1}^M D(\\|x-\xi\_k\\|/\lambda)}
\tag{6.30}$$

avoid this problem (lower panel).

The Nadaraya–Watson kernel regression estimator (6.2) in IR p can be
viewed as an expansion in renormalized radial basis functions,

$$\begin{align}
\hat{f}(x\_0) &= \sum\_{i=1}^N y\_i \frac
{K\_\lambda(x\_0, x\_i)}{\sum\_{i=1}^N K\_\lambda(x\_0, x\_i)} \\\\ &=
\sum\_{i=1}^N y\_i h\_i(x\_0)
\tag{6.31}\end{align}$$

with a basis function h i located at every observation and coefficients y i ;
that is, ξ i = x i , β̂ i = y i , i = 1, . . . , N .

Note the similarity between the expansion (6.31) and the solution (5.50)
on page 169 to the regularization problem induced by the kernel K. Radial
basis functions form the bridge between the modern “kernel methods” and
local fitting technology.