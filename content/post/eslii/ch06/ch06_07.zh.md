+++
title = "ESL-6.7 径向基函数与核函数"
summary = """
统计学习基础（译注）第六章第七节，第 212-214 页。
基函数展开的方法的灵活性在于大量的基函数，
核函数方法的灵活性在于局部性，
径向基函数则结合了上述两者。
"""

date = 2018-11-22T19:50:00+08:00
lastmod = 2018-11-22T19:50:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

在第五章中，函数被表述为基函数的拓展：
$f(x) = \sum\_{j=1}^M \beta\_j h\_j(x)$。
使用基函数展开建模的灵活性在于
选择合适的基函数族，
然后再通过筛选、正则化、或两者并举来控制函数表达式的复杂度。
有些类型的基函数存在着定义在局部的元素；
例如，B-样条即定义在 $\mathbb{R}$ 的局部。
若想要在特定的局部获得更灵活的模型，
则需要在该区域上的表达式增加更多的基函数
（在 B-样条例子中则是增加更多结点）。
$\mathbb{R}$ 的局部基函数的张量积可生成 $\mathbb{R}^p$ 上的局部基函数。
并不是所有基函数都是局部的。
例如，样条中的截断幂次基函数，
或神经网络（第十一章）中的 S 形基函数 $\sigma(\alpha\_0+\alpha x)$。
尽管如此，由于系数的特定的符号和取值可能会彼此消除某些全局效应，
其组成的函数 $f(x)$ 仍可表现出局部性。
例如，对截断幂次基函数，存在着等价的 B-样条基函数，
使得两者生成的函数空间是一样的；
在这种场景中，全局效应被完全消除。

核函数方法的灵活性在于
在目标点 $x\_0$ 的局部区域拟合简单的模型。
局部性是通过一个加权核函数 $K\_\lambda$ 实现的，
每个样本被赋予权重 $K\_\lambda(x\_0, x\_i)$。

径向基函数则结合了上述两种思想，
将核函数 $K\_\lambda(\xi, x)$ 作为一个基函数。
这引入了模型：

$$\begin{align}
f(x) &= \sum\_{j=1}^M K\_{\lambda\_j}(\xi\_j, x) \beta\_j \\\\ &=
\sum\_{j=1}^M D\left(\frac{\\|x-\xi\_j\\|}{\lambda\_j}\right) \beta\_j
\tag{6.28}\end{align}$$

其中的每个基函数成分被一个位置或**原型**（prototype）参数 $\xi\_j$
和一个尺度参数 $\lambda\_j$ 索引。
$D$ 的常见选择为标准高斯密度函数。
有很多种获得参数 $\\{\lambda\_j, \xi\_j, \beta\_j\\}$，$j=1,\dots,M$ 的方法。
简单起见，这里对回归问题着重于最小二乘方法，并使用高斯核函数。

* 对所有的参数进行平方和的最优化：
  $$\begin{align}
  \min\_{\\{\lambda\_j, \xi\_j, \beta\_j\\}\_1^M}
  \sum\_{i=1}^N \Bigg( & y\_i - \beta\_0 - \\\\ & \sum\_{j=1}^M
  \beta\_j \exp\bigg\\{
    -\frac{(x\_i-\xi\_j)^T(x\_i-\xi\_j)}{\lambda\_j^2}
  \bigg\\}\Bigg)
  \tag{6.29}\end{align}$$
  这个模型一般被称为径向基（RBF）网络，
  可视为是第十一章中的 S 状神经网络的一个替代方法；
  参数 $\xi\_j$ 和 $\lambda\_j$ 起到了权重的作用。
  这个准则函数是非凸的，有多个局部最小点，
  其最优化的算法与神经网络中使用的类似。

* 对不同 $\beta\_j$ 分别估计 $\\{\lambda\_j, \xi\_j\\}$。
  给定后者，则前者的估计是一个简单的最小二乘问题。
  通常会通过一个无监督的方式只从 $X$ 的分布来选择核函数的参数 $\lambda\_j$ 和 $\xi\_j$。
  其中一种方法是对训练样本 $x\_i$ 拟合一个高斯混合密度模型，
  同时得到多个中心 $\xi\_j$ 和尺度 $\lambda\_j$。
  另一个更特殊的方法是通过聚类方法来定位原型 $\xi\_j$，
  并将 $\lambda\_j=\lambda$ 作为一个超（hyper）参数。
  这类方法明显的问题是条件分布 $\text{Pr}(Y|X)$ 和特别是 $E(Y|X)$
  对选择集中的位置没有任何影响。
  相应地，其好处是比较容易实现。

{{< figure
  src="http://public.guansong.wang/eslii/ch06/eslii_fig_06_16.png"
  title="**图6.16**：$\mathbb{R}$ 上的固定宽度的高斯径向基函数可能造成空洞（上图）。重标准化的高斯径向基函数可避免这个问题，其产生的基函数在某些方面与 B-样条类似。"
>}}

尽管常数 $\lambda\_j=\lambda$ 的假设可缩减参数集的大小，
但这可能会在空间上形成**空洞**（holes）。
如图 6.16 中上图所示，在 $\mathbb{R}^p$ 中的一些区域上，
没有任一核函数有相应的支撑集。
使用**重标准化**（renormalized）的径向基函数可解决这个问题（下图）：

$$h\_j(x) = \frac
{D(\\|x-\xi\_j\\|/\lambda)}{\sum\_{k=1}^M D(\\|x-\xi\_k\\|/\lambda)}
\tag{6.30}$$

等式 6.2 中的 $\mathbb{R}^p$ 上的 Nadaraya-Watson 核函数回归估计
可被视为用重标准化径向基函数的展开：

$$\begin{align}
\hat{f}(x\_0) &= \sum\_{i=1}^N y\_i \frac
{K\_\lambda(x\_0, x\_i)}{\sum\_{i=1}^N K\_\lambda(x\_0, x\_i)} \\\\ &=
\sum\_{i=1}^N y\_i h\_i(x\_0)
\tag{6.31}\end{align}$$

其中的基函数 $h\_i$ 位置在每个样本处，系数为 $y\_i$；
即 $\xi\_i = x\_i$，$\hat{\beta}\_i = y\_i$，$i = 1,\dots,N$。

注意展开式 6.31 与
径向基函数引入的正则化问题解 5.50
（[第 5.8 节]({{< ref "/post/eslii/ch05/ch05_08.zh.md" >}})，第 169 页）
之间的相似之处，
将现代的“核方法”与局部拟合方法关联了起来。