+++
title = "ESL-6.9 Computational Considerations"
summary = """
统计学习基础（译注）第六章第九节，第 216 页。
"""

date = 2018-11-21T23:59:00+08:00
lastmod = 2018-11-21T23:59:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Kernel and local regression and density estimation are memory-based meth-
ods: the model is the entire training data set, and the fitting is done at
evaluation or prediction time. For many real-time applications, this can
make this class of methods infeasible.

The computational cost to fit at a single observation x 0 is O(N ) flops,
except in oversimplified cases (such as square kernels). By comparison,
an expansion in M basis functions costs O(M ) for one evaluation, and
typically M ∼ O(log N ). Basis function methods have an initial cost of at
least O(N M 2 + M 3 ).

The smoothing parameter(s) λ for kernel methods are typically deter-
mined off-line, for example using cross-validation, at a cost of O(N 2 ) flops.
Popular implementations of local regression, such as the loess function in
S-PLUS and R and the locfit procedure (Loader, 1999), use triangulation
schemes to reduce the computations. They compute the fit exactly at M
carefully chosen locations (O(N M )), and then use blending techniques to
interpolate the fit elsewhere (O(M ) per evaluation).