+++
title = "ESL-6.1 一维的核平滑器"
summary = """
统计学习基础（译注）第六章第一节，第 192-198 页。
"""

date = 2018-11-16T16:50:00+08:00
lastmod = 2018-11-16T16:50:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

在第二章中，简单介绍了 k-近邻的平均作为回归函数 $E(Y|X=x)$ 的估计：

$$\hat{f}(x) = \text{Ave}(y\_i|x\_i \in N\_k(x)) \tag{6.1}$$

其中的 $N\_k(x)$ 为与 $x$ 的平方距离最近的 $k$ 个点的集合，
“Ave” 代表了平均数（均值）的计算。
其基本思路是放宽条件期望的定义，计算目标点邻域上的平均，如图 6.1 的左边所示。

{{< figure
  src="http://public.guansong.wang/eslii/ch06/eslii_fig_06_01.png"
  title="**图6.1**："
>}}

在此例中使用了 30 个近邻点，
即在 $x\_0$ 的拟合值为输入变量点 $x\_i$ 与 $x\_0$ 最接近的 30 个样本的输出变量的平均。
如此在不同的 $x\_0$ 点处拟合，则可描出绿色的曲线。
由于 $\hat{f}(x)$ 对 $x$ 不连续，绿色曲线比较崎岖。
将 $x\_0$ 点从左向右移动，$k$ 个近邻点会保持不变，


as an estimate of the regression function E(Y |X = x). Here N k (x) is the set
of k points nearest to x in squared distance, and Ave denotes the average
(mean). The idea is to relax the definition of conditional expectation, as
illustrated in the left panel of Figure 6.1, and compute an average in a
neighborhood of the target point. In this case we have used the 30-nearest
neighborhood—the fit at x 0 is the average of the 30 pairs whose x i values
are closest to x 0 . The green curve is traced out as we apply this definition
at different values x 0 . The green curve is bumpy, since f ˆ (x) is discontinuous
in x. As we move x 0 from left to right, the k-nearest neighborhood remains
constant, until a point x i to the right of x 0 becomes closer than the furthest
point x i ′ in the neighborhood to the left of x 0 , at which time x i replaces x i ′ .
The average in (6.1) changes in a discrete way, leading to a discontinuous
f ˆ (x).

This discontinuity is ugly and unnecessary. Rather than give all the
points in the neighborhood equal weight, we can assign weights that die
off smoothly with distance from the target point. The right panel shows
an example of this, using the so-called Nadaraya–Watson kernel-weighted
average

$$\hat{f}(x\_0) = \frac
{\sum\_{i=1}^N K\_\lambda(x\_0, x\_i) y\_i}
{\sum\_{i=1}^N K\_\lambda(x\_0, x\_i)}
\tag{6.2}$$

with the Epanechnikov quadratic kernel

$$K\_\lambda(x\_0, x) = D \left (
\frac{\|x-x\_0\|}{\lambda} \right )
\tag{6.3}$$

with

$$D(t) = \begin{cases}
\frac{3}{4}(1-t^2) & \text{如果 } \|t\| \leq 1 \\\\ 0 & \text{其他} 
\end{cases}\tag{6.4}$$

The fitted function is now continuous, and quite smooth in the right panel
of Figure 6.1. As we move the target from left to right, points enter the
neighborhood initially with weight zero, and then their contribution slowly
increases (see Exercise 6.1).

In the right panel we used a metric window size λ = 0.2 for the kernel
fit, which does not change as we move the target point x 0 , while the size
of the 30-nearest-neighbor smoothing window adapts to the local density
of the x i . One can, however, also use such adaptive neighborhoods with
kernels, but we need to use a more general notation. Let h λ (x 0 ) be a width
function (indexed by λ) that determines the width of the neighborhood at
x 0 . Then more generally we have

$$K\_\lambda(x\_0, x) = D \left (
\frac{\|x-x\_0\|}{h\_\lambda(x\_0)} \right )
\tag{6.5}$$

In (6.3), h λ (x 0 ) = λ is constant. For k-nearest neighborhoods, the neigh-
borhood size k replaces λ, and we have h k (x 0 ) = |x 0 − x [k] | where x [k] is
the kth closest x i to x 0 .

There are a number of details that one has to attend to in practice:

* The smoothing parameter λ, which determines the width of the local
  neighborhood, has to be determined. Large λ implies lower variance
  (averages over more observations) but higher bias (we essentially as-
  sume the true function is constant within the window).
* Metric window widths (constant h λ (x)) tend to keep the bias of the
  estimate constant, but the variance is inversely proportional to the
  local density. Nearest-neighbor window widths exhibit the opposite
  behavior; the variance stays constant and the absolute bias varies
  inversely with local density.
* Issues arise with nearest-neighbors when there are ties in the x i . With
  most smoothing techniques one can simply reduce the data set by
  averaging the y i at tied values of X, and supplementing these new
  observations at the unique values of x i with an additional weight w i
  (which multiples the kernel weight).
* This leaves a more general problem to deal with: observation weights
  w i . Operationally we simply multiply them by the kernel weights be-
  fore computing the weighted average. With nearest neighborhoods, it
  is now natural P to insist on neighborhoods with a total weight content
  k (relative to w i ). In the event of overflow (the last observation
  needed in a neighborhood has a weight w j which causes the sum of
  weights to exceed the budget k), then fractional parts can be used.
* Boundary issues arise. The metric neighborhoods tend to contain less
  points on the boundaries, while the nearest-neighborhoods get wider.
* The Epanechnikov kernel has compact support (needed when used
  with nearest-neighbor window size). Another popular compact kernel
  is based on the tri-cube function
  $$D(t) = \begin{cases}
  (1-\|t\|^3)^3 & \text{如果 } \|t\| \leq 1 \\\\ 0 & \text{其他} 
  \end{cases}\tag{6.4}$$
  This is flatter on the top (like the nearest-neighbor box) and is differ-
  entiable at the boundary of its support. The Gaussian density func-
  tion D(t) = φ(t) is a popular noncompact kernel, with the standard-
  deviation playing the role of the window size. Figure 6.2 compares
  the three.

### 6.1.1 Local Linear Regression

We have progressed from the raw moving average to a smoothly varying
locally weighted average by using kernel weighting. The smooth kernel fit
still has problems, however, as exhibited in Figure 6.3 (left panel). Locally-
weighted averages can be badly biased on the boundaries of the domain,
because of the asymmetry of the kernel in that region. By fitting straight
lines rather than constants locally, we can remove this bias exactly to first
order; see Figure 6.3 (right panel). Actually, this bias can be present in the
interior of the domain as well, if the X values are not equally spaced (for
the same reasons, but usually less severe). Again locally weighted linear
regression will make a first-order correction.

Locally weighted regression solves a separate weighted least squares prob-
lem at each target point x 0 :

$$\min\_{\alpha(x\_0), \beta(x\_0)}
\sum\_{i=1}^N K\_\lambda(x\_0, x\_i)[y\_i - \alpha(x\_0) - \beta(x\_0)x\_i]^2
\tag{6.7}$$

The estimate is then f ˆ (x 0 ) = α̂(x 0 ) + β̂(x 0 )x 0 . Notice that although we fit
an entire linear model to the data in the region, we only use it to evaluate
the fit at the single point x 0 .

Define the vector-valued function b(x) T = (1, x). Let B be the N × 2
regression matrix with ith row b(x i ) T , and W(x 0 ) the N × N diagonal
matrix with ith diagonal element K λ (x 0 , x i ). Then

$$\begin{align}
\hat{f}(x\_0) &=
b(x\_0)^T(\mathbf{B}^T\mathbf{W}(x\_0)\mathbf{B})^{-1}
\mathbf{B}^T\mathbf{W}(x\_0)\mathbf{y} \tag{6.8} \\\\ &=
\sum\_{i=1}^N l\_i(x\_0) y\_i \tag{6.9}
\end{align}$$

Equation (6.8) gives an explicit expression for the local linear regression
estimate, and (6.9) highlights the fact that the estimate is linear in the
y i (the l i (x 0 ) do not involve y). These weights l i (x 0 ) combine the weight-
ing kernel K λ (x 0 , ·) and the least squares operations, and are sometimes
referred to as the equivalent kernel. Figure 6.4 illustrates the effect of lo-
cal linear regression on the equivalent kernel. Historically, the bias in the
Nadaraya–Watson and other local average kernel methods were corrected
by modifying the kernel. These modifications were based on theoretical
asymptotic mean-square-error considerations, and besides being tedious to
implement, are only approximate for finite sample sizes. Local linear re-
gression automatically modifies the kernel to correct the bias exactly to
first order, a phenomenon dubbed as automatic kernel carpentry. Consider
the following expansion for E f ˆ (x 0 ), using the linearity of local regression
and a series expansion of the true function f around x 0 ,

$$\begin{align}
E\hat{f}(x\_0) =& \sum\_{i=1}^N l\_i(x\_0) f(x\_i) \\\\ =&
f(x\_0) \sum\_{i=1}^N l\_i(x\_0) +
f^\prime(x\_0)\sum\_{i=1}^N(x\_i-x\_0) l\_i(x\_0) \\\\ +&
\frac{f^{\prime\prime}(x\_0)}{2}\sum\_{i=1}^N(x\_i-x\_0)^2 l\_i(x\_0)
+ R \tag{6.10}\end{align}$$

where the remainder term R involves third- and higher-order derivatives of
f , and is typically small under suitable smoothness assumptions. It can be
shown (Exercise 6.2) that for local linear regression, i=1 l i (x 0 ) = 1 and
P N i=1 (x i − x 0 )l i (x 0 ) = 0. Hence the middle term equals f (x 0 ), and since
the bias is E f ˆ (x 0 ) − f (x 0 ), we see that it depends only on quadratic and
higher–order terms in the expansion of f .

### 6.1.2 Local Polynomial Regression

Why stop at local linear fits? We can fit local polynomial fits of any degree d,

$$\min\_{\substack{{\alpha(x\_0), \beta\_j(x\_0)}\\\\{j=1,\dots,d}}}
\sum\_{i=1}^N K\_\lambda(x\_0, x\_i) \left [
y\_i - \alpha(x\_0) - \sum\_{j=1}^d \beta\_j(x\_0)x\_i^j \right]^2
\tag{6.11}$$

with solution f ˆ (x 0 ) = α̂(x 0 )+ j=1 β̂ j (x 0 )x j 0 . In fact, an expansion such as
(6.10) will tell us that the bias will only have components of degree d+1 and
higher (Exercise 6.2). Figure 6.5 illustrates local quadratic regression. Local
linear fits tend to be biased in regions of curvature of the true function, a
phenomenon referred to as trimming the hills and filling the valleys. Local
quadratic regression is generally able to correct this bias.

There is of course a price to be paid for this bias reduction, and that is
increased variance. The fit in the right panel of Figure 6.5 is slightly more
wiggly, especially in the tails. Assuming the model y i = f (x i ) + ε i , with
ε i independent and identically distributed with mean zero and variance
σ 2 , Var( f ˆ (x 0 )) = σ 2 ||l(x 0 )|| 2 , where l(x 0 ) is the vector of equivalent kernel
weights at x 0 . It can be shown (Exercise 6.3) that ||l(x 0 )|| increases with d,
and so there is a bias–variance tradeoff in selecting the polynomial degree.
Figure 6.6 illustrates these variance curves for degree zero, one and two
local polynomials. To summarize some collected wisdom on this issue:

* Local linear fits can help bias dramatically at the boundaries at a
modest cost in variance. Local quadratic fits do little at the bound-
aries for bias, but increase the variance a lot.
* Local quadratic fits tend to be most helpful in reducing bias due to
curvature in the interior of the domain.
* Asymptotic analysis suggest that local polynomials of odd degree
dominate those of even degree. This is largely due to the fact that
asymptotically the MSE is dominated by boundary effects.

While it may be helpful to tinker, and move from local linear fits at the
boundary to local quadratic fits in the interior, we do not recommend such
strategies. Usually the application will dictate the degree of the fit. For
example, if we are interested in extrapolation, then the boundary is of
more interest, and local linear fits are probably more reliable.