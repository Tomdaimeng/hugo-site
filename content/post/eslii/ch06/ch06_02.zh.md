+++
title = "ESL-6.2 核函数窗宽的选择"
summary = """
统计学习基础（译注）第六章第二节，第 198-199 页。
窗宽的选择背后是偏差方差权衡，
选择方法与第五章中对平滑样条惩罚参数类似
（交叉验证）。
"""

date = 2018-11-21T16:55:00+08:00
lastmod = 2018-11-21T16:55:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

在每个核函数 $K\_\lambda$ 中，
$\lambda$ 为控制窗宽的参数：

* 对于使用距离窗宽的 Epanechnikov 或三次核函数，
  $\lambda$ 为其支撑集的半径。
* 对高斯核函数，$\lambda$ 为其标准差。
* $k$-近邻中，$\lambda$ 为最近邻域中的样本个数 $k$，
  通常也被表达为一个对整体训练样本大小的比例 $k/N$。

在选择局部的窗宽时存在偏差方差权衡，这在局部平均中显而易见：

* 若窗宽较小，$\hat{f}(x\_0)$ 为离 $x\_0$ 很近的少数几个点的 $y\_i$ 平均，
  其方差相对较大，几乎接近于单个观测值 $y\_i$ 的方差。
  其偏差相对较小，这是由于在这个小邻域上每个点 $E(y\_i) = f(x\_i)$ 都接近于 $f(x\_0)$。
* 若窗宽较大，相对于任意 $y\_i$，$\hat{f}(x\_0)$ 的方差会小很多，
  这是由于它是更多样本的平均。
  但在平均中用到了距离 $x\_0$ 更远的 $x\_i$，
  无法保证 $f(x\_i)$ 与 $f(x\_0)$ 足够解近，
  故偏差会比较高。

以上逻辑也适用于局部回归估计，以局部线性为例：
随着窗宽缩小至零，
估计结果趋近于一个对训练样本数据进行内插的分段线性函数[^1]；
随着窗宽无限变大，
估计结果趋近于一个训练样本数据的全局的线性最小二乘拟合。

第五章中对选择平滑样条正则化参数的讨论，在这里同样适用，因此不再复述。
局部回归平滑器是线性的估计；
$\hat{\mathbf{f}} = \mathbf{S}\_\lambda \mathbf{y}$ 中的平滑矩阵
通过等价核（等式 6.8）构建，
其第 ij 个元素为 $\\{\mathbf{S}\_\lambda\\}\_{ij} = l\_i(x\_j)$。
留一法的交叉验证非常容易实施（练习 6.7），
类似地也可用一般性的交叉验证 $C\_p$（练习 6.10），
以及 $k$ 次交叉验证。
有效自由度也是被定义为 $\text{trace}(\mathbf{S}\_\lambda)$，
可被用来校准平滑程度。
图 6.7 中比较了平滑样条和局部线性回归的等价核。
局部回归平滑器的窗宽（比例）为 40%，对应的自由度
$\text{df} = \text{trace}(\mathbf{S}\_\lambda) = 5.86$
平滑样条被校准到相似的自由度，
它们的等价核从数值上分成相似。

{{< figure
  src="http://public.guansong.wang/eslii/ch06/eslii_fig_06_07.png"
  title="**图6.7**：局部线性回归平滑器（三次核函数；橙色）和平滑样条（蓝色）的等价核，两者有相同的有效自由度。突起的位置即为目标点。"
>}}

[^1]: 原文脚注 1：这是针对均匀分布的 $x\_i$；不规则分布的 $x\_i$ 会产生更差的结果。