+++
title = "ESL-6.5 Local Likelihood and Other Models"
summary = """
统计学习基础（译注）第六章第五节，第 205-208 页。
"""

date = 2018-11-21T21:45:00+08:00
lastmod = 2018-11-21T21:45:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The concept of local regression and varying coefficient models is extremely
broad: any parametric model can be made local if the fitting method ac-
commodates observation weights. Here are some examples:

* Associated with each observation y i is a parameter θ i = θ(x i ) = x Ti β
linear in the covariate(s) x i , and inference for β is based on the log-
likelihood l(β) = i=1 l(y i , x Ti β). We can model θ(X) more flexibly
by using the likelihood local to x 0 for inference of θ(x 0 ) = x T 0 β(x 0 ):
$$l(\beta(x\_0)) = \sum\_{i=1}^N
K\_\lambda(x\_0, x\_i) l(y\_i, x\_i^T\beta(x\_0))$$
Many likelihood models, in particular the family of generalized linear
models including logistic and log-linear models, involve the covariates
in a linear fashion. Local likelihood allows a relaxation from a globally
linear model to one that is locally linear.
* As above, except different variables are associated with θ from those
used for defining the local likelihood:
$$l(\theta(x\_0)) = \sum\_{i=1}^N
K\_\lambda(x\_0, x\_i) l(y\_i, \eta(x\_i, \theta(x\_0)))$$
For example, η(x, θ) = x T θ could be a linear model in x. This will fit
a varying coefficient model θ(z) by maximizing the local likelihood.
* Autoregressive time series models of order k have the form y t =
β 0 + β 1 y t−1 + β 2 y t−2 + · · · + β k y t−k + ε t . Denoting the lag set by
z t = (y t−1 , y t−2 , . . . , y t−k ), the model looks like a standard linear
model y t = z t T β + ε t , and is typically fit by least squares. Fitting
by local least squares with a kernel K(z 0 , z t ) allows the model to
vary according to the short-term history of the series. This is to be
distinguished from the more traditional dynamic linear models that
vary by windowing time.

As an illustration of local likelihood, we consider the local version of the
multiclass linear logistic regression model (4.36) of Chapter 4. The data
consist of features x i and an associated categorical response g i ∈ {1, 2, . . . , J},
and the linear model has the form

$$\text{Pr}(G=j|X=x) = \frac
{e^{\beta\_{j0} + \beta\_j^T x}}
{1 + \sum\_{k=1}^{J-1} e^{\beta\_{k0} + \beta\_k^T x}}
\tag{6.18}$$

The local log-likelihood for this J class model can be written

$$\begin{align}
\sum\_{i=1}^N  & K\_\lambda(x\_0, x\_i)  \Bigg\\{
\beta\_{g\_i 0}(x\_0) + \beta\_{g\_i}(x\_0)^T(x\_i-x\_0) \\\\ & -
\log\bigg[ 1 + \sum\_{k=1}^{J-1}\exp(
  \beta\_{g\_k 0}(x\_0) + \beta\_{g\_k}(x\_0)^T(x\_i-x\_0)
)\bigg]\Bigg\\}\tag{6.19}\end{align}$$

Notice that

* we have used g i as a subscript in the first line to pick out the appro-
priate numerator;
* β J0 = 0 and β J = 0 by the definition of the model;
* we have centered the local regressions at x 0 , so that the fitted poste-
rior probabilities at x 0 are simply
$$\hat{\text{Pr}}(G=j|X=x\_0) = \frac
{e^{\hat{\beta}\_{j0}(x\_0)}}
{1 + \sum\_{k=1}^{J-1} e^{\hat{\beta}\_{k0}(x\_0)}}
\tag{6.20}$$

This model can be used for flexible multiclass classification in moderately
low dimensions, although successes have been reported with the high-
dimensional ZIP-code classification problem. Generalized additive models
(Chapter 9) using kernel smoothing methods are closely related, and avoid
dimensionality problems by assuming an additive structure for the regres-
sion function.

As a simple illustration we fit a two-class local linear logistic model to
the heart disease data of Chapter 4. Figure 6.12 shows the univariate local
logistic models fit to two of the risk factors (separately). This is a useful
screening device for detecting nonlinearities, when the data themselves have
little visual information to offer. In this case an unexpected anomaly is
uncovered in the data, which may have gone unnoticed with traditional
methods.

Since CHD is a binary indicator, we could estimate the conditional preva-
lence Pr(G = j|x 0 ) by simply smoothing this binary response directly with-
out resorting to a likelihood formulation. This amounts to fitting a locally
constant logistic regression model (Exercise 6.5). In order to enjoy the bias-
correction of local-linear smoothing, it is more natural to operate on the
unrestricted logit scale.

Typically with logistic regression, we compute parameter estimates as
well as their standard errors. This can be done locally as well, and so
we can produce, as shown in the plot, estimated pointwise standard-error
bands about our fitted prevalence.