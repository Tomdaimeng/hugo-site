+++
title = "ESL-9.4 MARS: Multivariate Adaptive Regression Splines"
summary = """
统计学习基础（译注）第九章第四节，第 321-329 页。
"""

date = 2018-12-30T19:23:00+08:00
lastmod = 2018-12-30T19:23:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

MARS is an adaptive procedure for regression, and is well suited for high-
dimensional problems (i.e., a large number of inputs). It can be viewed as a
generalization of stepwise linear regression or a modification of the CART
method to improve the latter’s performance in the regression setting. We
introduce MARS from the first point of view, and later make the connection
to CART.

MARS uses expansions in piecewise linear basis functions of the form
(x − t) + and (t − x) + . The “+” means positive part, so

$$\begin{align}
(x-t)\_+ &= \begin{cases}
x - t & \text{if } x > t \\\\ 0 & \text{otherwise}
\end{cases}
\\\\ (t-x)\_+ &= \begin{cases}
t - x & \text{if } x < t \\\\ 0 & \text{otherwise}
\end{cases}
\end{align}$$

As an example, the functions (x − 0.5) + and (0.5 − x) + are shown in Fig-
ure 9.9.

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_09.png"
  title="**图9.9**："
>}}

Each function is piecewise linear, with a knot at the value t. In the
terminology of Chapter 5, these are linear splines. We call the two functions
a reflected pair in the discussion below. The idea is to form reflected pairs
for each input X j with knots at each observed value x ij of that input.
Therefore, the collection of basis functions is

$$\mathcal{C}= \\{(X\_j-t)\_+, (t-X\_j)\_+\\}\_{
  \substack{t \in \\{x\_{1j}, x\_{2j}, \dots, x\_{Nj}\\}
  \\\\ j = 1,2,\dots,p}}
\tag{9.18}$$

If all of the input values are distinct, there are 2N p basis functions alto-
gether. Note that although each basis function depends only on a single
X j , for example, h(X) = (X j − t) + , it is considered as a function over the
entire input space IR p .

The model-building strategy is like a forward stepwise linear regression,
but instead of using the original inputs, we are allowed to use functions
from the set C and their products. Thus the model has the form

$$f(X) = \beta\_0 + \sum\_{m=1}^M \beta\_m h\_m(X) \tag{9.19}$$

where each h m (X) is a function in C, or a product of two or more such
functions.

Given a choice for the h m , the coefficients β m are estimated by minimiz-
ing the residual sum-of-squares, that is, by standard linear regression. The
real art, however, is in the construction of the functions h m (x). We start
with only the constant function h 0 (X) = 1 in our model, and all functions
in the set C are candidate functions. This is depicted in Figure 9.10.

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_10.png"
  title="**图9.10**："
>}}

At each stage we consider as a new basis function pair all products of a
function h m in the model set M with one of the reflected pairs in C. We
add to the model M the term of the form

$$ \hat{\beta}\_{M+1} h\_l(X) \cdot (X\_j - t)\_+ +
\hat{\beta}\_{M+2} h\_l(X) \cdot (t - X\_j)\_+,
h\_l \in \mathcal{M}

that produces the largest decrease in training error. Here β̂ M +1 and β̂ M +2
are coefficients estimated by least squares, along with all the other M + 1
coefficients in the model. Then the winning products are added to the
model and the process is continued until the model set M contains some
preset maximum number of terms.

For example, at the first stage we consider adding to the model a function
of the form β 1 (X j − t) + + β 2 (t − X j ) + ; t ∈ {x ij }, since multiplication by
the constant function just produces the function itself. Suppose the best
choice is β̂ 1 (X 2 − x 72 ) + + β̂ 2 (x 72 − X 2 ) + . Then this pair of basis functions
is added to the set M, and at the next stage we consider including a pair
of products the form

$$h\_m(X) \cdot (X\_j - t)\_+ \text{ and }
h\_m(X) \cdot (t - X\_j)\_+, t \in \\{x\_{ij}\\}$$

where for h m we have the choices

$$\begin{align}
h\_0(X) &= 1,
\\\\ h\_1(X) &= (X\_2 - x\_{72})\_+, \text{ or}
\\\\ h\_2(X) &= (x\_{72} - X\_2)\_+
\end{align}$$

The third choice produces functions such as (X 1 − x 51 ) + · (x 72 − X 2 ) + ,
depicted in Figure 9.11.

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_11.png"
  title="**图9.11**："
>}}

At the end of this process we have a large model of the form (9.19). This
model typically overfits the data, and so a backward deletion procedure
is applied. The term whose removal causes the smallest increase in resid-
ual squared error is deleted from the model at each stage, producing an
estimated best model f ˆ λ of each size (number of terms) λ. One could use
cross-validation to estimate the optimal value of λ, but for computational
savings the MARS procedure instead uses generalized cross-validation. This
criterion is defined as

$$\text{GCV}(\lambda) = \frac
{\sum\_{i=1}^N (y\_i- \hat{f}\_\lambda(x\_i)^2)}
{(1 - M(\lambda) / N)^2} \tag{9.20}$$

The value M (λ) is the effective number of parameters in the model: this
accounts both for the number of terms in the models, plus the number
of parameters used in selecting the optimal positions of the knots. Some
mathematical and simulation results suggest that one should pay a price
of three parameters for selecting a knot in a piecewise linear regression.

Thus if there are r linearly independent basis functions in the model, and
K knots were selected in the forward process, the formula is M (λ) = r+cK,
where c = 3. (When the model is restricted to be additive—details below—
a penalty of c = 2 is used). Using this, we choose the model along the
backward sequence that minimizes GCV(λ).

Why these piecewise linear basis functions, and why this particular model
strategy? A key property of the functions of Figure 9.9 is their ability to
operate locally; they are zero over part of their range. When they are mul-
tiplied together, as in Figure 9.11, the result is nonzero only over the small
part of the feature space where both component functions are nonzero. As
a result, the regression surface is built up parsimoniously, using nonzero
components locally—only where they are needed. This is important, since
one should “spend” parameters carefully in high dimensions, as they can
run out quickly. The use of other basis functions such as polynomials, would
produce a nonzero product everywhere, and would not work as well.

The second important advantage of the piecewise linear basis function
concerns computation. Consider the product of a function in M with each
of the N reflected pairs for an input X j . This appears to require the fitting
of N single-input linear regression models, each of which uses O(N ) oper-
ations, making a total of O(N 2 ) operations. However, we can exploit the
simple form of the piecewise linear function. We first fit the reflected pair
with rightmost knot. As the knot is moved successively one position at a
time to the left, the basis functions differ by zero over the left part of the
domain, and by a constant over the right part. Hence after each such move
we can update the fit in O(1) operations. This allows us to try every knot
in only O(N ) operations.

The forward modeling strategy in MARS is hierarchical, in the sense that
multiway products are built up from products involving terms already in
the model. For example, a four-way product can only be added to the model
if one of its three-way components is already in the model. The philosophy
here is that a high-order interaction will likely only exist if some of its lower-
order “footprints” exist as well. This need not be true, but is a reasonable
working assumption and avoids the search over an exponentially growing
space of alternatives.

There is one restriction put on the formation of model terms: each input
can appear at most once in a product. This prevents the formation of
higher-order powers of an input, which increase or decrease too sharply
near the boundaries of the feature space. Such powers can be approximated
in a more stable way with piecewise linear functions.

A useful option in the MARS procedure is to set an upper limit on
the order of interaction. For example, one can set a limit of two, allowing
pairwise products of piecewise linear functions, but not three- or higher-
way products. This can aid in the interpretation of the final model. An
upper limit of one results in an additive model.

#### 9.4.1 Spam Example (Continued)

We applied MARS to the “spam” data analyzed earlier in this chapter. To
enhance interpretability, we restricted MARS to second-degree interactions.
Although the target is a two-class variable, we used the squared-error loss
function nonetheless (see Section 9.4.3). Figure 9.12 shows the test error
misclassification rate as a function of the rank (number of independent ba-
sis functions) in the model. The error rate levels off at about 5.5%, which is
slightly higher than that of the generalized additive model (5.3%) discussed
earlier. GCV chose a model size of 60, which is roughly the smallest model
giving optimal performance. The leading interactions found by MARS in-
volved inputs ( ch$, remove ), ( ch$, free ) and ( hp, CAPTOT ). However, these
interactions give no improvement in performance over the generalized ad-
ditive model.

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_12.png"
  title="**图9.12**："
>}}

### 9.4.2 Example (Simulated Data)

Here we examine the performance of MARS in three contrasting scenarios.
There are N = 100 observations, and the predictors X 1 , X 2 , . . . , X p and
errors ε have independent standard normal distributions.

* Scenario 1: The data generation model is
  $$Y = (X\_1 - 1)\_+ + (X\_1 - 1)\_+ \cdot (X\_2 - .8)\_+
  + 0.12 \cdot \varepsilon \tag{9.21}$$
  The noise standard deviation 0.12 was chosen so that the signal-to-
  noise ratio was about 5. We call this the tensor-product scenario; the
  product term gives a surface that looks like that of Figure 9.11.
* Scenario 2: This is the same as scenario 1, but with p = 20 total predictors;
  that is, there are 18 inputs that are independent of the response.
* Scenario 3: This has the structure of a neural network:
  $$\begin{align}
  l\_1 &= X\_1 + X\_2 + X\_3 + X\_4 + X\_5
  \\\\ l\_2 &= X\_6 - X\_7 + X\_8 - X\_9 + X\_{10}
  \\\\ \sigma(t) &= 1 / (1 + e^{-t})
  \\\\ Y &= \sigma(l\_1) + \sigma(l\_2) + 0.12 \cdot \varepsilon
  \end{align}\tag{9.22}$$

Scenarios 1 and 2 are ideally suited for MARS, while scenario 3 contains
high-order interactions and may be difficult for MARS to approximate. We
ran five simulations from each model, and recorded the results.

In scenario 1, MARS typically uncovered the correct model almost per-
fectly. In scenario 2, it found the correct structure but also found a few
extraneous terms involving other predictors.

Let μ(x) be the true mean of Y , and let

$$\begin{align}
\text{MSE}\_0 &= \text{ave}\_{x \in \text{Test}} (\bar{y} - \mu(x))^2
\text{MSE} &= \text{ave}\_{x \in \text{Test}} (\hat{f}(x) - \mu(x))^2
\end{align}\tag{9.23}$$

These represent the mean-square error of the constant model and the fitted
MARS model, estimated by averaging at the 1000 test values of x. Table 9.4
shows the proportional decrease in model error or R 2 for each scenario:

$$R^2 = \frac{\text{MSE\_0} - \text{MSE}}{\text{MSE}\_0} \tag{9.24}$$

| Scenario | Mean (S.E.) |
|----------|-------------|
| 1: Tensor product p = 2  | 0.97 (0.01) |
| 2: Tensor product p = 20 | 0.96 (0.01) |
| 3: Neural network        | 0.79 (0.01) |

The values shown are means and standard error over the five simulations.
The performance of MARS is degraded only slightly by the inclusion of the
useless inputs in scenario 2; it performs substantially worse in scenario 3.

### 9.4.3 Other Issues

#### MARS for Classification

The MARS method and algorithm can be extended to handle classification
problems. Several strategies have been suggested.

For two classes, one can code the output as 0/1 and treat the problem as
a regression; we did this for the spam example. For more than two classes,
one can use the indicator response approach described in Section 4.2. One
codes the K response classes via 0/1 indicator variables, and then per-
forms a multi-response MARS regression. For the latter we use a common
set of basis functions for all response variables. Classification is made to
the class with the largest predicted response value. There are, however, po-
tential masking problems with this approach, as described in Section 4.2.
A generally superior approach is the “optimal scoring” method discussed
in Section 12.5.

Stone et al. (1997) developed a hybrid of MARS called PolyMARS specif-
ically designed to handle classification problems. It uses the multiple logistic
framework described in Section 4.4. It grows the model in a forward stage-
wise fashion like MARS, but at each stage uses a quadratic approximation
to the multinomial log-likelihood to search for the next basis-function pair.
Once found, the enlarged model is fit by maximum likelihood, and the
process is repeated.

#### Relationship of MARS to CART

Although they might seem quite different, the MARS and CART strategies
actually have strong similarities. Suppose we take the MARS procedure and
make the following changes:

* Replace the piecewise linear basis functions by step functions I(x−t >
  0) and I(x − t ≤ 0).
* When a model term is involved in a multiplication by a candidate
  term, it gets replaced by the interaction, and hence is not available
  for further interactions.

With these changes, the MARS forward procedure is the same as the CART
tree-growing algorithm. Multiplying a step function by a pair of reflected
step functions is equivalent to splitting a node at the step. The second
restriction implies that a node may not be split more than once, and leads
to the attractive binary-tree representation of the CART model. On the
other hand, it is this restriction that makes it difficult for CART to model
additive structures. MARS forgoes the tree structure and gains the ability
to capture additive effects.

#### Mixed Inputs

Mars can handle “mixed” predictors—quantitative and qualitative—in a
natural way, much like CART does. MARS considers all possible binary
partitions of the categories for a qualitative predictor into two groups.
Each such partition generates a pair of piecewise constant basis functions—
indicator functions for the two sets of categories. This basis pair is now
treated as any other, and is used in forming tensor products with other
basis functions already in the model.