+++
title = "ESL-9.5 Hierarchical Mixtures of Experts"
summary = """
统计学习基础（译注）第九章第五节，第 329-332 页。
"""

date = 2019-01-02T11:28:00+08:00
lastmod = 2019-01-02T11:28:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The hierarchical mixtures of experts (HME) procedure can be viewed as a
variant of tree-based methods. The main difference is that the tree splits
are not hard decisions but rather soft probabilistic ones. At each node an
observation goes left or right with probabilities depending on its input val-
ues. This has some computational advantages since the resulting parameter
optimization problem is smooth, unlike the discrete split point search in the
tree-based approach. The soft splits might also help in prediction accuracy
and provide a useful alternative description of the data.

There are other differences between HMEs and the CART implementa-
tion of trees. In an HME, a linear (or logistic regression) model is fit in
each terminal node, instead of a constant as in CART. The splits can be
multiway, not just binary, and the splits are probabilistic functions of a
linear combination of inputs, rather than a single input as in the standard
use of CART. However, the relative merits of these choices are not clear,
and most were discussed at the end of Section 9.2.

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_13.png"
  title="**图9.13**："
>}}

A simple two-level HME model in shown in Figure 9.13. It can be thought
of as a tree with soft splits at each non-terminal node. However, the inven-
tors of this methodology use a different terminology. The terminal nodes
are called experts, and the non-terminal nodes are called gating networks.
The idea is that each expert provides an opinion (prediction) about the
response, and these are combined together by the gating networks. As we
will see, the model is formally a mixture model, and the two-level model
in the figure can be extend to multiple levels, hence the name hierarchical
mixtures of experts.

Consider the regression or classification problem, as described earlier in
the chapter. The data is (x i , y i ), i = 1, 2, . . . , N , with y i either a continuous
or binary-valued response, and x i a vector-valued input. For ease of nota-
tion we assume that the first element of x i is one, to account for intercepts.

Here is how an HME is defined. The top gating network has the output

$$g\_j(x, \gamma\_j) = \frac{e^{\gamma\_j^T x}}
{\sum\_{k=1}^K e^{\gamma\_j^T x}} \tag{9.25}$$

where each γ j is a vector of unknown parameters. This represents a soft
K-way split (K = 2 in Figure 9.13.) Each g j (x, γ j ) is the probability of
assigning an observation with feature vector x to the jth branch. Notice
that with K = 2 groups, if we take the coefficient of one of the elements of
x to be +∞, then we get a logistic curve with infinite slope. In this case,
the gating probabilities are either 0 or 1, corresponding to a hard split on
that input.

At the second level, the gating networks have a similar form:

$$g\_{l|j}(x, \gamma\_{jl}) = \frac{e^{\gamma\_{jl}^T x}}
{\sum\_{k=1}^K e^{\gamma\_{jk}^T x}} l=1,2,\dots,K \tag{9.26}$$

This is the probability of assignment to the lth branch, given assignment
to the jth branch at the level above.

At each expert (terminal node), we have a model for the response variable
of the form

$$ Y \sim \text{Pr}(y|x,\theta\_{jl}) \tag{9.27}$$

This differs according to the problem.

Regression: The Gaussian linear regression model is used, with θ jl =
(β jl , σ jl):

$$Y=\beta\_{jl}^T x + \varepsilon \text{ and }
\varepsilon \sim \mathcal{N}(0, \sigma\_{jl}^2) \tag{9.28}$$

Classification: The linear logistic regression model is used:

$$\text{Pr}(Y=1|x, \theta\_{jl}) = \frac{1}
{1 + e^{-\theta\_{jl}^T x}} \tag{9.29}$$

Denoting the collection of all parameters by Ψ = {γ j , γ jl , θ jl }, the total
probability that Y = y is

$$\text{Pr}(y|x, \Psi) = \sum\_{j=1}^K g\_j(x, \gamma\_j)
\sum\_{l=1}^K g\_{l|j}(x, \gamma\_{jl})\text{Pr}(y|x, \theta\_{jl})
\tag{9.30}$$

This is a mixture model, with the mixture probabilities determined by the
gating network models.

To estimate the parameters, we maximize the log-likelihood of the data,
i log Pr(y i |x i , Ψ), over the parameters in Ψ. The most convenient method
for doing this is the EM algorithm, which we describe for mixtures in
Section 8.5. We define latent variables ∆ j , all of which are zero except for
a single one. We interpret these as the branching decisions made by the top
level gating network. Similarly we define latent variables ∆ l|j to describe
the gating decisions at the second level.

e gating decisions at the second level.
In the E-step, the EM algorithm computes the expectations of the ∆ j
and ∆ l|j given the current values of the parameters. These expectations
are then used as observation weights in the M-step of the procedure, to
estimate the parameters in the expert networks. The parameters in the
internal nodes are estimated by a version of multiple logistic regression.
The expectations of the ∆ j or ∆ l|j are probability profiles, and these are
used as the response vectors for these logistic regressions.

The hierarchical mixtures of experts approach is a promising competitor
to CART trees. By using soft splits rather than hard decision rules it can
capture situations where the transition from low to high response is gradual.
The log-likelihood is a smooth function of the unknown weights and hence
is amenable to numerical optimization. The model is similar to CART with
linear combination splits, but the latter is more difficult to optimize. On
the other hand, to our knowledge there are no methods for finding a good
tree topology for the HME model, as there are in CART. Typically one uses
a fixed tree of some depth, possibly the output of the CART procedure.
The emphasis in the research on HMEs has been on prediction rather than
interpretation of the final model. A close cousin of the HME is the latent
class model (Lin et al., 2000), which typically has only one layer; here
the nodes or latent classes are interpreted as groups of subjects that show
similar response behavior.