+++
title = "ESL-9.5 层级混合专家"
summary = """
统计学习基础（译注）第九章第五节，第 329-332 页。
HME 与 CART 的主要差别是用软性的概率分割代替了硬性的决策分割，
并且在终节点中使用了回归模型而不是一个常数。
HME 对一个平滑的函数求解最优化，但结果不像 CART 可清楚地表达成树结构。
"""

date = 2019-01-16T15:05:00+08:00
lastmod = 2019-01-16T15:05:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++


**层级混合专家**（The hierarchical mixtures of experts，HME）模型
可视为树结构模型的一个变化。
主要的区别是对节点的分割不再是硬性的决定，而是软性的概率。
在每个节点处，一个样本按照由输入变量影响的概率
进入左边或右边的子节点。
与树结构方法中寻找离散分割点不同，
这样产生的是平滑的参数最优化问题，
所以这会带来计算上的优势。
软性的分割也可能有助于改善预测的准确性，
并提供对数据的另一种理解。

HME 与树模型的 CART 实现 之间还存在其他的区别。
HME 的每个终节点中拟合一个线性（或对数几率回归）模型，
而 CART 使用一个常数。
HME 的分割可以是多向的，而不只是二叉分割；
并且分割的形式是输入变量线性组合的概率函数，
而不是如在标准的 CART 中的以一个输入变量进行分割。
然而，并不明确这样的改动是否会带来相对改进，
这在[第 9.2 节]({{< ref "/post/eslii/ch09/ch09_02.zh.md" >}})
的结尾做过讨论。

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_13.png"
  title="**图9.13**：一个二级的层级混合专家（HME）模型。"
>}}

图 9.13 展示了一个简单的二级 HME 模型。
可将其视为在每个非终节点上进行软软性（概率）分割。
不过这个方法的设计者使用了不同的术语。
称终节点为**专家**（experts），称非终节点为**门网络**（gating networks）[^1]。
其想法是每个专家为输出变量给出意见（预测），
并通过门网络将它们结合在一起。
如下可见，这是一个混合模型，并且可将图中的二级模型扩展到多层级，
这也是其名称**层级混合专家**的由来。

考虑一个本章之前讨论过的回归或分类问题。
数据为 $(x\_i, y\_i)$，$i=1,2,\dots,N$，
$y\_i$ 为一个连续或二分类的输出变量，
$x\_i$ 为一个输入变量的向量。
为书写简便，假设 $x\_i$ 的第一个元素为常数一，对应着截距项。

HME 模型的定义如下。
顶部的门网络的输出为：

$$g\_j(x, \gamma\_j) = \frac{e^{\gamma\_j^T x}}
{\sum\_{k=1}^K e^{\gamma\_j^T x}} \tag{9.25}$$

其中每个 $\gamma\_j$ 为一个未知参数向量。
这表达了一个软性的 $K$ 向分割（图 9.13 中 $K=2$）。
每个 $g\_j(x, \gamma\_j)$ 是将一个特征向量为 $x$ 的观测样本
分配到第 $j$ 个分枝的概率。
注意到当有 $K=2$ 个组时，如果 $x$ 中有一个元素的系数为 $+\infty$，
则会得到一个斜率无穷大的对数几率曲线。
这时门网络概率只能取 0 或 1，恰好相当于基于那个输入变量的硬性分割。

在第二级，门网络的形式与上面类似：

$$g\_{l|j}(x, \gamma\_{jl}) = \frac{e^{\gamma\_{jl}^T x}}
{\sum\_{k=1}^K e^{\gamma\_{jk}^T x}} l=1,2,\dots,K \tag{9.26}$$

这是当在上一级分配到第 j 个分枝后，将样本分配到第 l 个分枝的概率。

在每个专家节点（终节点），输出变量的模型形式为：

$$ Y \sim \text{Pr}(y|x,\theta\_{jl}) \tag{9.27}$$

这会根据具体问题不同而采用不同的形式。

* 回归问题：使用高斯线性回归模型，其中
  $\theta\_{jl} = (\beta\_{jl}, \sigma^2\_{jl})$，
  $$Y=\beta\_{jl}^T x + \varepsilon \text{ 并且 }
  \varepsilon \sim \mathcal{N}(0, \sigma\_{jl}^2) \tag{9.28}$$
* 分类问题：使用线性对数几率回归模型，
  $$\text{Pr}(Y=1|x, \theta\_{jl}) = \frac{1}
  {1 + e^{-\theta\_{jl}^T x}} \tag{9.29}$$

将所有参数的集合记为 $\Psi = \\{\gamma\_j, \gamma\_{jl}, \theta\_{jl}\\}$，
则最终 $Y=y$ 的概率为

$$\text{Pr}(y|x, \Psi) = \sum\_{j=1}^K g\_j(x, \gamma\_j)
\sum\_{l=1}^K g\_{l|j}(x, \gamma\_{jl})\text{Pr}(y|x, \theta\_{jl})
\tag{9.30}$$

这是一个混合模型，其混合概率由门网络的模型得出。

为了估计参数值，对 $\Psi$ 中的参数最大化数据的对数似然函数，
即 $\sum\_i\log \text{Pr}(y\_i|x\_i, \Psi)$。
最方便的做法是通过
[第 8.5 节]({{< ref "/post/eslii/ch08/ch08_05.zh.md" >}})
以混合模型为背景介绍的 EM 算法。
定义一组隐变量 $\Delta\_j$，其中只有一个为一，其他都为零。
这些变量代表了顶部的门网络给出的分枝决策。
类似地定义若干组隐变量 $\Delta\_{l|j}$ 为二级门网络的分枝决策。

EM 算法在期望（E)步骤计算给定了当前参数取值后
$\Delta\_j$ 和 $\Delta\_{l|j}$ 的期望。
然后在最大化（M）步骤中估计专家网络中参数时，
将这些期望作为样本的权重。
内部节点的参数是通过一种多个对数几率回归的方法估计出的。
$\Delta\_j$ 或 $\Delta\_{l|j}$ 的期望概括了其概率分布，
并被作为这些对数几率回归的输出变量向量。

层级混合专家方法有望成为 CART 树模型的一个有力竞争者。
用软性分割代替硬性决策规则，使它可以捕捉输出变量从低到高存在逐渐过渡的效应。
对数似然函数是未知权重的一个平滑函数，
因此更方便进行数值优化。
这个模型相似于基于线性结合分割的 CART，
但后者相对更难以求解最优化。
但另一方面来说，
就作者所知目前还没有为 HME 模型构建一个类似
CART 的良好树状拓扑结构的方法。
通常会使用一个特定深度（可能由 CART 得出的结果）的固定的树结构。
对 HME 模型的研究更强调它的预测上而不太考虑对最终模型的解读。
**潜类别模型**（latent class model，Lin et al., 2000）
是一个与 HME 关系很近的方法，
其通常只有一个层级；
其中的节点或潜类别代表的是展示出相似的输出变量表现的一组样本。

[^1]: 参考神经网路中的门函数（gating functions）。