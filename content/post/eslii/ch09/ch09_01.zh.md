+++
title = "ESL-9.1 Generalized Additive Models"
summary = """
"""

date = 2018-07-16T11:52:07+08:00
lastmod = 2018-08-11T15:05:10+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Regression models play an important role in many data analyses, providing
prediction and classification rules, and data analytic tools for understanding
the importance of different inputs.

Although attractively simple, the traditional linear model often fails in
these situations: in real life, effects are often not linear. In earlier chapters
we described techniques that used predefined basis functions to achieve
nonlinearities. This section describes more automatic flexible statistical
methods that may be used to identify and characterize nonlinear regression
effects. These methods are called “generalized additive models.”

In the regression setting, a generalized additive model has the form
$$E(Y|X\_1, X\_2, \cdots, X\_p) =
\alpha + f\_1(X\_1) + f\_2(X\_2) + \cdots + f\_p(X\_p) \tag{9.1}$$
As usual $X\_1 , X\_2 , \cdots , X\_p$ represent predictors and $Y$ is the outcome;
the $f\_j$’s are unspecified smooth (“nonparametric”) functions. If we were to model
each function using an expansion of basis functions (as in Chapter 5), the
resulting model could then be fit by simple least squares. Our approach
here is different: we fit each function using a scatterplot smoother (e.g., a
cubic smoothing spline or kernel smoother), and provide an algorithm for
simultaneously estimating all $p$ functions (Section 9.1.1).

For two-class classification, recall the logistic regression model for binary
data discussed in Section 4.4. We relate the mean of the binary response
$\mu(X) = Pr(Y = 1|X)$ to the predictors via a linear regression model and
the logit link function:
$$\log\left(\frac{\mu(X)}{1-\mu(X)}\right)
= \alpha + \beta\_1 X\_1 + \cdots + \beta\_p X\_p
\tag{9.2}$$

The additive logistic regression model replaces each linear term by a more
general functional form
$$\log\left(\frac{\mu(X)}{1-\mu(X)}\right)
= f\_1(X\_1) + f\_2(X\_2) + \cdots + f\_p(X\_p)
\tag{9.3}$$
where again each $f\_j$ is an unspecified smooth function. While the non-
parametric form for the functions $f\_j$ makes the model more flexible, the
additivity is retained and allows us to interpret the model in much the
same way as before. The additive logistic regression model is an example
of a generalized additive model. In general, the conditional mean $\mu(X)$ of
a response $Y$ is related to an additive function of the predictors via a link
function $g$:
$$g[\mu(X)] = \alpha + f\_1(X\_1) + \cdots + f\_p(X\_p) \tag{9.4}$$
Examples of classical link functions are the following:

* $g(\mu) = \mu$ is the identity link, used for linear and additive models for Gaussian response data.
* $g(\mu) = logit(\mu)$ as above, or g(μ) = probit(μ), the probit link function, for modeling binomial probabilities. The probit function is the inverse Gaussian cumulative distribution function: $probit(\mu) = \Phi^{−1}(\mu)$.
* $g(\mu) = \log(\mu)$ for log-linear or log-additive models for Poisson count data.

All three of these arise from exponential family sampling models, which
in addition include the gamma and negative-binomial distributions. These
families generate the well-known class of generalized linear models, which
are all extended in the same way to generalized additive models.

The functions $f\_j$ are estimated in a flexible manner, using an algorithm
whose basic building block is a scatterplot smoother. The estimated function
\hat{f}\_j can then reveal possible nonlinearities in the effect of $X\_j$. Not all
of the functions $f\_j$ need to be nonlinear. We can easily mix in linear and
other parametric forms with the nonlinear terms, a necessity when some of
the inputs are qualitative variables (factors). The nonlinear terms are not
restricted to main effects either; we can have nonlinear components in two
or more variables, or separate curves in $X\_j$ for each level of the factor $X\_k$.
Thus each of the following would qualify:

* $g(\mu) = X^T \beta + \alpha\_k + f(Z)$, a semiparametric model, where $X$ is a vector of predictors to be modeled linearly, α $k$ the effect for the kth level of a qualitative input $V$ , and the effect of predictor $Z$ is modeled nonparametrically.
* $g(\mu) = f(X) + g\_k(Z)$, again $k$ indexes the levels of a qualitative input $V$ , and thus creates an interaction term $g(V, Z) = g\_k(Z)$ for the effect of $V$ and $Z$.
* $g(\mu) = f(X) + g(Z, W)$ where g is a nonparametric function in two features.

Additive models can replace linear models in a wide variety of settings,
for example an additive decomposition of time series,
$$Y\_t = S\_t + T\_t + \varepsilon\_t \tag{9.5}$$
where $S\_t$ is a seasonal component, $T\_t$ is a trend and $\varepsilon\_t$ is an error term.
