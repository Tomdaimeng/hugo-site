+++
title = "ESL-9.1 广义加性模型"
summary = """
统计学习基础（译注）第九章第一节，第 295-304 页。
广义加性模型是对线性模型的一个有效的推广，
在保持了可解释性的同时，可纳入一些非线性的效果。
"""

date = 2018-12-30T19:02:00+08:00
lastmod = 2018-12-30T19:02:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

回归模型在很多数据分析中扮演着重要的角色，
它给出了预测和分类规则，
并为理解不同输入变量的重要程度提供了数据分析工具。

传统的线性模型尽管非常简洁，但通常会在这些分析中失效：
在现实生活中，作用影响往往不是线性的。
之前的章节介绍的方法通过使用预先定义的基函数来达到非线性关系。
本节介绍可用于识别和刻画非线性回归关系的更自动和灵活的统计方法。
这些方法被称为**广义加性模型**（generalized additive model）。

在回归问题中，广义加性模型的形式可写为：

$$E(Y|X\_1, X\_2, \cdots, X\_p) =
\alpha + f\_1(X\_1) + f\_2(X\_2) + \cdots + f\_p(X\_p) \tag{9.1}$$

同以往一样令 $X\_1$，$X\_2$，……，$X\_p$ 代表自变量，
$Y$ 代表输出变量；
$f\_j$ 代表一些未指定的平滑（非参数）函数。
如果利用基函数的展开式为每个函数建模（如在第五章中），
则可以用简单的最小二乘来拟合得到的模型。
这里的方法与之不同：
先用散点图平滑器（例如三次平滑样条或核平滑器）拟合每个函数，
然后给出同时估计所有 $p$ 个函数的算法（第 9.1.1 节）。

回顾[第 4.4 节]({{< ref "/post/eslii/ch04/ch04_04.zh.md" >}})
中二分类问题的对数几率回归模型。
其中将二元输出变量的均值 $\mu(X) = Pr(Y = 1|X)$ 与自变量的关系表达为
线性回归模型和对数几率（logit）**联系函数**（link function）：

$$\log\left(\frac{\mu(X)}{1-\mu(X)}\right)
= \alpha + \beta\_1 X\_1 + \cdots + \beta\_p X\_p
\tag{9.2}$$

**加性**对数几率回归模型将每个线性项替换为更一般性的函数形式：

$$\log\left(\frac{\mu(X)}{1-\mu(X)}\right)
= f\_1(X\_1) + f\_2(X\_2) + \cdots + f\_p(X\_p)
\tag{9.3}$$

其中每个 $f\_j$ 同样是未指定的平滑函数。
非参数形式的函数 $f\_j$ 使模型更灵活，
同时也保持了“加性”从而可以跟以往大致相同的方式来理解模型。
加性对数几率回归模型是广义加性模型的一个例子。
一般来说，一个输出变量 $Y$ 的条件均值 $\mu(X)$
与自变量的加性函数之间的关系表达为**联系函数**（link function）$g$：

$$g[\mu(X)] = \alpha + f\_1(X\_1) + \cdots + f\_p(X\_p) \tag{9.4}$$

以下为一些经典的联系函数的例子：

* 恒等联系函数 $g(\mu) = \mu$，用于高斯输出变量的线性和加性模型。
* 上述的 $g(\mu) = \text{logit}(\mu)$，
  或 **概率单位**（probit）联系函数 $g(\mu) = \text{probit}(\mu)$，
  用于二项分布概率模型。
  概率单位函数是高斯累积分布函数的反函数：
  $\text{probit}(\mu) = \Phi^{-1}(\mu)$。
* $g(\mu) = \log(\mu)$，用于泊松计数样本的对数线性或对数加性模型。

这三个都属于指数族的抽样概率模型，
另外其中还包括伽玛（gamma）分布和负二项分布。
这族概率分布形成了熟悉的广义线性模型，
而也都可以用同样的方式拓展到广义加性模型。

函数 $f\_j$ 的估计方法比较灵活，
其使用的算法是以散点图平滑器为基础建立的。
那么函数估计 $\hat{f}\_j$ 可体现出在 $X\_j$ 中的非线性关系。
并不是所有的函数 $f\_j$ 都需要是非线性的。
广义加性模型可以方便地将线性和其他的参数形式与非线性项结合在一起，
这保证了模型可适用于部分输入变量为定性类型（因子，factor）的场景中。
非线性关系也不局限于输入变量对应的函数 $f\_j$ 上；
也可能存在两个或更多变量的（交叉）非线性成分，
或表现为因子 $X\_k$ 取不同类别时 $X\_j$ 采用不同的函数曲线。[^1]
因此以下描述的都属于非线性关系：

* $g(\mu) = X^T \beta + \alpha\_k + f(Z)$  
  半参数（semiparametric）模型，其中的 $X$ 为在模型中呈线性的自变量向量，
  $\alpha\_k$ 为分类输入变量 $V$ 取值第 k 类别时对结果的影响，
  而自变量 $Z$ 的影响以非参数的形式进入模型。
* $g(\mu) = f(X) + g\_k(Z)$  
  其中的 $k$ 同样代表了分类输入变量 $V$ 的取值类别，
  因此产生了一个 $V$ 和 $Z$ 影响的交叉项 $g(V, Z) = g\_k(Z)$。
* $g(\mu) = f(X) + g(Z, W)$  
  其中的 $g$ 为两个特征变量的非参数函数。

加性模型在很多种场景中可以替换线性模型，
例如可对一个时间序列进行加性分解：

$$Y\_t = S\_t + T\_t + \varepsilon\_t \tag{9.5}$$

其中的 $S\_t$ 为周期成分，$T\_t$ 为趋势项，$\varepsilon\_t$ 为误差项。

### 9.1.1 拟合加性模型

本节介绍拟合拟合加性模型及其推广模型的一个模块化的算法。
算法的基础模块是灵活地拟合非线性影响的散点图平滑器。
具体来说，这里用第五章中介绍的三次平滑样条作为散点图平滑器。
加性模型的表达式为：

$$Y = \alpha + \sum\_{j=1}^p f\_j(X\_j) + \varepsilon \tag{9.6}$$

其中的误差项 $\varepsilon$ 均值为零。
给定观测样本 $(x\_i,y\_i)$，
可以为这个问题指定一个类似于
[第 5.4 节]({{< ref "/post/eslii/ch05/ch05_04.zh.md" >}})
中的惩罚残差平方和（等式 5.9）的准则：

$$\begin{align}
&\text{PRSS}(\alpha, f\_1, f\_2, \dots, f\_p) = \\\\ &
\sum\_{i=1}^N \left(y\_i - \alpha - \sum\_{j=1}^p f\_j(x\_{ij})\right)^2
+ \sum\_{j=1}^p \lambda\_j \int f\_j^{\prime\prime}(t\_j)^2 dt\_j
\end{align}\tag{9.7}$$

其中的 $\lambda\_j \geq 0$ 为调节参数。
可证明表达式 9.7 的最小值解是一个加性三次样条模型；
每个函数 $f\_j$ 都是对应的 $X\_j$ 的三次样条，
其结点为 $x\_{ij}$ 的唯一值，$i=1,\dots,N$。
然而在不对模型做进一步假设时，这个解不唯一。
常数 $\alpha$ 就无法识别，
因为可以对每个函数 $f\_j$ 加减任意常数，并相应地调整 $\alpha$。
标准惯例是令 $sum\_{i=1}^N f(x\_{ij})=0 \forall j$，
即函数在数据样本上的平均为零。
这个惯例下可见 $\hat{\alpha}=\text{ave}(y\_i)$。
如果在这个约束下再要求输入变量的矩阵（第 ij 个元素为 $x\_{ij}$）为满列秩矩阵，
那么表达式 8.7 是严格凸函数准则，并且有唯一的最小值解。
如果这个矩阵是一个奇异（singular）矩阵，
那么无法唯一确定函数成分 $f\_j$ 中的线性部分
（但仍然可确定其非线性部分）（Buja et al., 1989）。

此外，存在一个求解的简单迭代过程。
令 $\hat{\alpha} = \text{ave}(y\_i)$，并固定不变。
对目标变量
$\\{y\_i-\hat{\alpha}-\sum\_{k\ne j}\hat{f}\_k(x\_{ik})\\}\_1^N$，
使用作为 $x\_{ij}$ 函数的三次平滑样条 $\mathcal{S}\_j$。
轮流对每个自变量进行这个操作，
并在计算
$y\_i - \hat{\alpha} - \sum\_{k\ne j}\hat{f}\_k(x\_{ik})\\}\_1^N$
时使用其他函数 $\hat{f}\_k$ 当前的估计。
持续这个过程，直到函数估计 $\hat{f}\_j$ 达到稳定。
算法 9.1 中给出了这个过程的细节，
也称为**回修**（backfitting），
并且得出的拟合可类比线性模型中的多元回归[^2]。
在理论上，算法 9.1 中第 2 步的第二个对 $\hat{f}\_j$ 的更新没有必要，
因为对零均值输出变量拟合的平滑样条，其期望也为零（练习 9.1）。
在实践中，运算中的近似有可能造成结果的偏移，
所以建议进行相应调整。

----------

#### 算法 9.1：加性模型的回修（backfitting）算法

1. 初始化：$\hat{\alpha} = \frac{1}{N}\sum\_{i=1}^N y\_i$，
   $\hat{f}\_j \equiv 0$，$\forall i,j$。
2. 循环迭代: $j=1,2,\dots,p,\dots,1,2,\dots,p,\dots$,
   $$\begin{align}
   & \hat{f}\_j\leftarrow \mathcal{S}\_j \left[
   \\{y\_i - \hat{\alpha} - \sum\_{k\ne j}\hat{f}\_k(x\_{ik})\\}\_1^N
   \right] \\\\ &
   \hat{f}\_j\leftarrow
   \hat{f}\_j - \frac{1}{N}\sum\_{i=1}^N \hat{f}\_j(x\_{ij})
   \end{align}$$
   直到函数 $\hat{f}\_j$ 的变动小于某个预设的阈值。

----------

这个算法也可以完全相同的方式应用于其他拟合方法，
只需要指定合适的平滑运算 $\mathcal{S}\_j$：

* 其他单变量回归平滑器，比如局部多项回归和核方法；
* 可以产生多项式拟合、分段常数拟合、参数样条拟合、序列和傅立叶拟合的线性回归运算；
* 其他更复杂的运算，比如捕捉二次或更高次交叉作用的曲面平滑器，
  或捕捉周期性作用的周期平滑器。

如果只是在训练集上考虑平滑器运算 $\mathcal{S}\_j$，
则它可表达为一个 $N \times N$ 的算子矩阵
（见[第 5.4.1 节]({{< ref "/post/eslii/ch05/ch05_04.zh.md" >}})）。
那么与第五和第六章讨论的平滑器的自由度相类比，
第 j 项的自由度可（近似地）计算为
$\text{df}\_j=\text{trace}[\mathbf{S}\_j] - 1$。

对很多类型的线性平滑器 $\mathbf{S}\_j$，
回修算法在求解一个线性方程组时与高斯-赛德尔（Gauss-Seidel）算法是等价的。
练习 9.2 给出了相关细节。

对于对数几率回归模型和其他部分广义加性模型而言，
惩罚对数似然度是一个合理的准则。
在对其最大化中，同时使用了回修过程和似然函数最大化方法。
可将常规的最大化广义线性模型的对数似然函数的
牛顿-拉弗森（Newton-Raphson）方法重改为一个
**迭代重加权最小二乘**算法
（iteratively reweighted least squares，IRLS）。
这个过程是重复地进行当前输出变量对协变量（covariate）的加权线性回归；
每个回归会产生新的参数估计值，这反过来也会得出新的当前输出变量和权重，
并迭代这个过程
（见[第 4.4.1 节]({{< ref "/post/eslii/ch04/ch04_04.zh.md" >}})）。
在广义加性模型中，就是简单地将其中的加权线性回归替换成加权回修算法。
下面更详细地介绍这个算法在对数几率回归中的应用，
并且在 Hastie and Tiebshirani (1990) 的第六章有更一般性地介绍。

### 9.1.2 示例：加性对数几率回归

二分类数据的对数几率模型可能是医学研究中使用最广泛的模型。
这个模型中的输出变量 $Y$ 可被编码为 0 或 1，
用 1 表示一个事件（如死亡或疾病复发）的发生，
而用 0 表示没有事件发生。
我们要在给定的预后（prognostic）因子的取值上
建立一个事件发生概率 $\text{Pr}(Y=1|X)$ 的模型。
通常的目标是为了理解这些预后因子的作用，
而不是对新个体进行分类。
对数几率回归也适用于估计类别概率从而筛查风险的应用中。
除医学应用外，模型也在信用风险识别里普遍使用。

广义加性对数几率模型的表达式为：

$$\log\frac{\text{Pr}(Y=1|X)}{\text{Pr}(Y=0|X)} =
\alpha + f\_1(X\_1) + \dots + f\_p(X\_p) \tag{9.8}$$

函数 $f\_1$，$f\_2$，……，$f\_p$ 在
牛顿-拉弗森（Newton–Raphson）过程中通过回修（backfitting）算法估计出，
详见算法 9.2。

----------

#### 算法 9.2 加性对数几率回归模型的局部分数（scoring）算法

1. 计算初始值：$\hat{\alpha} = \log[\bar{y} / (1 − \bar{y})]$,
   其中的 $\bar{y} = \text{ave}(y\_i)$，
   即样本中类别一的比例，
   并设置函数 $\hat{f}\_j \equiv 0 \forall j$。
2. 定义 $\hat{\eta}\_i = \hat{\alpha} + \sum\_j \hat{f}\_j(x\_{ij})$
   以及 $\hat{\rho}\_i = 1 / [1 + \exp(−\hat{\eta}\_ i)]$。
   进行迭代：
   1. 构建当前目标变量：
      $$z\_i = \hat{\eta} +
      \frac{(y\_i - \hat{p}\_i)}{\hat{p}\_i(1-\hat{p}\_i)}$$
   2. 构建权重 $w\_i = \hat{p}\_i (1-\hat{p}\_i)$。
   3. 通过一个加权回修算法，
      对目标变量 $z\_i$ 和权重 $w\_i$ 拟合一个加性模型。
      结果得出新的估计 $\hat{\alpha}$，$\hat{f}\_j$，$\forall j$。
3. 持续步骤 2，直到函数的变动小于某个预设的阈值。

----------

算法 9.2 的步骤 2.2 中的加性模型拟合需要一个加权的散点图平滑器。
大多的平滑过程都适用于带权重的观测样本（练习 5.12）；
Hastie and Tibshirani (1990) 的第三章提供了更多细节。

利用[第 4.4 节]({{< ref "/post/eslii/ch04/ch04_04.zh.md" >}})
中简述的多项对数几率（multilogit）表达式，
可将加性对数几率回归模型进一步推广到两个以上类别的问题中。
尽管其表达式是对表达式 9.8 的直接推广，
但拟合这个模型的算法要更复杂。
更多细节见 Yee and Wild (1996)；
另外软件 `VGAM` 可见下面的网站：

http://www.stat.auckland.ac.nz/∼yee.

#### 例：预测垃圾邮件

我们在第一章中介绍的垃圾邮件数据上应用广义加性模型。
数据为 4601 个电子邮件中的信息，
来自于一个筛查垃圾（spam or junk）邮件的研究。
数据可从 ftp.ics.uci.edu 公开获取，
数据由加利福尼亚州（California）帕洛阿尔托市（Palo Alto）
惠普（Hewlett-Packard）实验室的 George Forman 捐赠。

输出变量是二分类，取值为“正常邮件”或“垃圾邮件”，
并且有 57 个自变量：

* 48 个数值自变量——某个单词在一封邮件所有单词中所占的百分比。
  例如 `business`、`address`、`internet`、`free`和`george`。
  这是考虑到这些单词可能可以区分不同的用户。
* 6 个数值自变量——某个符号在一封邮件所有符号中所占的百分比。
  这几个符号为 `ch;`、`ch(`、`ch[`、`ch!`、`ch$` 和 `ch#`。
* 连续无间断的大写字母字符串的平均长度：`CAPAVE`。
* 连续无间断的大写字母字符串的平均长度：`CAPMAX`。
* 连续无间断的大写字母字符串的长度之和：`CAPTOT`。

将“垃圾邮件”编码为 1，“正常邮件”编码为 0。
随机选出 1536 个样本作为测试集，
留在训练集中的观测样本有 3065 个。
在拟合广义加性模型中，使用了三次平滑样条，
每个自变量有四个名义的（nominal）自由度。
也就是说对每个自变量 $X\_j$，
选择使得 $\text{trace}[\mathbf{S}\_j(\lambda\_j)]-1 = 4$ 的
平滑样条参数 $\lambda\_j$，
其中的 $\mathbf{S}\_j(\lambda)$ 为观测值 $x\_{ij}$，$i=1,\dots,N$
构建的平滑样条算子矩阵。
这是一个在这样复杂模型中设定平滑程度的一个简便的方法。

大部分自变量的分布都有很重的长尾（long-tailed）。
在拟合广义加性模型前，先对每个变量进行了对数转换
（实际的转换是 $\log(x+0.1)$），
但图 9.1 中的曲线仍是对原始变量的函数。


|             | 预测类别     |             |
|-------------|-------------|-------------|
| **实际类别** | 正常邮件（0） | 垃圾邮件（1） |
| 正常邮件（0） | 58.3%       | 2.5%        |
| 垃圾邮件（1） | 3.0%        | 36.3%       |
**表9.1**：用垃圾邮件训练数据拟合的加性对数几率回归模型，
在测试集上的混淆（confusion）矩阵。
总体测试误差率为 5.5%。

表 9.1 给出了测试误差率；总体的误差率为 5.3%。
与之相比，线性对数几率回归的测试误差率为 7.6%。
表 9.1 给出了加性模型中最显著的一些自变量。

| 变量名 | 序号 | 自由度 | 系数 | 标准误差 | Z 分数 | 非线性 P 值 |
|-------|-----|-------|-----|---------|-------|------------|
| **正影响** | | | | | | |
| our      | 5  | 3.9 | 0.566       | 0.114      | 4.970   | 0.052 |
| over     | 6  | 3.9 | 0.244       | 0.195      | 1.249   | 0.004 |
| remove   | 7  | 4.0 | 0.949       | 0.183      | 5.201   | 0.093 |
| internet | 8  | 4.0 | 0.524       | 0.176      | 2.974   | 0.028 |
| free     | 16 | 3.9 | 0.507       | 0.127      | 4.010   | 0.065 |
| business | 17 | 3.8 | 0.779       | 0.186      | 4.179   | 0.194 |
| hpl      | 26 | 3.8 | 0.045       | 0.250      | 0.181   | 0.002 |
| ch!      | 52 | 4.0 | 0.674       | 0.128      | 5.283   | 0.164 |
| ch$      | 53 | 3.9 | 1.419       | 0.280      | 5.062   | 0.354 |
| CAPMAX   | 56 | 3.8 | 0.247       | 0.228      | 1.080   | 0.000 |
| CAPTOT   | 57 | 4.0 | 0.755       | 0.165      | 4.566   | 0.063 |
| **负影响** | | | | | | |
| hp       | 25 | 3.9 | −1.404      | 0.224      | −6.262  | 0.140 |
| george   | 27 | 3.7 | −5.003      | 0.744      | −6.722  | 0.045 |
| 1999     | 37 | 3.8 | −0.672      | 0.191      | −3.512  | 0.011 |
| re       | 45 | 3.9 | −0.620      | 0.133      | −4.649  | 0.597 |
| edu      | 46 | 4.0 | −1.183      | 0.209      | −5.647  | 0.000 |
**表9.2**：用垃圾邮件训练数据拟合的加性模型中的显著自变量。
系数、标准误差和 Z 评分代表的是 $\hat{f}\_j$ 的线性部分。
非线性 P 值来自对 $\hat{f}\_j$ 的非线性检验。

为了易于理解，表 9.2 中每个变量的贡献被分解为线性成分和剩余的非线性成分。
上半区为与垃圾邮件有正相关性的自变量，
而下半区为有负相关性的自变量。
线性的成分是来自拟合的曲线对自变量的加权最小二乘线性拟合，
而非线性的成分就是其残差。
系数、标准误差和 Z 分数概括了估计函数的线性成分的信息；
Z 分数是系数与其标准误差的比值，
若它超出某个标准正太分布的分位数则认为对应变量是显著的。
“非线性 P 值”一列是对估计函数的非线性检验结果。
然而需要注意的是，
每个自变量的影响中已经排除了其他自变量整体的影响，
而不仅仅是它们影响中线性的部分。
表中展示的为至少有一个（线性或非线性）检验中
在（双边） $p=0.01$ 水平上呈显著的自变量。

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_01.png"
  title="**图9.1**：垃圾邮件分析：显著自变量的函数估计。每个曲线下的地毯图代表了对应自变量的观测值分布。对大部分自变量来说，非线性捕捉到了在零点附近的不连续性。"
>}}

图 9.1 展示了表 9.2 中出现的显著自变量的函数估计。
大部分的非线性影响看起来是由于在零点处的不连续性。
例如，随着 `george` 的频率从零点增加，垃圾邮件的概率显著地下降，
但之后却几乎不再变动。
据此可尝试将每个频率自变量替换为零出现的指示变量，
然后实施线性对数几率模型。
这样得到的测试误差率为 7.4%；
同时加入频率的线性作用后，测试误差率降为 6.6%。
这样看来，加性模型中的非线性成分可提供额外的预测能力。

相比之下将一个正常邮件分类为垃圾邮件是后果更严重的错误，
因为这样会使一封正常的邮件被过滤掉而无法发送给用户。
可以通过改变损失函数来改变不同分类误差的轻重程度。
如果将真实类别为 0 的样本预测为 1 的损失设为 $L\_{01}$，
将真实类别为 1 的样本预测为 0 的损失设为 $L\_{10}$，
那么根据估计的贝叶斯规则，
当类别 1 的概率大于 $L\_{01} / (L\_{01}+L\_{10})$ 时分类为类别 1。
例如，如果令 $L\_{01} = 10$ 和 $L\_{10}=1$，
则（实际的）类别 0 和 类别 1 的误差率变为 0.8% 和 8.7%。

更进一步，为了促使模型对类别 0 这部分数据的拟合结果更好，
可以对类别 0 的观测值使用权重 $L\_{01}$
并对类别 1 的观测值使用权重 $L\_{10}$。
再用同上的估计贝叶斯规则进行预测。
这样得到的（实际的）类别 0 和 类别 1 的误差率分别为 1.2% 和 8.0%。
后续会在树结构（tree-based）模型的场景中会进一步讨论不相等损失的话题。

在拟合了加性模型后，需要核查是否加入某些交叉项可能会显著地改善拟合。
可通过手动的方式进行，即加入某些或全部显著输入变量的乘积；
或者可通过 MARS 过程自动地完成（第 9.4 节）。

这个例子以自动化的方式应用加性模型。
而作为一个数据分析工具，
加性模型常常会以更交互的方式使用，
即逐步增加或去除某些项来查看它们的作用。
另外通过调整 $\text{df}\_j$ 从而调节平滑程度，
可以使拟合结果在线性模型（$\text{df}\_j=1$）与
部分线性模型（让某些项有更灵活的拟合）之间无缝地切换。
更多细节见 Hastie and Tibshirani (1990)。

### 9.1.3 总结

加性模型是对线性模型的一个有效的推广，
使模型更加灵活却仍基本上保持了其可解释性。
线性模型中熟悉的建模和推断的工具仍然适用于加性模型，
可参考表 9.2 的例子。
拟合这些模型的回修过程简便而且模块化，
即可为每个输入变量选择相应合适的拟合方法。
因此加性模型在统计学中被广泛地使用。

然而加性模型在大型的数据挖掘应用中可能存在缺陷。
回修算法会拟合所有的自变量，
当存在很多自变量时，这个算法不太理想甚至不再可行。
BRUTO 过程（Hastie and Tibshirani, 1990，第九章）
在回修算法中加入了变量选择，
但其设计并没有考虑大型的数据挖掘问题。
近期也有一些用套索（lasso）类型的惩罚项来估计稀疏加性模型的研究，
例如 Lin and Zhang (2006) 的 COSSO 过程
以及 Ravikumar et al. (2008) 提出的 SpAM 方法。
对大型的问题，
一个诸如提升方法（第十章）的前向分段方法更有效，
并且同时可在模型中加入交叉项。

[^1]: 译者在本段中加入了自己的理解，所以与原文的差异相对较大，可参考原书 296 页最后一段（至 297 页）。
[^2]: 原文为“multiple regression”。字面上是“多次回归”，但在之前的章节中并没有类似多次回归的迭代算法。