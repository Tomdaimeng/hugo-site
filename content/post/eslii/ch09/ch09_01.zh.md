+++
title = "ESL-9.1 广义加性模型"
summary = """
统计学习基础（译注）第九章第一节，第 295-304 页。
"""

date = 2018-12-28T10:15:00+08:00
lastmod = 2018-12-28T10:15:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Regression models play an important role in many data analyses, providing
prediction and classification rules, and data analytic tools for understanding
the importance of different inputs.

Although attractively simple, the traditional linear model often fails in
these situations: in real life, effects are often not linear. In earlier chapters
we described techniques that used predefined basis functions to achieve
nonlinearities. This section describes more automatic flexible statistical
methods that may be used to identify and characterize nonlinear regression
effects. These methods are called “generalized additive models.”

In the regression setting, a generalized additive model has the form

$$E(Y|X\_1, X\_2, \cdots, X\_p) =
\alpha + f\_1(X\_1) + f\_2(X\_2) + \cdots + f\_p(X\_p) \tag{9.1}$$

As usual $X\_1 , X\_2 , \cdots , X\_p$ represent predictors and $Y$ is the outcome;
the $f\_j$’s are unspecified smooth (“nonparametric”) functions. If we were to model
each function using an expansion of basis functions (as in Chapter 5), the
resulting model could then be fit by simple least squares. Our approach
here is different: we fit each function using a scatterplot smoother (e.g., a
cubic smoothing spline or kernel smoother), and provide an algorithm for
simultaneously estimating all $p$ functions (Section 9.1.1).

For two-class classification, recall the logistic regression model for binary
data discussed in Section 4.4. We relate the mean of the binary response
$\mu(X) = Pr(Y = 1|X)$ to the predictors via a linear regression model and
the logit link function:

$$\log\left(\frac{\mu(X)}{1-\mu(X)}\right)
= \alpha + \beta\_1 X\_1 + \cdots + \beta\_p X\_p
\tag{9.2}$$

The additive logistic regression model replaces each linear term by a more
general functional form

$$\log\left(\frac{\mu(X)}{1-\mu(X)}\right)
= f\_1(X\_1) + f\_2(X\_2) + \cdots + f\_p(X\_p)
\tag{9.3}$$

where again each $f\_j$ is an unspecified smooth function. While the non-
parametric form for the functions $f\_j$ makes the model more flexible, the
additivity is retained and allows us to interpret the model in much the
same way as before. The additive logistic regression model is an example
of a generalized additive model. In general, the conditional mean $\mu(X)$ of
a response $Y$ is related to an additive function of the predictors via a link
function $g$:

$$g[\mu(X)] = \alpha + f\_1(X\_1) + \cdots + f\_p(X\_p) \tag{9.4}$$

Examples of classical link functions are the following:

* $g(\mu) = \mu$ is the identity link, used for linear and additive models for Gaussian response data.
* $g(\mu) = logit(\mu)$ as above, or g(μ) = probit(μ), the probit link function, for modeling binomial probabilities. The probit function is the inverse Gaussian cumulative distribution function: $probit(\mu) = \Phi^{−1}(\mu)$.
* $g(\mu) = \log(\mu)$ for log-linear or log-additive models for Poisson count data.

All three of these arise from exponential family sampling models, which
in addition include the gamma and negative-binomial distributions. These
families generate the well-known class of generalized linear models, which
are all extended in the same way to generalized additive models.

The functions $f\_j$ are estimated in a flexible manner, using an algorithm
whose basic building block is a scatterplot smoother. The estimated function
\hat{f}\_j can then reveal possible nonlinearities in the effect of $X\_j$. Not all
of the functions $f\_j$ need to be nonlinear. We can easily mix in linear and
other parametric forms with the nonlinear terms, a necessity when some of
the inputs are qualitative variables (factors). The nonlinear terms are not
restricted to main effects either; we can have nonlinear components in two
or more variables, or separate curves in $X\_j$ for each level of the factor $X\_k$.
Thus each of the following would qualify:

* $g(\mu) = X^T \beta + \alpha\_k + f(Z)$, a semiparametric model, where $X$ is a vector of predictors to be modeled linearly, α $k$ the effect for the kth level of a qualitative input $V$ , and the effect of predictor $Z$ is modeled nonparametrically.
* $g(\mu) = f(X) + g\_k(Z)$, again $k$ indexes the levels of a qualitative input $V$ , and thus creates an interaction term $g(V, Z) = g\_k(Z)$ for the effect of $V$ and $Z$.
* $g(\mu) = f(X) + g(Z, W)$ where g is a nonparametric function in two features.

Additive models can replace linear models in a wide variety of settings,
for example an additive decomposition of time series,

$$Y\_t = S\_t + T\_t + \varepsilon\_t \tag{9.5}$$

where $S\_t$ is a seasonal component, $T\_t$ is a trend and $\varepsilon\_t$ is an error term.

### 9.1.1 Fitting Additive Models

In this section we describe a modular algorithm for fitting additive models
and their generalizations. The building block is the scatterplot smoother
for fitting nonlinear effects in a flexible way. For concreteness we use as our
scatterplot smoother the cubic smoothing spline described in Chapter 5.
The additive model has the form

$$Y = \alpha + \sum\_{j=1}^p f\_j(X\_j) + \varepsilon \tag{9.6}$$

where the error term ε has mean zero. Given observations x i , y i , a criterion
like the penalized sum of squares (5.9) of Section 5.4 can be specified for
this problem,

$$\begin{align}
&\text{PRSS}(\alpha, f\_1, f\_2, \dots, f\_p) = \\\\ &
\sum\_{i=1}^N \left(y\_i - \alpha - \sum\_{j=1}^p f\_j(x\_{ij})\right)^2
+ \sum\_{j=1}^p \lambda\_j \int f\_j^{\prime\prime}(t\_j)^2 dt\_j
\end{align}\tag{9.7}$$

where the λ j ≥ 0 are tuning parameters. It can be shown that the minimizer
of (9.7) is an additive cubic spline model; each of the functions f j is a
cubic spline in the component X j , with knots at each of the unique values
of x ij , i = 1, . . . , N . However, without further restrictions on the model,
the solution is not unique. The constant α is not identifiable, since we
can add or subtract any constants to each of the functions f j , and adjust
α accordingly. The standard convention is to assume that 1 f j (x ij ) =
0 ∀j—the functions average zero over the data. It is easily seen that α̂ =
ave(y i ) in this case. If in addition to this restriction, the matrix of input
values (having ijth entry x ij ) has full column rank, then (9.7) is a strictly
convex criterion and the minimizer is unique. If the matrix is singular, then
the linear part of the components f j cannot be uniquely determined (while
the nonlinear parts can!)(Buja et al., 1989).

Furthermore, a simple iterative procedure exists for finding the solution.
We set α̂ = ave(y i ), and it never changes. We apply a cubic smoothing
spline S j to the targets {y i − α̂ − k6 = j f ˆ k (x ik )} N1 , as a function of x ij ,
to obtain a new estimate f j . This is done for each predictor in turn, using
the current estimates of the other functions f ˆ k when computing y i − α̂ −
k6 = j f k (x ik ). The process is continued until the estimates f j stabilize. This
procedure, given in detail in Algorithm 9.1, is known as “backfitting” and
the resulting fit is analogous to a multiple regression for linear models.
In principle, the second step in (2) of Algorithm 9.1 is not needed, since
the smoothing spline fit to a mean-zero response has mean zero (Exer-
cise 9.1). In practice, machine rounding can cause slippage, and the ad-
justment is advised.

----------

#### Algorithm 9.1 The Backfitting Algorithm for Additive Models.

1. Initialize: $\hat{\alpha} = \frac{1}{N}\sum\_{i=1}^N y\_i$，
   $\hat{f}\equiv 0$$，$\forall i,j$。
2. Cycle: $j=1,2,\dots,p,\dots,1,2,\dots,p,\dots$,
   $$\begin{align}
   & \hat{f}\_j\leftarrow \mathcal{S}\_j \left[
   \\{y\_i - \hat{\alpha} - \sum\_{k\ne j}\hat{f}\_k(x\_{ik})\\}\_1^N
   \right] \\\\ &
   \hat{f}\_j\leftarrow
   \hat{f}\_j - \frac{1}{N}\sum\_{i=1}^N \hat{f}\_j(x\_{ij})
   \end{align}$$
   until the functions f ˆ j change less than a prespecified threshold.

----------

This same algorithm can accommodate other fitting methods in exactly
the same way, by specifying appropriate smoothing operators S j :

* other univariate regression smoothers such as local polynomial re-
gression and kernel methods;
* linear regression operators yielding polynomial fits, piecewise con-
stant fits, parametric spline fits, series and Fourier fits;
* more complicated operators such as surface smoothers for second or
higher-order interactions or periodic smoothers for seasonal effects.

If we consider the operation of smoother S j only at the training points, it
can be represented by an N × N operator matrix S j (see Section 5.4.1).
Then the degrees of freedom for the jth term are (approximately) computed
as df j = trace[S j ] − 1, by analogy with degrees of freedom for smoothers
discussed in Chapters 5 and 6.

For a large class of linear smoothers S j , backfitting is equivalent to a
Gauss–Seidel algorithm for solving a certain linear system of equations.
Details are given in Exercise 9.2.

For the logistic regression model and other generalized additive models,
the appropriate criterion is a penalized log-likelihood. To maximize it, the
backfitting procedure is used in conjunction with a likelihood maximizer.
The usual Newton–Raphson routine for maximizing log-likelihoods in gen-
eralized linear models can be recast as an IRLS (iteratively reweighted
least squares) algorithm. This involves repeatedly fitting a weighted linear
regression of a working response variable on the covariates; each regression
yields a new value of the parameter estimates, which in turn give new work-
ing responses and weights, and the process is iterated (see Section 4.4.1).
In the generalized additive model, the weighted linear regression is simply
replaced by a weighted backfitting algorithm. We describe the algorithm in
more detail for logistic regression below, and more generally in Chapter 6
of Hastie and Tibshirani (1990).

### 9.1.2 Example: Additive Logistic Regression

Probably the most widely used model in medical research is the logistic
model for binary data. In this model the outcome Y can be coded as 0
or 1, with 1 indicating an event (like death or relapse of a disease) and
0 indicating no event. We wish to model Pr(Y = 1|X), the probability of
an event given values of the prognostic factors X T = (X 1 , . . . , X p ). The
goal is usually to understand the roles of the prognostic factors, rather
than to classify new individuals. Logistic models are also used in applica-
tions where one is interested in estimating the class probabilities, for use
in risk screening. Apart from medical applications, credit risk screening is
a popular application.

The generalized additive logistic model has the form

$$\log\frac{\text{Pr}(Y=1|X)}{\text{Pr}(Y=0|X)} =
\alpha + f\_1(X\_1) + \dots + f\_p(X\_p) \tag{9.8}$$

The functions f 1 , f 2 , . . . , f p are estimated by a backfitting algorithm
within a Newton–Raphson procedure, shown in Algorithm 9.2.

----------

#### Algorithm 9.2 Local Scoring Algorithm for the Additive Logistic Regression Model.

1. Compute starting values: α̂ = log[ȳ/(1 − ȳ)], where ȳ = ave(y i ),
   the sample proportion of ones, and set f ˆ j ≡ 0 ∀j.
2. Define η̂ i = α̂ + j f ˆ j (x ij ) and p̂ i = 1/[1 + exp(−η̂ i )].
   Iterate:
   1. Construct the working target variable
      $$z\_i = \hat{\eta} +
      \frac{(y\_i - \hat{p}\_i)}{\hat{p}\_i(1-\hat{p}\_i)}$$
   2. Construct weights w i = p̂ i (1 − p̂ i )
   3. Fit an additive model to the targets z i with weights w i ,
      using a weighted backfitting algorithm. This gives new estimates α̂, f ˆ j , ∀j
3. Continue step 2. until the change in the functions falls below a prespecified threshold.

----------

The additive model fitting in step (2) of Algorithm 9.2 requires a weighted
scatterplot smoother. Most smoothing procedures can accept observation
weights (Exercise 5.12); see Chapter 3 of Hastie and Tibshirani (1990) for
further details.

The additive logistic regression model can be generalized further to han-
dle more than two classes, using the multilogit formulation as outlined in
Section 4.4. While the formulation is a straightforward extension of (9.8),
the algorithms for fitting such models are more complex. See Yee and Wild
(1996) for details, and the VGAM software currently available from:

http://www.stat.auckland.ac.nz/∼yee.

#### Example: Predicting Email Spam

We apply a generalized additive model to the spam data introduced in
Chapter 1. The data consists of information from 4601 email messages, in
a study to screen email for “spam” (i.e., junk email). The data is publicly
available at ftp.ics.uci.edu, and was donated by George Forman from
Hewlett-Packard laboratories, Palo Alto, California.

The response variable is binary, with values email or spam , and there are
57 predictors as described below:

* 48 quantitative predictors—the percentage of words in the email that
match a given word. Examples include business , address , internet ,
free , and george . The idea was that these could be customized for
individual users.
* 6 quantitative predictors—the percentage of characters in the email
that match a given character. The characters are ch; , ch( , ch[ , ch! ,
ch$ , and ch# .
* The average length of uninterrupted sequences of capital letters:
CAPAVE .
* The length of the longest uninterrupted sequence of capital letters:
CAPMAX .
* The sum of the length of uninterrupted sequences of capital letters:
CAPTOT .

We coded spam as 1 and email as zero. A test set of size 1536 was randomly
chosen, leaving 3065 observations in the training set. A generalized additive
model was fit, using a cubic smoothing spline with a nominal four degrees of
freedom for each predictor. What this means is that for each predictor X j ,
the smoothing-spline parameter λ j was chosen so that trace[S j (λ j )]−1 = 4,
where S j (λ) is the smoothing spline operator matrix constructed using the
observed values x ij , i = 1, . . . , N . This is a convenient way of specifying
the amount of smoothing in such a complex model.

Most of the spam predictors have a very long-tailed distribution. Before
fitting the GAM model, we log-transformed each variable (actually log(x +
0.1)), but the plots in Figure 9.1 are shown as a function of the original
variables.

|               | Predicted Class |                 |
|---------------|-----------------|-----------------|
| **True Class**| email(0)        | spam(1)         |
| email(0)   | 58.3%           | 2.5%            |
| spam(1)    | 3.0%            | 36.3%           |
**表9.1**：Test data confusion matrix for the additive logistic regression model
fit to the spam training data. The overall test error rate is 5.5%.

The test error rates are shown in Table 9.1; the overall error rate is 5.3%.
By comparison, a linear logistic regression has a test error rate of 7.6%.
Table 9.2 shows the predictors that are highly significant in the additive
model.

| Name | Num. | df | Coefficient | Std. Error | Z Score | Nonlinear P-value |
|------|------|----|-------------|------------|---------|-------------------|
| **Positive effects** | | | | | | |
| our      | 5  | 3.9 | 0.566       | 0.114      | 4.970   | 0.052 |
| over     | 6  | 3.9 | 0.244       | 0.195      | 1.249   | 0.004 |
| remove   | 7  | 4.0 | 0.949       | 0.183      | 5.201   | 0.093 |
| internet | 8  | 4.0 | 0.524       | 0.176      | 2.974   | 0.028 |
| free     | 16 | 3.9 | 0.507       | 0.127      | 4.010   | 0.065 |
| business | 17 | 3.8 | 0.779       | 0.186      | 4.179   | 0.194 |
| hpl      | 26 | 3.8 | 0.045       | 0.250      | 0.181   | 0.002 |
| ch!      | 52 | 4.0 | 0.674       | 0.128      | 5.283   | 0.164 |
| ch$      | 53 | 3.9 | 1.419       | 0.280      | 5.062   | 0.354 |
| CAPMAX   | 56 | 3.8 | 0.247       | 0.228      | 1.080   | 0.000 |
| CAPTOT   | 57 | 4.0 | 0.755       | 0.165      | 4.566   | 0.063 |
| **Negative effects** | | | | | | |
| hp       | 25 | 3.9 | −1.404      | 0.224      | −6.262  | 0.140 |
| george   | 27 | 3.7 | −5.003      | 0.744      | −6.722  | 0.045 |
| 1999     | 37 | 3.8 | −0.672      | 0.191      | −3.512  | 0.011 |
| re       | 45 | 3.9 | −0.620      | 0.133      | −4.649  | 0.597 |
| edu      | 46 | 4.0 | −1.183      | 0.209      | −5.647  | 0.000 |
**表9.2**：Significant predictors from the additive model fit to the spam train-
ing data. The coefficients represent the linear part of f ˆ j , along with their standard
errors and Z-score. The nonlinear P-value is for a test of nonlinearity of f ˆ j .

For ease of interpretation, in Table 9.2 the contribution for each variable
is decomposed into a linear component and the remaining nonlinear com-
ponent. The top block of predictors are positively correlated with spam,
while the bottom block is negatively correlated. The linear component is a
weighted least squares linear fit of the fitted curve on the predictor, while
the nonlinear part is the residual. The linear component of an estimated
function is summarized by the coefficient, standard error and Z-score; the
latter is the coefficient divided by its standard error, and is considered
significant if it exceeds the appropriate quantile of a standard normal dis-
tribution. The column labeled nonlinear P -value is a test of nonlinearity
of the estimated function. Note, however, that the effect of each predictor
is fully adjusted for the entire effects of the other predictors, not just for
their linear parts. The predictors shown in the table were judged signifi-
cant by at least one of the tests (linear or nonlinear) at the p = 0.01 level
(two-sided).

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_01.png"
  title="**图9.1**：Spam analysis: estimated functions for significant predictors. The rug plot along the bottom of each frame indicates the observed values of the corresponding predictor. For many of the predictors the nonlinearity picks up the discontinuity at zero."
>}}

Figure 9.1 shows the estimated functions for the significant predictors
appearing in Table 9.2. Many of the nonlinear effects appear to account for
a strong discontinuity at zero. For example, the probability of spam drops
significantly as the frequency of george increases from zero, but then does
not change much after that. This suggests that one might replace each of
the frequency predictors by an indicator variable for a zero count, and resort
to a linear logistic model. This gave a test error rate of 7.4%; including the
linear effects of the frequencies as well dropped the test error to 6.6%. It
appears that the nonlinearities in the additive model have an additional
predictive power.

It is more serious to classify a genuine email message as spam , since then
a good email would be filtered out and would not reach the user. We can
alter the balance between the class error rates by changing the losses (see
Section 2.4). If we assign a loss L 01 for predicting a true class 0 as class 1,
and L 10 for predicting a true class 1 as class 0, then the estimated Bayes
rule predicts class 1 if its probability is greater than L 01 /(L 01 + L 10 ). For
example, if we take L 01 = 10, L 10 = 1 then the (true) class 0 and class 1
error rates change to 0.8% and 8.7%.

More ambitiously, we can encourage the model to fit better data in the
class 0 by using weights L 01 for the class 0 observations and L 10 for the
class 1 observations. As above, we then use the estimated Bayes rule to
predict. This gave error rates of 1.2% and 8.0% in (true) class 0 and class 1,
respectively. We discuss below the issue of unequal losses further, in the
context of tree-based models.

After fitting an additive model, one should check whether the inclusion
of some interactions can significantly improve the fit. This can be done
“manually,” by inserting products of some or all of the significant inputs,
or automatically via the MARS procedure (Section 9.4).

This example uses the additive model in an automatic fashion. As a data
analysis tool, additive models are often used in a more interactive fashion,
adding and dropping terms to determine their effect. By calibrating the
amount of smoothing in terms of df j , one can move seamlessly between
linear models (df j = 1) and partially linear models, where some terms are
modeled more flexibly. See Hastie and Tibshirani (1990) for more details.

### 9.1.3 Summary

Additive models provide a useful extension of linear models, making them
more flexible while still retaining much of their interpretability. The familiar
tools for modeling and inference in linear models are also available for
additive models, seen for example in Table 9.2. The backfitting procedure
for fitting these models is simple and modular, allowing one to choose a
fitting method appropriate for each input variable. As a result they have
become widely used in the statistical community.

However additive models can have limitations for large data-mining ap-
plications. The backfitting algorithm fits all predictors, which is not feasi-
ble or desirable when a large number are available. The BRUTO procedure
(Hastie and Tibshirani, 1990, Chapter 9) combines backfitting with selec-
tion of inputs, but is not designed for large data-mining problems. There
has also been recent work using lasso-type penalties to estimate sparse ad-
ditive models, for example the COSSO procedure of Lin and Zhang (2006)
and the SpAM proposal of Ravikumar et al. (2008). For large problems a
forward stagewise approach such as boosting (Chapter 10) is more effective,
and also allows for interactions to be included in the model.
