+++
title = "ESL-9.3 PRIM: Bump Hunting"
summary = """
统计学习基础（译注）第九章第三节，第 317-321 页。
"""

date = 2018-12-30T19:22:00+08:00
lastmod = 2018-12-30T19:22:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Tree-based methods (for regression) partition the feature space into box-
shaped regions, to try to make the response averages in each box as differ-
ent as possible. The splitting rules defining the boxes are related to each
through a binary tree, facilitating their interpretation.

The patient rule induction method (PRIM) also finds boxes in the feature
space, but seeks boxes in which the response average is high. Hence it looks
for maxima in the target function, an exercise known as bump hunting. (If
minima rather than maxima are desired, one simply works with the negative
response values.)

PRIM also differs from tree-based partitioning methods in that the box
definitions are not described by a binary tree. This makes interpretation of
the collection of rules more difficult; however, by removing the binary tree
constraint, the individual rules are often simpler.

The main box construction method in PRIM works from the top down,
starting with a box containing all of the data. The box is compressed along
one face by a small amount, and the observations then falling outside the
box are peeled off. The face chosen for compression is the one resulting in
the largest box mean, after the compression is performed. Then the process
is repeated, stopping when the current box contains some minimum number
of data points.

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_07.png"
  title="**图9.7**："
>}}

This process is illustrated in Figure 9.7. There are 200 data points uni-
formly distributed over the unit square. The color-coded plot indicates the
response Y taking the value 1 (red) when 0.5 < X 1 < 0.8 and 0.4 < X 2 <
0.6. and zero (blue) otherwise. The panels shows the successive boxes found
by the top-down peeling procedure, peeling off a proportion α = 0.1 of the
remaining data points at each stage.

{{< figure
  src="http://public.guansong.wang/eslii/ch09/eslii_fig_09_08.png"
  title="**图9.8**："
>}}

Figure 9.8 shows the mean of the response values in the box, as the box
is compressed.

After the top-down sequence is computed, PRIM reverses the process,
expanding along any edge, if such an expansion increases the box mean.
This is called pasting. Since the top-down procedure is greedy at each step,
such an expansion is often possible.

The result of these steps is a sequence of boxes, with different numbers
of observation in each box. Cross-validation, combined with the judgment
of the data analyst, is used to choose the optimal box size.

Denote by B 1 the indices of the observations in the box found in step 1.
The PRIM procedure then removes the observations in B 1 from the training
set, and the two-step process—top down peeling, followed by bottom-up
pasting—is repeated on the remaining dataset. This entire process is re-
peated several times, producing a sequence of boxes B 1 , B 2 , . . . , B k . Each
box is defined by a set of rules involving a subset of predictors like

$$\begin{align}
& (a\_1 \leq X\_1 \leq b\_1) \\\\ & (b\_1 \leq X\_3 \leq b\_2)
\end{align}$$

A summary of the PRIM procedure is given Algorithm 9.3.

----------

#### Algorithm 9.3 Patient Rule Induction Method.

1. Start with all of the training data, and a maximal box containing all
of the data.
2. Consider shrinking the box by compressing one face, so as to peel off
the proportion α of observations having either the highest values of
a predictor X j , or the lowest. Choose the peeling that produces the
highest response mean in the remaining box. (Typically α = 0.05 or
0.10.)
3. Repeat step 2 until some minimal number of observations (say 10)
remain in the box.
4. Expand the box along any face, as long as the resulting box mean
increases.
5. Steps 1–4 give a sequence of boxes, with different numbers of obser-
vations in each box. Use cross-validation to choose a member of the
sequence. Call the box B 1 .
6. Remove the data in box B 1 from the dataset and repeat steps 2–5 to
obtain a second box, and continue to get as many boxes as desired.

----------

PRIM can handle a categorical predictor by considering all partitions of
the predictor, as in CART. Missing values are also handled in a manner
similar to CART. PRIM is designed for regression (quantitative response
variable); a two-class outcome can be handled simply by coding it as 0 and
1. There is no simple way to deal with k > 2 classes simultaneously: one
approach is to run PRIM separately for each class versus a baseline class.

An advantage of PRIM over CART is its patience. Because of its bi-
nary splits, CART fragments the data quite quickly. Assuming splits of
equal size, with N observations it can only make log 2 (N ) − 1 splits before
running out of data. If PRIM peels off a proportion α of training points
at each stage, it can perform approximately − log(N )/ log(1 − α) peeling
steps before running out of data. For example, if N = 128 and α = 0.10,
then log 2 (N ) − 1 = 6 while − log(N )/ log(1 − α) ≈ 46. Taking into account
that there must be an integer number of observations at each stage, PRIM
in fact can peel only 29 times. In any case, the ability of PRIM to be more
patient should help the top-down greedy algorithm find a better solution.

### 9.3.1 Spam Example (Continued)

We applied PRIM to the spam data, with the response coded as 1 for spam
and 0 for email .

The first two boxes found by PRIM are summarized below:

| Rule 1 | Global Mean | Box Mean | Box Support |
|--------|-------------|----------|-------------|
| Training | 0.3931 | 0.9607 | 0.1413 |
| Test     | 0.3958 | 1.0000 | 0.1536 |

$$\text{Rule 1} \begin{cases}
\text{ch!} &> 0.029
\\\\ \text{CAPAVE} &> 2.331
\\\\ \text{your} &> 0.705
\\\\ \text{1999} &< 0.040
\\\\ \text{CAPTOT} &> 79.50
\\\\ \text{edu} &< 0.070
\\\\ \text{re} &< 0.535
\\\\ \text{ch;} &< 0.030
\end{cases}$$

| Rule 2 | Remain Mean | Box Mean | Box Support |
|--------|-------------|----------|-------------|
| Training | 0.2998 | 0.9560 | 0.1043 |
| Test     | 0.2862 | 0.9264 | 0.1061 |

$$\text{Rule 2} \begin{cases}
\text{remove} &> 0.010
\\\\ \text{george} &< 0.110
\end{cases}$$

The box support is the proportion of observations falling in the box.
The first box is purely spam , and contains about 15% of the test data.
The second box contains 10.6% of the test observations, 92.6% of which
are spam . Together the two boxes contain 26% of the data and are about
97% spam . The next few boxes (not shown) are quite small, containing only
about 3% of the data.

The predictors are listed in order of importance. Interestingly the top
splitting variables in the CART tree (Figure 9.5) do not appear in PRIM’s
first box.