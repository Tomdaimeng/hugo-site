+++
title = "ESL-5.7 多维样条"
summary = """
统计学习基础（译注）第五章第七节，第 162-167 页。
"""

date = 2018-10-24T11:13:00+08:00
lastmod = 2018-10-24T11:13:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

So far we have focused on one-dimensional spline models. Each of the ap-
proaches have multidimensional analogs. Suppose X ∈ IR 2 , and we have
a basis of functions h 1k (X 1 ), k = 1, . . . , M 1 for representing functions of
coordinate X 1 , and likewise a set of M 2 functions h 2k (X 2 ) for coordinate
X 2 . Then the M 1 × M 2 dimensional tensor product basis defined by

$$ g\_{jk}(X) = h\_{1j}(X\_1) h\_{2k}(X\_2),
j = 1,\dots,M\_1, k = 1, \dots, M\_2 \tag{5.35}$$

can be used for representing a two-dimensional function:

$$ g(X) = \sum\_{j=1}^{M\_1} \sum\_{k=1}^{M\_2} \theta\_{jk}g\_{jk}(X) \tag{5.36}$$

{{< figure src="http://public.guansong.wang/eslii/ch05/eslii_fig_05_10.png"
  title="**图5.10**："
>}}

Figure 5.10 illustrates a tensor product basis using B-splines. The coeffi-
cients can be fit by least squares, as before. This can be generalized to d
dimensions, but note that the dimension of the basis grows exponentially
fast—yet another manifestation of the curse of dimensionality. The MARS
procedure discussed in Chapter 9 is a greedy forward algorithm for includ-
ing only those tensor products that are deemed necessary by least squares.

{{< figure src="http://public.guansong.wang/eslii/ch05/eslii_fig_05_11.png"
  title="**图5.11**："
>}}

Figure 5.11 illustrates the difference between additive and tensor product
(natural) splines on the simulated classification example from Chapter 2.
A logistic regression model logit[Pr(T |x)] = h(x) T θ is fit to the binary re-
sponse, and the estimated decision boundary is the contour h(x) T θ̂ = 0.
The tensor product basis can achieve more flexibility at the decision bound-
ary, but introduces some spurious structure along the way.

One-dimensional smoothing splines (via regularization) generalize to high-
er dimensions as well. Suppose we have pairs y i , x i with x i ∈ IR d , and we
seek a d-dimensional regression function f (x). The idea is to set up the
problem

$$ \min\_{f} \sum\_{i=1}^N \\{ y\_i - f(x\_i)\\}^2 + \lambda J[f] \tag{5.37}$$

where J is an appropriate penalty functional for stabilizing a function f in
IR d . For example, a natural generalization of the one-dimensional roughness
penalty (5.9) for functions on IR 2 is

$$ J[f] = \iint\_{\mathbb{R}} \left [
  \left ( \frac{\partial^2 f(x)}{\partial x\_1^2} \right )^2 +
  2 \left ( \frac{\partial^2 f(x)}{\partial x\_1 \partial x\_2} \right )^2 +
  \left ( \frac{\partial^2 f(x)}{\partial x\_2^2} \right )^2
\right ] dx\_1 dx\_2 \tag{5.38}$$

Optimizing (5.37) with this penalty leads to a smooth two-dimensional
surface, known as a thin-plate spline. It shares many properties with the
one-dimensional cubic smoothing spline:

* as λ → 0, the solution approaches an interpolating function [the one
with smallest penalty (5.38)];
* as λ → ∞, the solution approaches the least squares plane;
* for intermediate values of λ, the solution can be represented as a
linear expansion of basis functions, whose coefficients are obtained
by a form of generalized ridge regression.

The solution has the form

$$ f(x) = \beta\_0 + \beta^T x + \sum\_{j=1}^N \alpha\_j h\_j(x) \tag{5.39}$$

where h j (x) = ||x − x j || 2 log ||x − x j ||. These h j are examples of radial
basis functions, which are discussed in more detail in the next section. The
coefficients are found by plugging (5.39) into (5.37), which reduces to a
finite-dimensional penalized least squares problem. For the penalty to be
finite, the coefficients α j have to satisfy a set of linear constraints; see
Exercise 5.14.

Thin-plate splines are defined more generally for arbitrary dimension d,
for which an appropriately more general J is used.

There are a number of hybrid approaches that are popular in practice,
both for computational and conceptual simplicity. Unlike one-dimensional
smoothing splines, the computational complexity for thin-plate splines is
O(N 3 ), since there is not in general any sparse structure that can be ex-
ploited. However, as with univariate smoothing splines, we can get away
with substantially less than the N knots prescribed by the solution (5.39).

In practice, it is usually sufficient to work with a lattice of knots covering
the domain. The penalty is computed for the reduced expansion just as
before. Using K knots reduces the computations to O(N K 2 + K 3 ). Fig-
ure 5.12 shows the result of fitting a thin-plate spline to some heart disease
risk factors, representing the surface as a contour plot. Indicated are the
location of the input features, as well as the knots used in the fit. Note that
λ was specified via df λ = trace(S λ ) = 15.

{{< figure src="http://public.guansong.wang/eslii/ch05/eslii_fig_05_12.png"
  title="**图5.12**："
>}}

More generally one can represent f ∈ IR d as an expansion in any arbi-
trarily large collection of basis functions, and control the complexity by ap-
plying a regularizer such as (5.38). For example, we could construct a basis
by forming the tensor products of all pairs of univariate smoothing-spline
basis functions as in (5.35), using, for example, the univariate B-splines
recommended in Section 5.9.2 as ingredients. This leads to an exponential
growth in basis functions as the dimension increases, and typically we have
to reduce the number of functions per coordinate accordingly.

The additive spline models discussed in Chapter 9 are a restricted class
of multidimensional splines. They can be represented in this general formu-
lation as well; that is, there exists a penalty J[f ] that guarantees that the
solution has the form f (X) = α + f 1 (X 1 ) + · · · + f d (X d ) and that each of
the functions f j are univariate splines. In this case the penalty is somewhat
degenerate, and it is more natural to assume that f is additive, and then
simply impose an additional penalty on each of the component functions:

$$\begin{align} J[f] &=
  J(f\_1 + f\_2 + \dots + f\_d) \\\\ &=
  \sum\_{j=1}^d \int f^{\prime\prime}\_j(t\_j)^2 dt\_j
\tag{5.40}\end{align}$$

These are naturally extended to ANOVA spline decompositions,

$$ f(X) = \alpha +
  \sum\_j f\_j(X\_j) + \sum\_{j < k} f\_{jk}(X\_j, X\_k) + \dots \tag{5.41}$$

where each of the components are splines of the required dimension. There
are many choices to be made:

* The maximum order of interaction—we have shown up to order 2
above.
* Which terms to include—not all main effects and interactions are
necessarily needed.
* What representation to use—some choices are:
  - regression splines with a relatively small number of basis func-
  tions per coordinate, and their tensor products for interactions;
  - a complete basis as in smoothing splines, and include appropri-
  ate regularizers for each term in the expansion.

In many cases when the number of potential dimensions (features) is large,
automatic methods are more desirable. The MARS and MART procedures
(Chapters 9 and 10, respectively), both fall into this category.
