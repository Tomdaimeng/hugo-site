+++
title = "ESL-5.7 多维样条"
summary = """
统计学习基础（译注）第五章第七节，第 162-167 页。
"""

date = 2018-10-24T11:13:00+08:00
lastmod = 2018-10-24T11:13:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

之前一直讨论的是一维的样条模型，
然而每个模型均可用在多维的场景中。
假设 $X \in \mathbb{R}^2$，
$h\_{1k}(X\_1)$，$k=1,\dots,M\_1$ 为表达坐标 $X\_1$ 的函数的一组基函数，
类似地，
$h\_{2k}(X\_2)$，$k=1,\dots,M\_1$ 为表达坐标 $X\_2$ 的函数的一组基函数。
则 $M\_1 \times M\_2$ 维的**张量积（tensor product）**基函数定义为：

$$ g\_{jk}(X) = h\_{1j}(X\_1) h\_{2k}(X\_2),
j = 1,\dots,M\_1, k = 1, \dots, M\_2 \tag{5.35}$$

则一个二维的函数可用上式的基函数表达为：

$$ g(X) = \sum\_{j=1}^{M\_1} \sum\_{k=1}^{M\_2} \theta\_{jk}g\_{jk}(X) \tag{5.36}$$

{{< figure src="http://public.guansong.wang/eslii/ch05/eslii_fig_05_10.png"
  title="**图5.10**：B 样条的张量积基函数，图中展示了某几对组合。其中每个二维函数均为相应的两个一维边际函数的乘积。"
>}}

图 5.10 展示了 B 样条的张量积基函数。
系数可以与之前一样用最小二乘拟合得出。
这种处理方法可以推广到 $d$ 维度，
然而需要注意基函数的维度以指数级别快速地增加
——维数灾难的再现。
第九章会介绍一个只使用最小二乘中必需的张量积基函数的贪心前向算法，MARS。

{{< figure src="http://public.guansong.wang/eslii/ch05/eslii_fig_05_11.png"
  title="**图5.11**："
>}}
The simulation example of Figure 2.1. The upper panel shows the
decision boundary of an additive logistic regression model, using natural splines
in each of the two coordinates (total df = 1 + (4 − 1) + (4 − 1) = 7). The lower
panel shows the results of using a tensor product of natural spline bases in each
coordinate (total df = 4 × 4 = 16). The broken purple boundary is the Bayes
decision boundary for this problem.

图 5.11 演示了在第二章的分类模拟例子中，加性和张量积（自然）样条的区别。
在二分类的训练样本上拟合对数几率回归模型 $\text{logit}[\text{Pr}(T|x)]=h(x)^T\theta$，
估计的判别边界为等值线 $h(x)^T\hat{\theta} = 0$。
张量积基函数方法得到的判别边界更灵活

Figure 5.11 illustrates the difference between additive and tensor product
(natural) splines on the simulated classification example from Chapter 2.
A logistic regression model logit[Pr(T |x)] = h(x) T θ is fit to the binary re-
sponse, and the estimated decision boundary is the contour h(x) T θ̂ = 0.
The tensor product basis can achieve more flexibility at the decision bound-
ary, but introduces some spurious structure along the way.

One-dimensional smoothing splines (via regularization) generalize to high-
er dimensions as well. Suppose we have pairs y i , x i with x i ∈ IR d , and we
seek a d-dimensional regression function f (x). The idea is to set up the
problem

$$ \min\_{f} \sum\_{i=1}^N \\{ y\_i - f(x\_i)\\}^2 + \lambda J[f] \tag{5.37}$$

where J is an appropriate penalty functional for stabilizing a function f in
IR d . For example, a natural generalization of the one-dimensional roughness
penalty (5.9) for functions on IR 2 is

$$ J[f] = \iint\_{\mathbb{R}} \left [
  \left ( \frac{\partial^2 f(x)}{\partial x\_1^2} \right )^2 +
  2 \left ( \frac{\partial^2 f(x)}{\partial x\_1 \partial x\_2} \right )^2 +
  \left ( \frac{\partial^2 f(x)}{\partial x\_2^2} \right )^2
\right ] dx\_1 dx\_2 \tag{5.38}$$

Optimizing (5.37) with this penalty leads to a smooth two-dimensional
surface, known as a thin-plate spline. It shares many properties with the
one-dimensional cubic smoothing spline:

* as λ → 0, the solution approaches an interpolating function [the one
with smallest penalty (5.38)];
* as λ → ∞, the solution approaches the least squares plane;
* for intermediate values of λ, the solution can be represented as a
linear expansion of basis functions, whose coefficients are obtained
by a form of generalized ridge regression.

The solution has the form

$$ f(x) = \beta\_0 + \beta^T x + \sum\_{j=1}^N \alpha\_j h\_j(x) \tag{5.39}$$

where h j (x) = ||x − x j || 2 log ||x − x j ||. These h j are examples of radial
basis functions, which are discussed in more detail in the next section. The
coefficients are found by plugging (5.39) into (5.37), which reduces to a
finite-dimensional penalized least squares problem. For the penalty to be
finite, the coefficients α j have to satisfy a set of linear constraints; see
Exercise 5.14.

Thin-plate splines are defined more generally for arbitrary dimension d,
for which an appropriately more general J is used.

There are a number of hybrid approaches that are popular in practice,
both for computational and conceptual simplicity. Unlike one-dimensional
smoothing splines, the computational complexity for thin-plate splines is
O(N 3 ), since there is not in general any sparse structure that can be ex-
ploited. However, as with univariate smoothing splines, we can get away
with substantially less than the N knots prescribed by the solution (5.39).

In practice, it is usually sufficient to work with a lattice of knots covering
the domain. The penalty is computed for the reduced expansion just as
before. Using K knots reduces the computations to O(N K 2 + K 3 ). Fig-
ure 5.12 shows the result of fitting a thin-plate spline to some heart disease
risk factors, representing the surface as a contour plot. Indicated are the
location of the input features, as well as the knots used in the fit. Note that
λ was specified via df λ = trace(S λ ) = 15.

{{< figure src="http://public.guansong.wang/eslii/ch05/eslii_fig_05_12.png"
  title="**图5.12**："
>}}

More generally one can represent f ∈ IR d as an expansion in any arbi-
trarily large collection of basis functions, and control the complexity by ap-
plying a regularizer such as (5.38). For example, we could construct a basis
by forming the tensor products of all pairs of univariate smoothing-spline
basis functions as in (5.35), using, for example, the univariate B-splines
recommended in Section 5.9.2 as ingredients. This leads to an exponential
growth in basis functions as the dimension increases, and typically we have
to reduce the number of functions per coordinate accordingly.

The additive spline models discussed in Chapter 9 are a restricted class
of multidimensional splines. They can be represented in this general formu-
lation as well; that is, there exists a penalty J[f ] that guarantees that the
solution has the form f (X) = α + f 1 (X 1 ) + · · · + f d (X d ) and that each of
the functions f j are univariate splines. In this case the penalty is somewhat
degenerate, and it is more natural to assume that f is additive, and then
simply impose an additional penalty on each of the component functions:

$$\begin{align} J[f] &=
  J(f\_1 + f\_2 + \dots + f\_d) \\\\ &=
  \sum\_{j=1}^d \int f^{\prime\prime}\_j(t\_j)^2 dt\_j
\tag{5.40}\end{align}$$

These are naturally extended to ANOVA spline decompositions,

$$ f(X) = \alpha +
  \sum\_j f\_j(X\_j) + \sum\_{j < k} f\_{jk}(X\_j, X\_k) + \dots \tag{5.41}$$

where each of the components are splines of the required dimension. There
are many choices to be made:

* The maximum order of interaction—we have shown up to order 2
above.
* Which terms to include—not all main effects and interactions are
necessarily needed.
* What representation to use—some choices are:
  - regression splines with a relatively small number of basis func-
  tions per coordinate, and their tensor products for interactions;
  - a complete basis as in smoothing splines, and include appropri-
  ate regularizers for each term in the expansion.

In many cases when the number of potential dimensions (features) is large,
automatic methods are more desirable. The MARS and MART procedures
(Chapters 9 and 10, respectively), both fall into this category.
