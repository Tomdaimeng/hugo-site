+++
title = "ESL-5.8 Regularization and Reproducing Kernel Hilbert Spaces 😱"
summary = """
统计学习基础（译注）第五章第八节，第 167-174 页。
"""

date = 2018-10-25T14:37:00+08:00
lastmod = 2018-10-25T14:37:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In this section we cast splines into the larger context of regularization meth-
ods and reproducing kernel Hilbert spaces. This section is quite technical
and can be skipped by the disinterested or intimidated reader.

A general class of regularization problems has the form

$$\min\_{f\in\mathcal{H}} \left [
  \sum\_{i=1}^N L(y\_i, f(x\_i)) + \lambda J(f)
\right ] \tag{5.42}$$

where L(y, f (x)) is a loss function, J(f ) is a penalty functional, and H is
a space of functions on which J(f ) is defined. Girosi et al. (1995) describe
quite general penalty functionals of the form

$$J(f) = \int\_{\mathbb{R}^d}
  \frac{\|\tilde{f}(s)\|^2}{\tilde{G}(s)} ds
\tag{5.43}$$

where f  ̃ denotes the Fourier transform of f , and G̃ is some positive function
that falls off to zero as ||s|| → ∞. The idea is that 1/G̃ increases the penalty
for high-frequency components of f . Under some additional assumptions
they show that the solutions have the form

$$f(X) = \sum\_{k=1}^K \alpha\_k \phi\_k(X) +
  \sum\_{i=1}^N \theta\_i G(X - x\_i)
\tag{5.44}$$

where the φ k span the null space of the penalty functional J, and G is the
inverse Fourier transform of G̃. Smoothing splines and thin-plate splines
fall into this framework. The remarkable feature of this solution is that
while the criterion (5.42) is defined over an infinite-dimensional space, the
solution is finite-dimensional. In the next sections we look at some specific
examples.

### 5.8.1 Spaces of Functions Generated by Kernels

An important subclass of problems of the form (5.42) are generated by
a positive definite kernel K(x, y), and the corresponding space of func-
tions H K is called a reproducing kernel Hilbert space (RKHS). The penalty
functional J is defined in terms of the kernel as well. We give a brief and
simplified introduction to this class of models, adapted from Wahba (1990)
and Girosi et al. (1995), and nicely summarized in Evgeniou et al. (2000).
Let x, y ∈ IR p . We consider the space of functions generated by the linear
span of P

{K(·, y), y ∈ IR p )}; i.e arbitrary linear combinations of the form
f (x) = m α m K(x, y m ), where each kernel term is viewed as a function
of the first argument, and indexed by the second. Suppose that K has an
eigen-expansion

$$K(x,y) = \sum\_{i=1}^\infty \gamma\_i \phi\_i(x) \phi\_i(y)
\tag{5.45}$$

with γ i ≥ 0,< ∞. Elements of H K have an expansion in terms of
these eigen-functions,

$$f(x) = \sum\_{i=1}^\infty c\_i \phi\_i(x) \tag{5.46}$$

with the constraint that

$$\\|f\\|^2\_{\mathcal{H}\_K} \stackrel{\text{def}}{=}
\sum\_{i=1}^\infty \frac{c\_i^2}{\gamma\_i} < \infty
\tag{5.47}$$

where ||f || H K is the norm induced by K. The penalty functional in (5.42)
for the space H K is defined to be the squared norm J(f ) = ||f || 2 H K . The
quantity J(f ) can be interpreted as a generalized ridge penalty, where
functions with large eigenvalues in the expansion (5.45) get penalized less,
and vice versa.

Rewriting (5.42) we have

$$\min\_{f\in\mathcal{H}\_K} \left [
  \sum\_{i=1}^N L(y\_i, f(x\_i)) + \lambda \\|f\\|^2\_{\mathcal{H}\_K}
\right ] \tag{5.48}$$

or equivalently

$$\min\_{\\{c\_j\\}\_1^\infty} \left [
  \sum\_{i=1}^N L(y\_i, \sum\_{j=1}^\infty c\_j \phi\_j(x\_i)) +
  \lambda \sum\_{j=1}^\infty \frac{c\_j^2}{\gamma\_j}
\right ]\tag{5.49}$$

It can be shown (Wahba, 1990, see also Exercise 5.15) that the solution
to (5.48) is finite-dimensional, and has the form


$$f(x) = \sum\_{i=1}^N \alpha\_i K(x, x\_i) \tag{5.50}$$

The basis function h i (x) = K(x, x i ) (as a function of the first argument) is
known as the representer of evaluation at x i in H K , since for f ∈ H K , it is
easily seen that hK(·, x i ), f i H K = f (x i ). Similarly hK(·, x i ), K(·, x j )i H K =
K(x i , x j ) (the reproducing property of H K ), and hence

$$J(f) = \sum\_{i=1}^N \sum\_{j=1}^N
  K(x\_i, x\_j) \alpha\_i \alpha\_j
\tag{5.51}$$

for f (x) = i=1 α i K(x, x i ).

In light of (5.50) and (5.51), (5.48) reduces to a finite-dimensional crite-
rion

$$\min\_{\mathbf{\alpha}} L(\mathbf{y}, \mathbf{K}\mathbf{\alpha}) +
\lambda \mathbf{\alpha}^T\mathbf{K}\mathbf{\alpha}
\tag{5.52}$$

We are using a vector notation, in which K is the N × N matrix with ijth
entry K(x i , x j ) and so on. Simple numerical algorithms can be used to
optimize (5.52). This phenomenon, whereby the infinite-dimensional prob-
lem (5.48) or (5.49) reduces to a finite dimensional optimization problem,
has been dubbed the kernel property in the literature on support-vector
machines (see Chapter 12).

There is a Bayesian interpretation of this class of models, in which f
is interpreted as a realization of a zero-mean stationary Gaussian process,
with prior covariance function K. The eigen-decomposition produces a se-
ries of orthogonal eigen-functions φ j (x) with associated variances γ j . The
typical scenario is that “smooth” functions φ j have large prior variance,
while “rough” φ j have small prior variances. The penalty in (5.48) is the
contribution of the prior to the joint likelihood, and penalizes more those
components with smaller prior variance (compare with (5.43)).

For simplicity we have dealt with the case here where all members of H
are penalized, as in (5.48). More generally, there may be some components
in H that we wish to leave alone, such as the linear functions for cubic
smoothing splines in Section 5.4. The multidimensional thin-plate splines
of Section 5.7 and tensor product splines fall into this category as well.
In these cases there is a more convenient representation H = H 0 ⊕ H 1 ,
with the null space H 0 consisting of, for example, low degree polynomi-
als in x that do not get penalized. The penalty becomes J(f ) = kP 1 f k,
where P 1 is the of f onto H 1 . The solution has the
P M orthogonal projection
P N
form f (x) = j=1 β j h j (x) + i=1 α i K(x, x i ), where the first term repre-
sents an expansion in H 0 . From a Bayesian perspective, the coefficients of
components in H 0 have improper priors, with infinite variance.

### 5.8.2 Examples of RKHS

The machinery above is driven by the choice of the kernel K and the loss
function L. We consider first regression using squared-error loss. In this
case (5.48) specializes to penalized least squares, and the solution can be
characterized in two equivalent ways corresponding to (5.49) or (5.52):

$$\min\_{\\{c\_j\\}\_1^\infty}
  \sum\_{i=1}^N \left ( y\_i - \sum\_{j=1}^\infty c\_j \phi\_j(x\_i)
    \right )^2 + \lambda \sum\_{j=1}^\infty \frac{c\_j^2}{\gamma\_j}
\tag{5.53}$$

an infinite-dimensional, generalized ridge regression problem, or

$$\min\_{\mathbf{\alpha}}
  (\mathbf{y} - \mathbf{K}\mathbf{\alpha})^T
  (\mathbf{y} - \mathbf{K}\mathbf{\alpha}) +
  \lambda \mathbf{\alpha}^T\mathbf{K}\mathbf{\alpha}
\tag{5.54}$$

The solution for α is obtained simply as

$$\hat{\mathbf{\alpha}} = (\mathbf{K} + \lambda \mathbf{I})^{-1} \mathbf{y}\tag{5.55}$$

and

$$\hat{f}(x) = \sum\_{j=1}^N \hat{\alpha}\_j K(x, x\_j) \tag{5.56}$$

The vector of N fitted values is given by

$$\begin{align} \hat{\mathbf{f}} &=
  \mathbf{K} \hat{\mathbf{\alpha}} \\\\ &=
  \mathbf{K} (\mathbf{K} + \lambda \mathbf{I})^{-1} \mathbf{y}
  \tag{5.57} \\\\ &=
  (\mathbf{I} + \lambda \mathbf{K}^{-1})^{-1} \mathbf{y}
  \tag{5.58}
\end{align}$$

The estimate (5.57) also arises as the kriging estimate of a Gaussian ran-
dom field in spatial statistics (Cressie, 1993). Compare also (5.58) with the
smoothing spline fit (5.17) on page 154.

#### Penalized Polynomial Regression

The kernel  K(x, y) = (hx, yi + 1) d (Vapnik, 1996), for x, y ∈ IR p , has
M = p+d eigen-functions that span the space of polynomials in IR p of d
total degree d. For example, with p = 2 and d = 2, M = 6 and

$$\begin{align} K(x,y) &=
  1 + 2 x\_1 y\_1 + 2 x\_2 y\_2  + x\_1^2 y\_1^2 +
  x\_2^2 y\_2^2 + 2 x\_1 x\_2 y\_1 y\_2
  \tag{5.59} \\\\ &=
  \sum\_{m=1}^M h\_m(x) h\_m(y) \tag{5.60}
\end{align}$$

with

$$h(x)^T = (1, \sqrt{2}x\_1, \sqrt{2}x\_2, x\_1^2, x\_2^2, \sqrt{2}x\_1 x\_2) \tag{5.61}$$

One can represent h in terms of the M orthogonal eigen-functions and
eigenvalues of K,

$$h(x) = \mathbf{V}\mathbf{D}\_\gamma^{\frac{1}{2}} \phi(x) \tag{5.62}$$

where D γ = diag(γ 1 , γ 2 , . . . , γ M ), and V is M × M and orthogonal.

Suppose we wish to solve the penalized polynomial regression problem

$$\min\_{\\{\beta\_m\\}\_1^M}
  \sum\_{i=1}^N \left ( y\_i - \sum\_{m=1}^M \beta\_m h\_m(x\_i)
    \right )^2 + \lambda \sum\_{m=1}^M \beta\_m^2
\tag{5.63}$$

Substituting (5.62) into (5.63), we get an expression of the form (5.53) to
optimize (Exercise 5.16).

The number of basis functions M = p+d d can be very large, often much
larger than N . Equation (5.55) tells us that if we use the kernel represen-
tation for the solution function, we have only to evaluate the kernel N 2
times, and can compute the solution in O(N 3 ) operations.

This simplicity is not without implications. Each of the polynomials h m
in (5.61) inherits a scaling factor from the particular form of K, which has
a bearing on the impact of the penalty in (5.63). We elaborate on this in
the next section.

#### Gaussian Radial Basis Functions

In the preceding example, the kernel is chosen because it represents an
expansion of polynomials and can conveniently compute high-dimensional
inner products. In this example the kernel is chosen because of its functional
form in the representation (5.50).

The Gaussian kernel K(x, y) = e −ν||x−y|| along with squared-error loss,
for example, leads to a regression model that is an expansion in Gaussian
radial basis functions,

$$k\_m(x) = e^{-\nu \\|x-x\_m\\|^2}, m=1,\dots,N \tag{5.64}$$

each one centered at one of the training feature vectors x m . The coefficients
are estimated using (5.54).

Figure 5.13 illustrates radial kernels in IR 1 using the first coordinate of
the mixture example from Chapter 2. We show five of the 200 kernel basis
functions k m (x) = K(x, x m ).

Figure 5.14 illustrates the implicit feature space for the radial kernel
with x ∈ IR 1 . We computed the 200 × 200 kernel matrix K, and its eigen-
decomposition ΦD γ Φ T . We can think of the columns of Φ and the corre-
sponding eigenvalues in D γ as empirical estimates of the eigen expansion
(5.45) 2 . Although the eigenvectors are discrete, we can represent them as
functions on IR 1 (Exercise 5.17). Figure 5.15 shows the largest 50 eigenval-
ues of K. The leading eigenfunctions are smooth, and they are successively
more wiggly as the order increases. This brings to life the penalty in (5.49),
where we see the coefficients of higher-order functions get penalized more
than lower-order ones. The right panel in Figure 5.14 shows the correspond-
ing feature space representation of the eigenfunctions

$$h\_l(x) = \sqrt{\hat{\gamma}\_l} \hat{\phi}\_l(x), l=1,\dots,N \tag{5.65}$$

Note that hh(x i ), h(x i ′ )i = K(x i , x i ′ ). The scaling by the eigenvalues quickly
shrinks most of the functions down to zero, leaving an effective dimension
of about 12 in this case. The corresponding optimization problem is a stan-
dard ridge regression, as in (5.63). So although in principle the implicit
feature space is infinite dimensional, the effective dimension is dramat-
ically lower because of the relative amounts of shrinkage applied to each
basis function. The kernel scale parameter ν plays a role here as well; larger
ν implies more local k m functions, and increases the effective dimension of
the feature space. See Hastie and Zhu (2006) for more details.

It is also known (Girosi et al., 1995) that a thin-plate spline (Section 5.7)
is an expansion in radial basis functions, generated by the kernel

$$K(x,y) = \\|x-y\\|^2 \log(\\|x-y\\|) \tag{5.66}$$

Radial basis functions are discussed in more detail in Section 6.7.

#### Support Vector Classifiers

The support vector machines of Chapter 12 for a two-class classification
problem have the form f (x) = α 0 + i=1 α i K(x, x i ), where the parameters
are chosen to minimize

$$\min\_{\alpha\_0, \mathbf{\alpha} } \left \\{
  \sum\_{i=1}^N [1 - y\_i f(x\_i)]\_+ +
  \frac{\lambda}{2} \mathbf{\alpha}^T\mathbf{K}\mathbf{\alpha}
\right \\}\tag{5.67}$$

where y i ∈ {−1, 1}, and [z] + denotes the positive part of z. This can be
viewed as a quadratic optimization problem with linear constraints, and
requires a quadratic programming algorithm for its solution. The name
support vector arises from the fact that typically many of the α̂ i = 0 [due
to the piecewise-zero nature of the loss function in (5.67)], and so f ˆ is an
expansion in a subset of the K(·, x i ). See Section 12.3.3 for more details.