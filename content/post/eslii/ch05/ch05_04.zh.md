+++
title = "ESL-5.4 平滑样条"
summary = """
统计学习基础（译注）第五章第四节，第 151-156 页。
"""

date = 2018-10-19T19:54:00+08:00
lastmod = 2018-10-19T19:54:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Here we discuss a spline basis method that avoids the knot selection prob-
lem completely by using a maximal set of knots. The complexity of the fit
is controlled by regularization. Consider the following problem: among all
functions f (x) with two continuous derivatives, find one that minimizes the
penalized residual sum of squares

$$\text{RSS}(f, \lambda) =
  \sum\_{i=1}^N \\{y\_i - f(x\_i)\\}^2 + \lambda\int\\{f^{\prime\prime}(t)\\}^2 dt
\tag{5.9}$$

where λ is a fixed smoothing parameter. The first term measures closeness
to the data, while the second term penalizes curvature in the function, and
λ establishes a tradeoff between the two. Two special cases are:

λ = 0 : f can be any function that interpolates the data.
λ = ∞ : the simple least squares line fit, since no second derivative can
be tolerated.

These vary from very rough to very smooth, and the hope is that λ ∈ (0, ∞)
indexes an interesting class of functions in between.

The criterion (5.9) is defined on an infinite-dimensional function space—
in fact, a Sobolev space of functions for which the second term is defined.
Remarkably, it can be shown that (5.9) has an explicit, finite-dimensional,
unique minimizer which is a natural cubic spline with knots at the unique
values of the x i , i = 1, . . . , N (Exercise 5.7). At face value it seems that
the family is still over-parametrized, since there are as many as N knots,
which implies N degrees of freedom. However, the penalty term translates
to a penalty on the spline coefficients, which are shrunk some of the way
toward the linear fit.

Since the solution is a natural spline, we can write it as

$$f(x) = \sum\_{j=1}^N N\_j(x)\theta\_j \tag{5.10}$$

where the N j (x) are an N -dimensional set of basis functions for repre-
senting this family of natural splines (Section 5.2.1 and Exercise 5.4). The
criterion thus reduces to

$$\text{RSS}(\theta, \lambda) =
  (\mathbf{y} - \mathbf{N}\theta)^T(\mathbf{y} - \mathbf{N}\theta) +
  \lambda \theta^T\mathbf{\Omega}\_N\theta
\tag{5.11}$$

where {N} ij = N j (x i ) and {Ω N } jk = N j (t)N k ′′ (t)dt. The solution is
easily seen to be

$$\hat{\theta} =
  (\mathbf{N}^T\mathbf{N} + \lambda \mathbf{\Omega}\_N)^{-1}
  \mathbf{N}^T \mathbf{y}
\tag{5.12}$$

a generalized ridge regression. The fitted smoothing spline is given by

$$\hat{f}(x) = \sum\_{j=1}^N N\_j(x) \hat{\theta}\_j \tag{5.13}$$

Efficient computational techniques for smoothing splines are discussed in
the Appendix to this chapter.

Figure 5.6 shows a smoothing spline fit to some data on bone mineral
density (BMD) in adolescents. The response is relative change in spinal
BMD over two consecutive visits, typically about one year apart. The data
are color coded by gender, and two separate curves were fit. This simple
summary reinforces the evidence in the data that the growth spurt for
females precedes that for males by about two years. In both cases the
smoothing parameter λ was approximately 0.00022; this choice is discussed
in the next section.

### 5.4.1 Degrees of Freedom and Smoother Matrices

We have not yet indicated how λ is chosen for the smoothing spline. Later
in this chapter we describe automatic methods using techniques such as
cross-validation. In this section we discuss intuitive ways of prespecifying
the amount of smoothing.

A smoothing spline with prechosen λ is an example of a linear smoother
(as in linear operator). This is because the estimated parameters in (5.12)
are a linear combination of the y i . Denote by f̂ the N -vector of fitted values
f ˆ (x i ) at the training predictors x i . Then

$$\begin{align}
\hat{\mathbf{f}} &= \mathbf{N}(\mathbf{N}^T\mathbf{N}+\lambda\mathbf{\Omega}\_N)^{-1}
  \mathbf{N}^T\mathbf{y} \\\\ &=
  \mathbf{S}\_\lambda \mathbf{y} \tag{5.14}
\end{align}$$

Again the fit is linear in y, and the finite linear operator S λ is known as
the smoother matrix. One consequence of this linearity is that the recipe
for producing f̂ from y does not depend on y itself; S λ depends only on
the x i and λ.

Linear operators are familiar in more traditional least squares fitting as
well. Suppose B ξ is a N × M matrix of M cubic-spline basis functions
evaluated at the N training points x i , with knot sequence ξ, and M ≪ N .
Then the vector of fitted spline values is given by

$$\begin{align}
\hat{\mathbf{f}} &= \mathbf{B}\_\xi(\mathbf{B}\_\xi^T\mathbf{B}\_\xi)^{-1}
  \mathbf{B}\_\xi^T\mathbf{y} \\\\ &=
  \mathbf{H}\_\xi \mathbf{y} \tag{5.15}
\end{align}$$

Here the linear operator H ξ is a projection operator, also known as the hat
matrix in statistics. There are some important similarities and differences
between H ξ and S λ :

* Both are symmetric, positive semidefinite matrices.
* H ξ H ξ = H ξ (idempotent), while S λ S λ  S λ , meaning that the right-
  hand side exceeds the left-hand side by a positive semidefinite matrix.
  This is a consequence of the shrinking nature of S λ , which we discuss
  further below.
• H ξ has rank M , while S λ has rank N .

The expression M = trace(H ξ ) gives the dimension of the projection space,
which is also the number of basis functions, and hence the number of pa-
rameters involved in the fit. By analogy we define the effective degrees of
freedom of a smoothing spline to be

$$\text{df}\_\lambda = \text{trace}(\mathbf{S}\_\lambda) \tag{5.16}$$

the sum of the diagonal elements of S λ . This very useful definition allows
us a more intuitive way to parameterize the smoothing spline, and indeed
many other smoothers as well, in a consistent fashion. For example, in Fig-
ure 5.6 we specified df λ = 12 for each of the curves, and the corresponding
λ ≈ 0.00022 was derived numerically by solving trace(S λ ) = 12. There are
many arguments supporting this definition of degrees of freedom, and we
cover some of them here.

Since S λ is symmetric (and positive semidefinite), it has a real eigen-
decomposition. Before we proceed, it is convenient to rewrite S λ in the
Reinsch form

$$\mathbf{S}\_\lambda = (\mathbf{I} + \lambda\mathbf{K})^{-1} \tag{5.17}$$

where K does not depend on λ (Exercise 5.9). Since f̂ = S λ y solves

$$\min\_{\mathbf{f}}
(\mathbf{y} - \mathbf{f})^T(\mathbf{y} - \mathbf{f}) +
\lambda\mathbf{f}^T\mathbf{K}\mathbf{f}
\tag{5.18}$$

K is known as the penalty matrix, and indeed a quadratic form in K has
a representation in terms of a weighted sum of squared (divided) second
differences. The eigen-decomposition of S λ is

$$ \mathbf{S}\_\lambda =
  \sum\_{k=1}^N \rho\_k(\lambda)\mathbf{u}\_k\mathbf{u}\_k^T
\tag{5.19}$$

with 

$$\rho\_k(\lambda) = \frac{1}{1+\lambda d\_k} \tag{5.20}$$

and d k the corresponding eigenvalue of K. Figure 5.7 (top) shows the re-
sults of applying a cubic smoothing spline to some air pollution data (128
observations). Two fits are given: a smoother fit corresponding to a larger
penalty λ and a rougher fit for a smaller penalty. The lower panels repre-
sent the eigenvalues (lower left) and some eigenvectors (lower right) of the
corresponding smoother matrices. Some of the highlights of the eigenrep-
resentation are the following:

* The eigenvectors are not affected by changes in λ, and hence the whole
  family of smoothing splines (for a particular sequence x) indexed by
  λ have the same eigenvectors.
* S λ y = k=1 u k ρ k (λ)hu k , yi, and hence the smoothing spline oper-
  ates by decomposing y w.r.t. the (complete) basis {u k }, and differ-
  entially shrinking the contributions using ρ k (λ). This is to be con-
  trasted with a basis-regression method, where the components are
  either left alone, or shrunk to zero—that is, a projection matrix such
  as H ξ above has M eigenvalues equal to 1, and the rest are 0. For
  this reason smoothing splines are referred to as shrinking smoothers,
  while regression splines are projection smoothers (see Figure 3.17 on
  page 80).
* The sequence of u k , ordered by decreasing ρ k (λ), appear to increase
  in complexity. Indeed, they have the zero-crossing behavior of polyno-
  mials of increasing degree. Since S λ u k = ρ k (λ)u k , we see how each of
  the eigenvectors themselves are shrunk by the smoothing spline: the
  higher the complexity, the more they are shrunk. If the domain of X
  is periodic, then the u k are sines and cosines at different frequencies.
* The first two eigenvalues are always one, and they correspond to the
  two-dimensional eigenspace of functions linear in x (Exercise 5.11),
  which are never shrunk.
* The eigenvalues ρ k (λ) = 1/(1 + λd k ) are an inverse function of the
  eigenvalues d k of the penalty matrix K, moderated by λ; λ controls
  the rate at which the ρ k (λ) decrease to zero. d 1 = d 2 = 0 and again
  linear functions are not penalized.
* One can reparametrize the smoothing spline using the basis vectors
  u k (the Demmler–Reinsch basis). In this case the smoothing spline
  solves
  $$\min\_\mathbf{\theta}\\|\mathbf{y} - \mathbf{U}\mathbf{\theta}\\|^2+
    \lambda\mathbf{\theta}^T\mathbf{D}\mathbf{\theta}
    \tag{5.21}$$
  where U has columns u k and D is a diagonal matrix with elements d k .
• df λ = trace(S λ ) = k=1 ρ k (λ). For projection smoothers, all the
  eigenvalues are 1, each one corresponding to a dimension of the pro-
  jection subspace.

Figure 5.8 depicts a smoothing spline matrix, with the rows ordered with
x. The banded nature of this representation suggests that a smoothing
spline is a local fitting method, much like the locally weighted regression
procedures in Chapter 6. The right panel shows in detail selected rows of
S, which we call the equivalent kernels. As λ → 0, df λ → N , and S λ → I,
the N -dimensional identity matrix. As λ → ∞, df λ → 2, and S λ → H, the
hat matrix for linear regression on x.