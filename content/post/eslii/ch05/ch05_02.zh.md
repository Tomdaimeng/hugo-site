+++
title = "ESL-5.2 分段多项式和样条函数"
summary = """
统计学习基础（译注）第五章第二节，第 141-150 页。
"""

date = 2018-10-18T08:39:00+08:00
lastmod = 2018-10-18T08:39:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++


We assume until Section 5.7 that X is one-dimensional. A piecewise poly-
nomial function f (X) is obtained by dividing the domain of X into contigu-
ous intervals, and representing f by a separate polynomial in each interval.
Figure 5.1 shows two simple piecewise polynomials. The first is piecewise
constant, with three basis functions:

$$h\_1(X) = I(X < \xi\_1),
h\_2(X) = I(\xi\_1 \leq X < \xi\_2),
h\_3(X) = I(\xi\_2 \leq X)
$$

Since these are positive over disjoint regions, the least squares estimate of
the model f (X) = m=1 β m h m (X) amounts to β̂ m = Ȳ m , the mean of Y
in the mth region.

The top right panel shows a piecewise linear fit. Three additional basis
functions are needed: h m+3 = h m (X)X, m = 1, . . . , 3. Except in special
cases, we would typically prefer the third panel, which is also piecewise
linear, but restricted to be continuous at the two knots. These continu-
ity restrictions lead to linear constraints on the parameters; for example,
f (ξ 1 − ) = f (ξ 1 + ) implies that β 1 + ξ 1 β 4 = β 2 + ξ 1 β 5 . In this case, since there
are two restrictions, we expect to get back two parameters, leaving four free
parameters.

A more direct way to proceed in this case is to use a basis that incorpo-
rates the constraints:

$$h\_1(X) = 1,
h\_2(X) = X,
h\_3(X) = (X - \xi\_1)\_+
h\_4(X) = (X - \xi\_2)\_+$$

where t + denotes the positive part. The function h 3 is shown in the lower
right panel of Figure 5.1. We often prefer smoother functions, and these
can be achieved by increasing the order of the local polynomial. Figure 5.2
shows a series of piecewise-cubic polynomials fit to the same data, with
increasing orders of continuity at the knots. The function in the lower
right panel is continuous, and has continuous first and second derivatives
at the knots. It is known as a cubic spline. Enforcing one more order of
continuity would lead to a global cubic polynomial. It is not hard to show
(Exercise 5.1) that the following basis represents a cubic spline with knots
at ξ 1 and ξ 2 :

$$\begin{gather}
h\_1(X) = 1, h\_3(X) = X^2,
h\_5(X) = (X - \xi\_1)^3\_+ \\\\ h\_2(X) = X,
h\_4(X) = X^3, h\_6(X) = (X - \xi\_2)^3\_+ 
\end{gather}\tag{5.3}$$

There are six basis functions corresponding to a six-dimensional linear space
of functions. A quick check confirms the parameter count: (3 regions)×(4
parameters per region) −(2 knots)×(3 constraints per knot)= 6.

More generally, an order-M spline with knots ξ j , j = 1, . . . , K is a
piecewise-polynomial of order M , and has continuous derivatives up to
order M − 2. A cubic spline has M = 4. In fact the piecewise-constant
function in Figure 5.1 is an order-1 spline, while the continuous piece-
wise linear function is an order-2 spline. Likewise the general form for the
truncated-power basis set would be

$$\begin{align}
h\_j(X) &= X^{j-1}, j = 1, \dots, M \\\\ h\_{M+l}(X) &=
(X-\xi\_l)^{M-1}\_+, l = 1, \dots, K
\end{align}$$

It is claimed that cubic splines are the lowest-order spline for which the
knot-discontinuity is not visible to the human eye. There is seldom any
good reason to go beyond cubic-splines, unless one is interested in smooth
derivatives. In practice the most widely used orders are M = 1, 2 and 4.

These fixed-knot splines are also known as regression splines. One needs
to select the order of the spline, the number of knots and their placement.
One simple approach is to parameterize a family of splines by the number
of basis functions or degrees of freedom, and have the observations x i de-
termine the positions of the knots. For example, the expression bs(x,df=7)
in R generates a basis matrix of cubic-spline functions evaluated at the N
observations in x , with the 7 − 3 = 4 1 interior knots at the appropriate per-
centiles of x (20, 40, 60 and 80th.) One can be more explicit, however; bs(x,
degree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,
with three interior knots, and returns an N × 4 matrix.

Since the space of spline functions of a particular order and knot sequence
is a vector space, there are many equivalent bases for representing them
(just as there are for ordinary polynomials.) While the truncated power
basis is conceptually simple, it is not too attractive numerically: powers of
large numbers can lead to severe rounding problems. The B-spline basis,
described in the Appendix to this chapter, allows for efficient computations
even when the number of knots K is large.

### 5.2.1 Natural Cubic Splines

We know that the behavior of polynomials fit to data tends to be erratic
near the boundaries, and extrapolation can be dangerous. These problems
are exacerbated with splines. The polynomials fit beyond the boundary
knots behave even more wildly than the corresponding global polynomials
in that region. This can be conveniently summarized in terms of the point-
wise variance of spline functions fit by least squares (see the example in the
next section for details on these variance calculations). Figure 5.3 compares
the pointwise variances for a variety of different models. The explosion of
the variance near the boundaries is clear, and inevitably is worst for cubic
splines.

A natural cubic spline adds additional constraints, namely that the func-
tion is linear beyond the boundary knots. This frees up four degrees of
freedom (two constraints each in both boundary regions), which can be
spent more profitably by sprinkling more knots in the interior region. This
tradeoff is illustrated in terms of variance in Figure 5.3. There will be a
price paid in bias near the boundaries, but assuming the function is lin-
ear near the boundaries (where we have less information anyway) is often
considered reasonable.

A natural cubic spline with K knots is represented by K basis functions.
One can start from a basis for cubic splines, and derive the reduced ba-
sis by imposing the boundary constraints. For example, starting from the
truncated power series basis described in Section 5.2, we arrive at (Exer-
cise 5.4):

$$N\_1(X) = 1, N\_2(X) = X,
N\_{k+2}(X) = d\_k(X) - d\_{K-1}(X)\tag{5.4}$$

where

$$d\_k(X) = \frac
{(X-\xi\_k)^3\_+ - (X-\xi\_K)^3\_+}
{\xi\_K - \xi\_k}\tag{5.5}$$

Each of these basis functions can be seen to have zero second and third
derivative for X ≥ ξ K .

### 5.2.2 Example: South African Heart Disease (Continued)

In Section 4.4.2 we fit linear logistic regression models to the South African
heart disease data. Here we explore nonlinearities in the functions using
natural splines. The functional form of the model is

$$\text{logit}[\text{Pr}(chd|X)] =
\theta\_0 + h\_1(X\_1)^T\theta\_1 + h\_2(X\_2)^T\theta\_2 + \dots +
  h\_p(X\_p)^T\theta\_p
\tag{5.6}$$

where each of the θ j are vectors of coefficients multiplying their associated
vector of natural spline basis functions h j .

We use four natural spline bases for each term in the model. For example,
with X 1 representing sbp , h 1 (X 1 ) is a basis consisting of four basis func-
tions. This actually implies three rather than two interior knots (chosen at
uniform quantiles of sbp ), plus two boundary knots at the extremes of the
data, since we exclude the constant term from each of the h j .

Since famhist is a two-level factor, it is coded by a simple binary or
dummy variable, and is associated with a single coefficient in the fit of the
model.

More compactly we can combine all p vectors of basis functions (and
the constant term) into one big vector h(X), and then model is simply
 with total number of parameters df = 1 + j=1 df j , the sum of
the parameters in each component term. Each basis function is evaluated
at each of the N samples, resulting in a N × df basis matrix H. At this
point the model is like any other linear logistic model, and the algorithms
described in Section 4.4.1 apply.

We carried out a backward stepwise deletion process, dropping terms
from this model while preserving the group structure of each term, rather
than dropping one coefficient at a time. The AIC statistic (Section 7.5) was
used to drop terms, and all the terms remaining in the final model would
cause AIC to increase if deleted from the model (see Table 5.1). Figure 5.4
shows a plot of the final model selected by the stepwise regression. The
functions displayed are f ˆ j (X j ) = h j (X j ) T θ̂ j for each variable X j . The
covariance matrix Cov( θ̂) = Σ is estimated by Σ̂ = (H T WH) −1 , where W
is the diagonal weight matrix from the logistic regression. Hence v j (X j ) =
Var[ f ˆ j (X j )] = h j (X j ) T Σ̂ jj h j (X j ) is the pointwise variance function of f ˆ j ,
where Cov( θ̂ j ) = Σ̂ jj is the appropriate sub-matrix of Σ̂. The shaded region
in each panel is defined by f ˆ j (X j ) ± 2 v j (X j ).

The AIC statistic is slightly more generous than the likelihood-ratio test
(deviance test). Both sbp and obesity are included in this model, while
they were not in the linear model. The figure explains why, since their
contributions are inherently nonlinear. These effects at first may come as
a surprise, but an explanation lies in the nature of the retrospective data.
These measurements were made sometime after the patients suffered a
heart attack, and in many cases they had already benefited from a healthier
diet and lifestyle, hence the apparent increase in risk at low values for
obesity and sbp . Table 5.1 shows a summary of the selected model.

### 5.2.3 Example: Phoneme Recognition

In this example we use splines to reduce flexibility rather than increase it;
the application comes under the general heading of functional modeling. In
the top panel of Figure 5.5 are displayed a sample of 15 log-periodograms
for each of the two phonemes “aa” and “ao” measured at 256 frequencies.
The goal is to use such data to classify a spoken phoneme. These two
phonemes were chosen because they are difficult to separate.

The input feature is a vector x of length 256, which we can think of as
a vector of evaluations of a function X(f ) over a grid of frequencies f . In
reality there is a continuous analog signal which is a function of frequency,
and we have a sampled version of it.

The gray lines in the lower panel of Figure 5.5 show the coefficients of
a linear logistic regression model fit by maximum likelihood to a training
sample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s. The
coefficients are also plotted as a function of frequency, and in fact we can
think of the model in terms of its continuous counterpart

$$\log\frac{\text{Pr}(aa|X)}{\text{Pr}(ao|X)} =
\int X(f)\beta(f)df
\tag{5.7}$$

which we approximate by

$$\sum^{256}\_{j=1}X(f\_j)\beta(f\_j) =
\sum^{256}\_{j=1}x\_j\beta\_j\tag{5.8}$$

The coefficients compute a contrast functional, and will have appreciable
values in regions of frequency where the log-periodograms differ between
the two classes.

The gray curves are very rough. Since the input signals have fairly strong
positive autocorrelation, this results in negative autocorrelation in the co-
The smooth red curve was obtained through a very simple use of natural
cubic splines. We
P M can represent the coefficient function as an expansion of
splines β(f ) = m=1 h m (f )θ m . In practice this means that β = Hθ where,
H is a p × M basis matrix of natural cubic splines, defined on the set of
frequencies. Here we used M = 12 basis functions, with knots uniformly
placed over the integers 1, 2, . . . , 256 representing the frequencies. Since
x T β = x T Hθ, we can simply replace the input features x by their filtered
versions x ∗ = H T x, and fit θ by linear logistic regression on the x ∗ . The
red curve is thus β̂(f ) = h(f ) T θ̂.ively provides only four obser-
The smooth red curve was obtained through a very simple use of natural
cubic splines. We
P M can represent the coefficient function as an expansion of
splines β(f ) = m=1 h m (f )θ m . In practice this means that β = Hθ where,
H is a p × M basis matrix of natural cubic splines, defined on the set of
frequencies. Here we used M = 12 basis functions, with knots uniformly
placed over the integers 1, 2, . . . , 256 representing the frequencies. Since
x T β = x T Hθ, we can simply replace the input features x by their filtered
versions x ∗ = H T x, and fit θ by linear logistic regression on the x ∗ . The
red curve is thus β̂(f ) = h(f ) T θ̂.
The smooth red curve was obtained through a very simple use of natural
cubic splines. We
P M can represent the coefficient function as an expansion of
splines β(f ) = m=1 h m (f )θ m . In practice this means that β = Hθ where,
H is a p × M basis matrix of natural cubic splines, defined on the set of
frequencies. Here we used M = 12 basis functions, with knots uniformly
placed over the integers 1, 2, . . . , 256 representing the frequencies. Since
x T β = x T Hθ, we can simply replace the input features x by their filtered
versions x ∗ = H T x, and fit θ by linear logistic regression on the x ∗ . The
red curve is thus β̂(f ) = h(f ) T θ̂.
Applications such as this permit a natural regularization. We force the
coefficients to vary smoothly as a function of frequency. The red curve in the
lower panel of Figure 5.5 shows such a smooth coThe smooth red curve was obtained through a very simple use of natural
cubic splines. We
P M can represent the coefficient function as an expansion of
splines β(f ) = m=1 h m (f )θ m . In practice this means that β = Hθ where,
H is a p × M basis matrix of natural cubic splines, defined on the set of
frequencies. Here we used M = 12 basis functions, with knots uniformly
placed over the integers 1, 2, . . . , 256 representing the frequencies. Since
x T β = x T Hθ, we can simply replace the input features x by their filtered
versions x ∗ = H T x, and fit θ by linear logistic regression on the x ∗ . The
red curve is thus β̂(f ) = h(f ) T θ̂.efficient curve fit to these
data. We see that the lower frequencies offer the most discriminatory power.
Not only does the smoothing allow easier interpretation of the contrast, it
also produces a more accurate classifier:

The smooth red curve was obtained through a very simple use of natural
cubic splines. We
P M can represent the coefficient function as an expansion of
splines β(f ) = m=1 h m (f )θ m . In practice this means that β = Hθ where,
H is a p × M basis matrix of natural cubic splines, defined on the set of
frequencies. Here we used M = 12 basis functions, with knots uniformly
placed over the integers 1, 2, . . . , 256 representing the frequencies. Since
x T β = x T Hθ, we can simply replace the input features x by their filtered
versions x ∗ = H T x, and fit θ by linear logistic regression on the x ∗ . The
red curve is thus β̂(f ) = h(f ) T θ̂.