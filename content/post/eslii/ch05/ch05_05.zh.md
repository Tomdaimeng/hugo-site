+++
title = "ESL-5.5 Automatic Selection of the Smoothing Parameters"
summary = """
统计学习基础（译注）第五章第五节，第 156-161 页。
"""

date = 2018-10-23T10:44:00+08:00
lastmod = 2018-10-23T10:44:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++


The smoothing parameters for regression splines encompass the degree of
the splines, and the number and placement of the knots. For smoothing
splines, we have only the penalty parameter λ to select, since the knots are
at all the unique training X’s, and cubic degree is almost always used in
practice.

Selecting the placement and number of knots for regression splines can be
a combinatorially complex task, unless some simplifications are enforced.
The MARS procedure in Chapter 9 uses a greedy algorithm with some
additional approximations to achieve a practical compromise. We will not
discuss this further here.

### 5.5.1 Fixing the Degrees of Freedom

Since df λ = trace(S λ ) is monotone in λ for smoothing splines, we can in-
vert the relationship and specify λ by fixing df. In practice this can be
achieved by simple numerical methods. So, for example, in R one can use
smooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-
ages a more traditional mode of model selection, where we might try a cou-
ple of different values of df, and select one based on approximate F -tests,
residual plots and other more subjective criteria. Using df in this way pro-
vides a uniform approach to compare many different smoothing methods.
It is particularly useful in generalized additive models (Chapter 9), where
several smoothing methods can be simultaneously used in one model.

### 5.5.2 The Bias–Variance Tradeoff

Figure 5.9 shows the effect of the choice of df λ when using a smoothing
spline on a simple example:

$$\begin{align}
Y &= f(X) + \varepsilon \\\\ f(X) &=
\frac{\sin(12(X + 0.2))}{X + 0.2}
\end{align}\tag{5.22}$$

with X ∼ U [0, 1] and ε ∼ N (0, 1). Our training sample consists of N = 100
pairs x i , y i drawn independently from this model.

The fitted splines for three different values of df λ are shown. The yellow
shaded region in the figure represents the pointwise standard error of f ˆ λ ,
that is, we have shaded the region between f ˆ λ (x) ± 2 · se( f ˆ λ (x)). Since
f̂ = S λ y,

$$\begin{align}
\text{Cov}(\hat{\mathbf{f}}) &=
  \mathbf{S}\_\lambda \text{Cov}(\mathbf{y}) \mathbf{S}\_\lambda^T \\\\ &=
  \mathbf{S}\_\lambda \mathbf{S}\_\lambda^T \tag{5.23}
\end{align}$$

The diagonal contains the pointwise variances at the training x i . The bias
is given by

$$\begin{align}
\text{Bias}(\hat{\mathbf{f}}) &=
  \mathbf{f} - E(\hat{\mathbf{f}}) \\\\ &=
  \mathbf{f} - \mathbf{S}\_\lambda \mathbf{f} \tag{5.24}
\end{align}$$

where f is the (unknown) vector of evaluations of the true f at the training
X’s. The expectations and variances are with respect to repeated draws
of samples of size N = 100 from the model (5.22). In a similar fashion
Var( f ˆ λ (x 0 )) and Bias( f ˆ λ (x 0 )) can be computed at any point x 0 (Exer-
cise 5.10). The three fits displayed in the figure give a visual demonstration
of the bias-variance tradeoff associated with selecting the smoothing
parameter.

* df λ = 5: The spline under fits, and clearly trims down the hills and fills in
the valleys. This leads to a bias that is most dramatic in regions of
high curvature. The standard error band is very narrow, so we esti-
mate a badly biased version of the true function with great reliability!
* df λ = 9: Here the fitted function is close to the true function, although a
slight amount of bias seems evident. The variance has not increased
appreciably.
* df λ = 15: The fitted function is somewhat wiggly, but close to the true
function. The wiggliness also accounts for the increased width of the
standard error bands—the curve is starting to follow some individual
points too closely.

Note that in these figures we are seeing a single realization of data and
hence fitted spline f ˆ in each case, while the bias involves an expectation
E( f ˆ ). We leave it as an exercise (5.10) to compute similar figures where the
bias is shown as well. The middle curve seems “just right,” in that it has
achieved a good compromise between bias and variance.

The integrated squared prediction error (EPE) combines both bias and
variance in a single summary:

$$\begin{align}
\text{EPE}(\hat{f}\_\lambda) &=
  E(Y - \hat{f}\_\lambda(X))^2 \\\\ &=
  \text{Var}(Y) + E \left [ \text{Bias}^2(\hat{f}\_\lambda(x)) + \text{Var}(\hat{f}\_\lambda(x)) \right ] \\\\ &=
  \sigma^2 + \text{MSE}(\hat{f}\lambda) \tag{5.25}
\end{align}$$

Note that this is averaged both over the training sample (giving rise to f ˆ λ ),
and the values of the (independently chosen) prediction points (X, Y ). EPE
is a natural quantity of interest, and does create a tradeoff between bias
and variance. The blue points in the top left panel of Figure 5.9 suggest
that df λ = 9 is spot on!

Since we don’t know the true function, we do not have access to EPE, and
need an estimate. This topic is discussed in some detail in Chapter 7, and
techniques such as K-fold cross-validation, GCV and C p are all in common
use. In Figure 5.9 we include the N -fold (leave-one-out) cross-validation
curve:

$$\begin{align}
\text{CV}(\hat{f}\_\lambda) &=
  \frac{1}{N} \sum\_{i=1}^N (y\_i - \hat{f}\_\lambda^{(-i)}(x\_i))^2 \tag{5.26} \\\\ &=
  \frac{1}{N} \sum\_{i=1}^N \left ( \frac{y\_i - \hat{f}\_\lambda(x\_i)}{1 - S\_\lambda(i,i)} \right )^2 \tag{5.27}
\end{align}$$


which can (remarkably) be computed for each value of λ from the original
fitted values and the diagonal elements S λ (i, i) of S λ (Exercise 5.13).

The EPE and CV curves have a similar shape, but the entire CV curve
is above the EPE curve. For some realizations this is reversed, and overall
the CV curve is approximately unbiased as an estimate of the EPE curve.