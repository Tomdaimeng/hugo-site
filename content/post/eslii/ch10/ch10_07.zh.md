+++
title = "ESL-10.7 “Off-the-Shelf” Procedures for Data Mining"
summary = """
统计学习基础（译注）第十章第七节，第 350-352 页。
"""

date = 2019-01-22T15:35:00+08:00
lastmod = 2019-01-22T15:35:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = []
categories = []

[header]
image = ""
caption = ""
preview = true
+++

Predictive learning is an important aspect of data mining. As can be seen
from this book, a wide variety of methods have been developed for predic-
tive learning from data. For each particular method there are situations
for which it is particularly well suited, and others where it performs badly
compared to the best that can be done with that data. We have attempted
to characterize appropriate situations in our discussions of each of the re-
spective methods. However, it is seldom known in advance which procedure
will perform best or even well for any given problem. Table 10.1 summarizes
some of the characteristics of a number of learning methods.


| Characteristic | NN | SVM | Trees | MARS | k-NN |
|----------------|----|-----|-------|------|------|
| Natural handling of data of “mixed” type | :x: | :x: | :heavy_check_mark: |  :heavy_check_mark: |  :x: | 
| Handling of missing values | :x: | :x: | :heavy_check_mark: |  :heavy_check_mark: |  :heavy_check_mark: | 
| Robustness to outliers in input space | :x: | :x: | :heavy_check_mark: |  :x: |  :heavy_check_mark: | 
| Insensitive to monotone transformations of inputs | :x: | :x: | :heavy_check_mark: |  :x: |  :x: | 
| Computational scalability (large N ) | :x: | :x: | :heavy_check_mark: |  :heavy_check_mark: |  :x: | 
| Ability to deal with irrelevant inputs | :x: | :x: | :heavy_check_mark: |  :heavy_check_mark: |  :x: | 
| Ability to extract linear combinations of features | :heavy_check_mark: |  :heavy_check_mark: |:x: | :x: | :heavy_minus_sign: | 
| Interpretability | :x: | :x: | :heavy_minus_sign: | :heavy_check_mark: | :x: | 
| Predictive power | :heavy_check_mark: |  :heavy_check_mark: |:x: | :heavy_minus_sign: | :heavy_check_mark: |  
**表10.1**：Some characteristics of different learning methods.
Key: :heavy_check_mark: = good, :heavy_minus_sign: = fair, and :x:=poor.

Industrial and commercial data mining applications tend to be especially
challenging in terms of the requirements placed on learning procedures.
Data sets are often very large in terms of number of observations and
number of variables measured on each of them. Thus, computational con-
siderations play an important role. Also, the data are usually messy: the
inputs tend to be mixtures of quantitative, binary, and categorical vari-
ables, the latter often with many levels. There are generally many missing
values, complete observations being rare. Distributions of numeric predic-
tor and response variables are often long-tailed and highly skewed. This
is the case for the spam data (Section 9.1.2); when fitting a generalized
additive model, we first log-transformed each of the predictors in order to
get a reasonable fit. In addition they usually contain a substantial fraction
of gross mis-measurements (outliers). The predictor variables are generally
measured on very different scales.

In data mining applications, usually only a small fraction of the large
number of predictor variables that have been included in the analysis are
actually relevant to prediction. Also, unlike many applications such as pat-
tern recognition, there is seldom reliable domain knowledge to help create
especially relevant features and/or filter out the irrelevant ones, the inclu-
sion of which dramatically degrades the performance of many methods.

In addition, data mining applications generally require interpretable mod-
els. It is not enough to simply produce predictions. It is also desirable to
have information providing qualitative understanding of the relationship
between joint values of the input variables and the resulting predicted re-
sponse value. Thus, black box methods such as neural networks, which can
be quite useful in purely predictive settings such as pattern recognition,
are far less useful for data mining.

These requirements of speed, interpretability and the messy nature of
the data sharply limit the usefulness of most learning procedures as off-
the-shelf methods for data mining. An “off-the-shelf” method is one that
can be directly applied to the data without requiring a great deal of time-
consuming data preprocessing or careful tuning of the learning procedure.

Of all the well-known learning methods, decision trees come closest to
meeting the requirements for serving as an off-the-shelf procedure for data
mining. They are relatively fast to construct and they produce interpretable
models (if the trees are small). As discussed in Section 9.2, they naturally
incorporate mixtures of numeric and categorical predictor variables and
missing values. They are invariant under (strictly monotone) transforma-
tions of the individual predictors. As a result, scaling and/or more general
transformations are not an issue, and they are immune to the effects of pre-
dictor outliers. They perform internal feature selection as an integral part
of the procedure. They are thereby resistant, if not completely immune,
to the inclusion of many irrelevant predictor variables. These properties of
decision trees are largely the reason that they have emerged as the most
popular learning method for data mining.

Trees have one aspect that prevents them from being the ideal tool for
predictive learning, namely inaccuracy. They seldom provide predictive ac-
curacy comparable to the best that can be achieved with the data at hand.
As seen in Section 10.1, boosting decision trees improves their accuracy,
often dramatically. At the same time it maintains most of their desirable
properties for data mining. Some advantages of trees that are sacrificed by
boosting are speed, interpretability, and, for AdaBoost, robustness against
overlapping class distributions and especially mislabeling of the training
data. A gradient boosted model (GBM) is a generalization of tree boosting
that attempts to mitigate these problems, so as to produce an accurate and
effective off-the-shelf procedure for data mining.