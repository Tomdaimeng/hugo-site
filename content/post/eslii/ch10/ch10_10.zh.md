+++
title = "ESL-10.10 Numerical Optimization via Gradient Boosting"
summary = """
统计学习基础（译注）第十章第十节，第 358-361 页。
"""

date = 2019-01-27T16:13:00+08:00
lastmod = 2019-01-27T16:13:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = []
categories = []

[header]
image = ""
caption = ""
preview = true
+++

Fast approximate algorithms for solving (10.29) with any differentiable loss
criterion can be derived by analogy to numerical optimization. The loss in
using f (x) to predict y on the training data is

$$L(f) = \sum\_{i=1}^N L(y\_i, f(x\_i)) \tag{10.33}$$

The goal is to minimize L(f ) with respect to f , where here f (x) is con-
strained to be a sum of trees (10.28). Ignoring this constraint, minimizing
(10.33) can be viewed as a numerical optimization

$$\hat{\mathbf{f}} = \underset{\mathbf{f}}{\text{argmin }}
L(\mathbf{f}) \tag{10.34}$$

where the “parameters” f ∈ IR N are the values of the approximating func-
tion f (x i ) at each of the N data points x i :

$$\mathbf{f} = \\{f(x\_1), f(x\_2), \dots, f(x\_N)\\}$$

Numerical optimization procedures solve (10.34) as a sum of component
vectors

$$\hat{\mathbf{f}}\_M = \sum\_{m=0}^M \mathbf{h}\_m,
\mathbf{h}\_m \in \mathbb{R}^N$$

where f 0 = h 0 is an initial guess, and each successive f m is induced based
on the current parameter vector f m−1 , which is the sum of the previously
induced updates. Numerical optimization methods differ in their prescrip-
tions for computing each increment vector h m (“step”).

### 10.10.1 Steepest Descent

Steepest descent chooses h m = −ρ m g m where ρ m is a scalar and g m ∈ IR N
is the gradient of L(f ) evaluated at f = f m−1 . The components of the
gradient g m are

$$g\_{im} = \left[\frac
{\partial L(y\_i, f(x\_i))}{\partial f(x\_i)}
\right]\_{f(x\_i) = f\_{m-1}(x\_i)} \tag{10.35}$$

The step length ρ m is the solution to

$$\rho\_m = \underset{\rho}{\text{argmin }}
L(\mathbf{f}\_{m-1} - \rho \mathbf{g}\_m) \tag{10.36}$$

The current solution is then updated

$$\mathbf{f}\_m = \mathbf{f}\_{m-1} - \rho\_m \mathbf{g}\_m$$

and the process repeated at the next iteration. Steepest descent can be
viewed as a very greedy strategy, since −g m is the local direction in IR N
for which L(f ) is most rapidly decreasing at f = f m−1 .

### 10.10.2 Gradient Boosting

Forward stagewise boosting (Algorithm 10.2) is also a very greedy strategy.
At each step the solution tree is the one that maximally reduces (10.29),
given the current model f m−1 and its fits f m−1 (x i ). Thus, the tree predic-
tions T (x i ; Θ m ) are analogous to the components of the negative gradient
(10.35). The principal difference between them is that the tree compo-
nents t m = (T (x 1 ; Θ m ), . . . , T (x N ; Θ m ) are not independent. They are con-
strained to be the predictions of a J m -terminal node decision tree, whereas
the negative gradient is the unconstrained maximal descent direction.

The solution to (10.30) in the stagewise approach is analogous to the line
search (10.36) in steepest descent. The difference is that (10.30) performs
a separate line search for those components of t m that correspond to each
separate terminal region {T (x i ; Θ m )} x i ∈R jm .

If minimizing loss on the training data (10.33) were the only goal, steep-
est descent would be the preferred strategy. The gradient (10.35) is trivial
to calculate for any differentiable loss function L(y, f (x)), whereas solving
(10.29) is difficult for the robust criteria discussed in Section 10.6. Unfor-
tunately the gradient (10.35) is defined only at the training data points x i ,
whereas the ultimate goal is to generalize f M (x) to new data not repre-
sented in the training set.

A possible resolution to this dilemma is to induce a tree T (x; Θ m ) at the
mth iteration whose predictions t m are as close as possible to the negative
gradient. Using squared error to measure closeness, this leads us to

$$\tilde{\Theta}\_m = \underset{\Theta}{\text{argmin }}
\sum\_{i=1}^N (-g\_{im} - T(x\_i; \Theta))^2 \tag{10.37}$$

That is, one fits the tree T to the negative gradient values (10.35) by least
squares. As noted in Section 10.9 fast algorithms exist for least squares
decision tree induction. Although the solution regions R̃ jm to (10.37) will
not be identical to the regions R jm that solve (10.29), it is generally sim-
ilar enough to serve the same purpose. In any case, the forward stagewise
boosting procedure, and top-down decision tree induction, are themselves
approximation procedures. After constructing the tree (10.37), the corre-
sponding constants in each region are given by (10.30).

| Setting | Loss Function | $-\partial L(y\_i, f(x\_i)) / \partial f(x\_i)$ |
|---------|---------------|-------------------------------------------------|
| Regression | $\frac{1}{2}[y\_i - f(x\_i)]^2$ | $y\_i - f(x\_i)$ |
| Regression | $\|y\_i - f(x\_i)\|$ | $\text{sign}[y\_i - f(x\_i)]$ |
| Regression | Huber | $\begin{align}& y\_i-f(x\_i)\text{ for }\|y\_i-f(x\_i)\|\leq\delta\_m \\\\ & \delta\_m\text{sign}[y\_i-f(x\_i)]\text{ for }\|y\_i-f(x\_i)\|>\delta\_m\\\\ & \text{where }\delta\_m=\alpha\text{th-quantile}\\{\|y\_i-f(x\_i)\|\\}\end{align}$ |
| Classification | Deviance | kth component: $I(y\_i = \mathcal{G}\_k) - p\_k(x\_i)$ |
**TABLE 10.2**. Gradients for commonly used loss functions.

Table 10.2 summarizes the gradients for commonly used loss functions.
For squared error loss, the negative gradient is just the ordinary residual
−g im = y i − f m−1 (x i ), so that (10.37) on its own is equivalent standard
least squares boosting. With absolute error loss, the negative gradient is
the sign of the residual, so at each iteration (10.37) fits the tree to the
sign of the current residuals by least squares. For Huber M-regression, the
negative gradient is a compromise between these two (see the table).

For classification the loss function is the multinomial deviance (10.22),
and K least squares trees are constructed at each iteration. Each tree T km
is fit to its respective negative gradient vector g km ,

$$\begin{align}
-g\_{ikm} &= \frac
{\partial L(y\_i, f\_{1m}(x\_i), \dots, f\_{Nm}(x\_i))}
{\partial f\_{km}(x\_i)}
\\\\ &= I(y\_i = \mathcal{G}\_k) - p\_k(x\_i)
\tag{10.38}\end{align}$$

with p k (x) given by (10.21). Although K separate trees are built at each
iteration, they are related through (10.21). For binary classification (K =
2), only one tree is needed (exercise 10.10).

### 10.10.3 Implementations of Gradient Boosting

----------

#### Algorithm 10.3 Gradient Tree Boosting Algorithm.

1. Initialize
   $f\_0(x) = \text{argmin}\_\gamma \sum\_{i=1}^N L(y\_i, \gamma)$.
2. For m = 1 to M :
   1. For i = 1, 2, . . . , N compute
      $$r\_{im} = - \left[\frac
      {\partial L(y\_i, f(x\_i))}{\partial f(x\_i)}
      \right]\_{f = f\_{m-1}}$$
   2. Fit a regression tree to the targets r im giving terminal regions
      R jm , j = 1, 2, . . . , J m .
   3. For j = 1, 2, . . . , J m compute
      $$\gamma\_{jm} = \underset{\gamma}{\text{argmin }}
      \sum\_{x\_i \in R\_{jm}} L(y\_i, f\_{m-1}(x\_i) + \gamma)$$
   4. Update
      $f\_m(x) = f\_{m-1}(x) + \sum\_{j=1}^{J\_m} \gamma\_{jm} I(x \in R\_{jm})$.
3. Output f ˆ (x) = f M (x).

----------

Algorithm 10.3 presents the generic gradient tree-boosting algorithm for
regression. Specific algorithms are obtained by inserting different loss cri-
teria L(y, f (x)). The first line of the algorithm initializes to the optimal
constant model, which is just a single terminal node tree. The components
of the negative gradient computed at line 2(a) are referred to as general-
ized or pseudo residuals, r. Gradients for commonly used loss functions are
summarized in Table 10.2.

The algorithm for classification is similar. Lines 2(a)–(d) are repeated
K times at each iteration m, once for each class using (10.38). The result
at line 3 is K different (coupled) tree expansions f kM (x), k = 1, 2, . . . , K.
These produce probabilities via (10.21) or do classification as in (10.20).
Details are given in Exercise 10.9. Two basic tuning parameters are the
number of iterations M and the sizes of each of the constituent trees
J m , m = 1, 2, . . . , M .

The original implementation of this algorithm was called MART for
“multiple additive regression trees,” and was referred to in the first edi-
tion of this book. Many of the figures in this chapter were produced by
MART. Gradient boosting as described here is implemented in the R gbm
package (Ridgeway, 1999, “Gradient Boosted Models”), and is freely avail-
able. The gbm package is used in Section 10.14.2, and extensively in Chap-
ters 16 and 15. Another R implementation of boosting is mboost (Hothorn
and Bühlmann, 2006). A commercial implementation of gradient boost-
ing/MART called TreeNet  is available from Salford Systems, Inc.