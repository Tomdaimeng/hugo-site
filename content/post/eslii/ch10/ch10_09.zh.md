+++
title = "ESL-10.9 Boosting Trees"
summary = """
统计学习基础（译注）第十章第九节，第 353-358 页。
"""

date = 2019-01-24T17:34:00+08:00
lastmod = 2019-01-24T17:34:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = []
categories = []

[header]
image = ""
caption = ""
preview = true
+++

Regression and classification trees are discussed in detail in Section 9.2.
They partition the space of all joint predictor variable values into disjoint
regions R j , j = 1, 2, . . . , J, as represented by the terminal nodes of the tree.

A constant γ j is assigned to each such region and the predictive rule is

$$x \in R\_j \Rightarrow f(x) = \gamma\_j$$

Thus a tree can be formally expressed as

$$T(x; \Theta) = \sum\_{j=1}^J \gamma\_j I(x \in R\_j) \tag{10.25}$$

with parameters Θ = {R j , γ j } J 1 . J is usually treated as a meta-parameter.
The parameters are found by minimizing the empirical risk

$$\hat{\Theta} = \underset{\Theta}{\text{argmin}}
\sum\_{j=1}^J \sum\_{x\_i \in R\_j} L(y\_i, \gamma\_j) \tag{10.26}$$

This is a formidable combinatorial optimization problem, and we usually
settle for approximate suboptimal solutions. It is useful to divide the opti-
mization problem into two parts:

* Finding γ j given R j : Given the R j , estimating the γ j is typically trivial,
  and often γ̂ j = ȳ j , the mean of the y i falling in region R j . For mis-
  classification loss, γ̂ j is the modal class of the observations falling in
  region R j .
* Finding R j : This is the difficult part, for which approximate solutions are
  found. Note also that finding the R j entails estimating the γ j as well.
  A typical strategy is to use a greedy, top-down recursive partitioning
  algorithm to find the R j . In addition, it is sometimes necessary to
  approximate (10.26) by a smoother and more convenient criterion for
  optimizing the R j :
  $$\tilde{\Theta} = \underset{\Theta}{\text{argmin}}
  \sum\_{i=1}^N \tilde{L}(y\_i,T(x\_i, \Theta)) \tag{10.27}$$
  Then given the R̂ j = R̃ j , the γ j can be estimated more precisely
  using the original criterion.

In Section 9.2 we described such a strategy for classification trees. The Gini
index replaced misclassification loss in the growing of the tree (identifying
the R j ).

The boosted tree model is a sum of such trees,

$$f\_M(x) = \sum\_{m=1}^M T(x; \Theta\_m) \tag{10.28}$$

induced in a forward stagewise manner (Algorithm 10.2). At each step in
the forward stagewise procedure one must solve

$$\hat{\Theta}\_m = \underset{\Theta\_m}{\text{argmin}}
\sum\_{i=1}^N L(y\_i, f\_{m-1}(x\_i) + T(x\_i; \Theta\_m)) \tag{10.29}$$

for the region set and constants Θ m = {R jm , γ jm } J 1 m of the next tree, given
the current model f m−1 (x).

Given the regions R jm , finding the optimal constants γ jm in each region
is typically straightforward:

$$\hat{\gamma}\_{jm} = \underset{\gamma\_{jm}}{\text{argmin}}
\sum\_{x\_i \in R\_{jm}} L(y\_i, f\_{m-1}(x) + \gamma\_{jm})
\tag{10.30}$$

Finding the regions is difficult, and even more difficult than for a single
tree. For a few special cases, the problem simplifies.

For squared-error loss, the solution to (10.29) is no harder than for a
single tree. It is simply the regression tree that best predicts the current
residuals y i − f m−1 (x i ), and γ̂ jm is the mean of these residuals in each
corresponding region.

For two-class classification and exponential loss, this stagewise approach
gives rise to the AdaBoost method for boosting classification trees (Algo-
rithm 10.1). In particular, if the trees T (x; Θ m ) are restricted to be scaled
classification trees, then we showed in Section 10.4 that the solution to
(10.29) is the tree that minimizes the weighted error rate i=1 w i I(y i 6 =
T (x i ; Θ m )) with weights w i = e −y i f m−1 (x i ) . By a scaled classification
tree, we mean β m T (x; Θ m ), with the restriction that γ jm ∈ {−1, 1}).

Without this restriction, (10.29) still simplifies for exponential loss to a
weighted exponential criterion for the new tree:

$$\hat{\Theta}\_m = \underset{\Theta\_m}{\text{argmin}}
\sum\_{i=1}^N w\_i^{(m)} \exp[-y\_i T(x\_i; \Theta\_m)] \tag{10.31}$$

It is straightforward to implement a greedy recursive-partitioning algorithm
using this weighted exponential loss as a splitting criterion. Given the R jm ,
one can show (Exercise 10.7) that the solution to (10.30) is the weighted
log-odds in each corresponding region

$$\gamma\_{jm} = \log \frac
{\sum\_{x\_i \in R\_{jm}} w\_i^{(m)} I(y\_i = 1)}
{\sum\_{x\_i \in R\_{jm}} w\_i^{(m)} I(y\_i = -1)}
\tag{10.32}$$

This requires a specialized tree-growing algorithm; in practice, we prefer
the approximation presented below that uses a weighted least squares re-
gression tree.

Using loss criteria such as the absolute error or the Huber loss (10.23) in
place of squared-error loss for regression, and the deviance (10.22) in place
of exponential loss for classification, will serve to robustify boosting trees.
Unfortunately, unlike their nonrobust counterparts, these robust criteria
do not give rise to simple fast boosting algorithms.

For more general loss criteria the solution to (10.30), given the R jm ,
is typically straightforward since it is a simple “location” estimate. For
absolute loss it is just the median of the residuals in each respective region.
For the other criteria fast iterative algorithms exist for solving (10.30),
and usually their faster “single-step” approximations are adequate. The
problem is tree induction. Simple fast algorithms do not exist for solving
(10.29) for these more general loss criteria, and approximations like (10.27)
become essential.