+++
title = "ESL-10.9 提升树模型"
summary = """
统计学习基础（译注）第十章第九节，第 353-358 页。
对树模型使用提升算法，一般会用一个近似过程来代替对整体的最优化计算。
这个过程可分为两部分，给定分割区域后确定区域上的拟合值通常比较容易，
寻找最优分割区域通常比较困难。
"""

date = 2019-01-27T16:10:00+08:00
lastmod = 2019-01-27T16:10:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = []
categories = []

[header]
image = ""
caption = ""
preview = true
+++

[第 9.2 节]({{< ref "/post/eslii/ch09/ch09_02.zh.md" >}})
很详细地介绍了回归和分类树模型。
它们将所有自变量的联合输入变量空间分割成不相交的区域
$R\_j$，$j=1,2,\dots,J$，
也就是树模型中的终节点。

每个这样的区域会指派一个常数 $\gamma\_j$，而预测的规则为

$$x \in R\_j \Rightarrow f(x) = \gamma\_j$$

因此一个树模型可正式地表述为

$$T(x; \Theta) = \sum\_{j=1}^J \gamma\_j I(x \in R\_j) \tag{10.25}$$

其中的参数 $\Theta = \\{R\_j, \gamma\_j\\}\_1^J$。
$J$ 通常被当作一个超（meta）参数。
这些参数[^1]可通过最小化经验风险/损失来计算：

$$\hat{\Theta} = \underset{\Theta}{\text{argmin}}
\sum\_{j=1}^J \sum\_{x\_i \in R\_j} L(y\_i, \gamma\_j) \tag{10.26}$$

这是一个难以处理的组合最优化问题，
而我们通常可以接受一个近似的次优解。
一个策略是将最优化问题分解成两部分：

* 给定 $R\_j$ 后寻找 $\gamma\_j$：  
  给定了 $R\_j$ 后，一般可很容易地估计出 $\gamma\_j$，
  通常有 $\hat{\gamma}\_j = \bar{y}\_j$，
  即落入区域 $R\_j$ 中的 $y\_j$ 的均值。
  对误分类损失来说，$\hat{\gamma}\_j$
  为落入区域 $R\_j$ 中观测样本的众数类。
* 寻找 $R\_j$：  
  这个部分比较困难，通常会使用某种近似的解。
  同时也注意到在寻找 $R\_j$ 时也需要用到 $\gamma\_j$ 的估计。
  一个通常的策略是使用贪婪的自上而下的循环分割算法来寻找 $R\_j$。
  另外，在对 $R\_j$ 最优化的过程中有时需要用一个更平滑和方便的准则
  替代表达式 10.26：
  $$\tilde{\Theta} = \underset{\Theta}{\text{argmin}}
  \sum\_{i=1}^N \tilde{L}(y\_i,T(x\_i, \Theta)) \tag{10.27}$$
  然后对给定的 $\hat{R}\_j = \tilde{R}\_j$，
  可以用原本的准则更准确地估计出 $\gamma\_j$。

[第 9.2 节]({{< ref "/post/eslii/ch09/ch09_02.zh.md" >}})
介绍了这个策略在分类树模型上的应用。
其中在生成树模型（寻找 $R\_j$）时
用基尼系数（Gini index）代替了误分类损失。

提升树模型（boosted tree model）为这样的一些树模型的和：

$$f\_M(x) = \sum\_{m=1}^M T(x; \Theta\_m) \tag{10.28}$$

这个模型是由一个前向分段的形式（算法 10.2）推导出的。
在这个前向分段过程中的每一步，需要求解：

$$\hat{\Theta}\_m = \underset{\Theta\_m}{\text{argmin}}
\sum\_{i=1}^N L(y\_i, f\_{m-1}(x\_i) + T(x\_i; \Theta\_m)) \tag{10.29}$$

给定了当前的模型 $f\_{m-1}(x)$，
其中的最小化参数为下一个树模型的区域的集合以及常数
$\Theta\_m = \\{R\_{jm}, \gamma\_{jm}\\}\_1^{J\_m}$。

给定了区域 $R\_{jm}$ 后，
一般很容易求出每个区域上的最优常数 $\gamma\_{jm}$：

$$\hat{\gamma}\_{jm} = \underset{\gamma\_{jm}}{\text{argmin}}
\sum\_{x\_i \in R\_{jm}} L(y\_i, f\_{m-1}(x) + \gamma\_{jm})
\tag{10.30}$$

但寻找这些区域并不容易，甚至比在单一的树模型中更加困难。
不过在一些特例中，可简化这个问题。

对于平方误差损失函数，求解表达式 10.29 的复杂度与单个树模型相符。
区域的解就是可最好地预测当前残差 $y\_i - f\_{m-1}(x\_i)$ 的回归树模型，
而 $\gamma\_{jm}$ 即为这些残差在每个区域上的均值。

对于两个类别的分类问题以及指数损失函数，
这个分段的方法会得出
作为提升分类树模型的自适应提升（AdaBoost）方法（算法 10.1）。
特别是如果约束树模型 $T(x; \Theta\_m)$ 在缩放（scaled）分类树模型，
则[第 10.4 节]({{< ref "/post/eslii/ch10/ch10_04.zh.md" >}})
证明了表达式 10.29 的解为
在权重 $w\_i = e^{-y\_i f\_{m-1}(x\_i)}$ 下
最小化加权误差率
$\sum\_{i=1}^N w\_i^{(m)} I(y\_i \ne T(x\_i; \Theta\_m))$
的树模型。
所谓缩放（scaled）分类树模型，指的是
$\beta\_m T(x\_i; \Theta\_m)$ 的形式，
其中约束了 $\gamma\_{jm} \in \\{-1, 1\\}$。

若没有这个约束，
对于指数损失函数表达式 10.29 仍可简化成
对一个加权指数准则求解新的树模型：

$$\hat{\Theta}\_m = \underset{\Theta\_m}{\text{argmin}}
\sum\_{i=1}^N w\_i^{(m)} \exp[-y\_i T(x\_i; \Theta\_m)] \tag{10.31}$$

那么可以直接地利用这个加权指数损失作为分割准则，
而进行一个贪婪的递归分割算法。
给定 $R\_{jm}$ 后，
可证明（练习 10.7）表达式 10.30 的解是
每个相应区域的加权对数几率：

$$\hat{\gamma}\_{jm} = \log \frac
{\sum\_{x\_i \in R\_{jm}} w\_i^{(m)} I(y\_i = 1)}
{\sum\_{x\_i \in R\_{jm}} w\_i^{(m)} I(y\_i = -1)}
\tag{10.32}$$

这就需要一个特殊的生成树模型的算法；
实践中更倾向于使用下面所述的
利用加权最小二乘回归树模型的一个近似方法。

使用其他的损失准则
可使提升树模型更稳健，
比如在回归问题中用绝对误差或 Huber 损失（表达式 10.23）
代替平方误差损失，
或在分类问题中用诸如偏差（表达式 10.22）
代替指数损失。
不过这些稳健的准则与原来的非稳健准则不同，
不再可得出简单快速的提升算法。

对于更一般的损失准则，
给定了 $R\_{jm}$ 后的表达式 10.30 的求解通常比较容易，
因为这是一个简单的“位置”估计问题。
对于绝对损失来说，这个解是残差在每个对应区域上的中位数。
对于其他的损失准则来说，求解表达式 10.30 有快速的递归算法，
但通常使用其更快速的“一步”近似算法就已经足够了。
问题的难点在于推导树的结构。
对于更一般的损失准则来说，并没有求解表达式 10.30
的简单快速的算法，
所以类似于表达式 10.27 的近似方法就非常重要了。

[^1]: 参数 $\Theta$ 可通过对损失函数在样本上的平均最小化解出。超参数 $J$ 可通过交叉验证的方法得出。