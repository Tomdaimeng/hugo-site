+++
title = "ESL-10.1 提升方法 DRAFT"
summary = """
"""

date = 2018-07-16T11:52:07+08:00
lastmod = 2018-08-11T15:05:10+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

**提升方法**[^1]（boosting method）是近二十年提出的最有影响的统计学习概念之一。
此方法原本为分类问题而设计，但在本章会阐述，其也可以运用在回归问题中。
提升方法的思路是将很多能力“弱”的分类模型的结果结合起来，形成一个强大的（投票）“委员会”（committee）。
从这个角度看，提升方法与**装袋**（bagging）以及其他“委员会”类型的方法（8.8节）有类似之处。
然而这种相似只是表面上的，提升方法从本质上与它们不同。

Boosting is one of the most powerful learning ideas introduced in the last
twenty years. It was originally designed for classification problems, but as
will be seen in this chapter, it can profitably be extended to regression
as well. The motivation for boosting was a procedure that combines the
outputs of many “weak” classifiers to produce a powerful “committee.”
From this perspective boosting bears a resemblance to bagging and other
committee-based approaches (Section 8.8). However we shall see that the
connection is at best superficial and that boosting is fundamentally different.

我们先介绍最流行的提升算法，由 Freund and Schapire (1997) 提出的“**自适应增强**[^2]（adaboost, adaptive boost） **M1**”。
考虑一个二分类问题，输出变量取值空间为 $Y \in \\{-1, 1\\}$。
给定一个自变量向量 $X$，一个分类器 $G(X)$ 会输出一个取值在 $\\{-1, 1\\}$ 中的预测。
其在训练样本中的错误率为

We begin by describing the most popular boosting algorithm due to
Freund and Schapire (1997) called “AdaBoost.M1.” Consider a two-class
problem, with the output variable coded as $Y \in \\{−1, 1\\}$. Given a vector of
predictor variables $X$, a classifier $G(X)$ produces a prediction taking one
of the two values $\\{−1, 1\\}$. The error rate on the training sample is

$$ \overline{\text{err}} = \frac{1}{N} \sum\_{i=1}^{N}I(y\_i \neq G(x\_i))  $$
对未来预测（总体样本）的预期错误率为 $E\_{XY} \[I(Y \neq G(X))\]$。

and the expected error rate on future predictions is $E\_{XY} \[I(Y \neq G(X))\]$.

某个弱分类器的错误率可能只比随机猜测的结果稍微好一点点。
提升方法则将这个弱分类算法应用于调整后的样本数据上，连续迭代这个过程，
从而得到一个弱分类器的序列 $G\_m(x), m = 1,2,\cdots, M$。
然后将它们输出的预测按某种权重进行投票汇总，根据投票的结果产生最终的预测：

A weak classifier is one whose error rate is only slightly better than
random guessing. The purpose of boosting is to sequentially apply the
weak classification algorithm to repeatedly modified versions of the data,
thereby producing a sequence of weak classifiers $G\_m(x), m = 1, 2, . . . , M$.
The predictions from all of them are then combined through a weighted
majority vote to produce the final prediction:

$$ G(x) = \text{sign} \left ( \sum\_{m=1}^M \alpha\_m G\_m(x) \right ) \tag{10.1}$$
这里的 $\alpha\_1, \alpha\_2, \cdots, \alpha\_M$ 从提升算法的过程中得出，作为每一个相对应的分类器 $G\_m(x)$ 在投票中的权重。
它们的作用是在最终投票中，给予序列中更准确的分类器更高的权重。
图10.1 展示了*自适应增强*的流程简图。

Here $\alpha\_1, \alpha\_2, \cdots, \alpha\_M$ are computed by the boosting algorithm, and weight
the contribution of each respective G m (x). Their effect is to give higher
influence to the more accurate classifiers in the sequence. Figure 10.1 shows
a schematic of the AdaBoost procedure.

{{< figure src="http://public.guansong.wang/eslii/ch10/eslii_fig_10_1.png" title="图10.1。自适应增强的流程简图。每一步迭代，在被权重调整过的数据集上训练分类器，然后将这一系列分类器结合在一起生成最终的预测。" >}}

在提升方法的每一步，会将权重 $w\_1, w\_2, \cdots, w\_N$ 加在训练样本上 $(x\_i, y\_i)$, $i = 1, 2, \cdots, N$。
初始化使用的权重为 $w\_i = 1/N$，所以第一步只是简单地将弱分类器使用在原始的训练样本上。
后续迭代的每一步 $m = 2, 3, \cdots, M$，
会对每一个样本按其权重进行调整，并将分类算法应用于权重调整后的样本上。
如在第 $m$ 步，被上一步的分类器 $G\_{m-1}(x)$ 误分类的样本会被赋予更高的权重，
而正确分类的样本会被赋予更低的权重。
因此随着不断地迭代，难以正确分类的样本会不断地增加对最终结果的影响。
这个过程使得每个分类器都会着重于训练样本中被之前的模型误分类的那部分。

The data modifications at each boosting step consist of applying weights
$w\_1, w\_2, \cdots, w\_N$
to each of the training observations $(x\_i , y\_i), i = 1, 2, . . . , N$.
Initially all of the weights are set to $w\_i = 1/N$, so that the first step simply
trains the classifier on the data in the usual manner. For each successive
iteration $m = 2, 3, . . . , M$ the observation weights are individually modi-
fied and the classification algorithm is reapplied to the weighted observa-
tions. At step $m$, those observations that were misclassified by the classifier
$G\_{m−1}(x)$ induced at the previous step have their weights increased, whereas
the weights are decreased for those that were classified correctly. Thus as
iterations proceed, observations that are difficult to classify correctly re-
ceive ever-increasing influence. Each successive classifier is thereby forced
to concentrate on those training observations that are missed by previous
ones in the sequence.

> ### 算法 10.1： 自适应增强 M1
> 1. 初始化样本权重为 $w\_i = 1/N, i = 1, 2, \cdots, N$.
> 2. 对每个迭代步骤 $m = 1$ 到 $M$：
>    1. 在用权重 $w\_i$ 调整过的训练样本上，训练分类器 $G\_m(x)$。
>    2. 计算加权错误率
>       $$\text{err}\_m = \frac{\sum\_{i=1}^N w_i I(y\_i \neq G\_m(x\_i))}{\sum\_{i=1}^N w\_i}$$
>    3. 计算投票权重 $\alpha\_m = \text{log}((1-\text{err}\_m)/\text{err}\_m)$
>    4. 对每个样本 $i$ 更新权重为 $w\_i \leftarrow w\_i \cdot exp[\alpha\_m \cdot I(y\_i \neq G\_m(x\_i ))], i = 1, 2, \cdots, N$.
> 3. 输出预测结果 $G(x) = \text{sign}\left [\sum\_{i=1}^N \alpha\_m G\_m(x) \right ]$

算法 10.1 详细说明了自适应增强 M1 算法。
在第 m 步迭代，行 2.1 得到这一步的分类器 $G\_m(x)$。
行 2.2 计算加权错误率。
行 2.3 计算在最终的分类结果 $G(x)$ 中（行 3），此步分类器 $G\_m(x)$ 所占的权重 $\alpha\_m$。
行 2.4 为下一步迭代更新每个样本的权重。
被 $G\_m(x)$ 误分类的样本的权重会乘以因子 $exp(\alpha\_m)$，在下一步迭代的训练中，会相对地增加对分类器 $G\_{m+1}(x)$ 的影响力。

Algorithm 10.1 shows the details of the AdaBoost.M1 algorithm. The
current classifier $G\_m(x)$ is induced on the weighted observations at line 2a.
The resulting weighted error rate is computed at line 2b. Line 2c calculates
the weight α m given to $G\_m(x)$ in producing the final classifier $G(x)$ (line
3). The individual weights of each of the observations are updated for the
next iteration at line 2d. Observations misclassified by $G\_m(x)$ have their
weights scaled by a factor $exp(\alpha\_m)$, increasing their relative influence for
inducing the next classifier $G\_{m+1}(x) in the sequence.

自适应增强 M1 算法在 Friedman et al. (2000) 文章中称为“离散自适应增强”，因为基于的分类模型（基模型）的输出为离散的标签变量。
如果基模型返回的是一个取值为连续实数的预测，例如一个映射到 $[-1,1]$ 区间上的概率分布，稍作调整后自适应增强也适用。
（参考 “连续自适应增强” in Friedman et al. (2000)）.

The AdaBoost.M1 algorithm is known as “Discrete AdaBoost” in Fried-
man et al. (2000), because the base classifier $G\_m(x)$ returns a discrete class
label. If the base classifier instead returns a real-valued prediction (e.g.,
a probability mapped to the interval $[−1, 1]$), AdaBoost can be modified
appropriately (see “Real AdaBoost” in Friedman et al. (2000)).

图 10.2 演示了使用适应性增强方法，极大地提高了一个很弱的分类器的表现。
特征变量 $X\_1, \cdots, X\_10$ 为独立的标准高斯分布（正态分布），真实的目标变量计算方法如下：

The power of AdaBoost to dramatically increase the performance of even
a very weak classifier is illustrated in Figure 10.2. The features $X\_1 , \cdots, X\_10$
are standard independent Gaussian, and the deterministic target Y is defined by

$$Y =  \begin{cases}
1& \text{当}\sum\_{j=1}^{10} X\_j^2 > \chi\_{10}^2(0.5)\\\\-1& \text{其它}
\end{cases}\tag{10.2}$$

上式中的 $\chi^2\_{10}(0.5) = 9.34$ 为自由度为 10 的卡方分布（10 个标准高斯分布的平方的和）的中位数。训练样本大小为 2000，两种目标变量的取值分别有大约 1000 个；测试样本有 10,000 个观测值。
所使用的弱分类器为一个“树桩”（单层决策树）：只有两个终结点的分类树。
简单地在使用这个分类器在训练样本上得到的模型，在测试样本上的错误率为 45.8%，对比随机猜测的错误率 50%，可以说这是一个很差的表现。
但是，随着增强迭代次数的增加，错误率稳步下降，在迭代了 400 次后达到了 5.8%。
因此，对这个非常简单的弱分类器使用增强方法，将预测的错误率降低到原来的八分之一。
而且它的表现也优于单个很深的分类决策树（错误率 24.7%）。
自这个方法出现后，有很多解释自适应增强在训练准确的分类模型上的成功的文献。
其中大部分都以分类决策树作为基模型 $G(x)$，表现的提升也常常十分显著。
Breiman (NIPS Workshop, 1996) 称基于决策树的适应增强为“世界上最好的即插即用的分类器”。
在数据挖掘的应用中尤其显著，这会在后面的第 10.7 节更详细地讨论。

Here $\chi^2\_{10}(0.5) = 9.34$ is the median of a chi-squared random variable with
10 degrees of freedom (sum of squares of 10 standard Gaussians). There are
2000 training cases, with approximately 1000 cases in each class, and 10,000
test observations. Here the weak classifier is just a “stump”: a two terminal-
node classification tree. Applying this classifier alone to the training data
set yields a very poor test set error rate of 45.8%, compared to 50% for
random guessing. However, as boosting iterations proceed the error rate
steadily decreases, reaching 5.8% after 400 iterations. Thus, boosting this
simple very weak classifier reduces its prediction error rate by almost a
factor of four. It also outperforms a single large classification tree (error
rate 24.7%). Since its introduction, much has been written to explain the
success of AdaBoost in producing accurate classifiers. Most of this work
has centered on using classification trees as the “base learner” G(x), where
improvements are often most dramatic. In fact, Breiman (NIPS Workshop,
1996) referred to AdaBoost with trees as the “best off-the-shelf classifier in
the world” (see also Breiman (1998)). This is especially the case for data-
mining applications, as discussed more fully in Section 10.7 later in this
chapter.

{{< figure src="http://public.guansong.wang/eslii/ch10/eslii_fig_10_2.png" title="图10.2。模拟数据（等式10.2）：基于决策树桩的增强，横轴为增强的迭代次数，纵轴为在测试样本上的错误率。同时，两个水平虚线分别为单层决策树和244个终节点的分类决策树的错误率。" >}}

### 10.1.1 本章概要
### 10.1.1 Outline of This Chapter

以下为本章的内容概要：

* 自适应增强可以看作是一个可加模型，最优化指数损失函数。这个损失函数与（负）二项分布的对数似然函数很相似（第 10.2-10.4 节）。
* 指数损失函数的总体最小值点，是每个分类的概率的对数几率（第 10.5 节）。
* 在分类和回归问题中，可以选择比平方误差和指数损失函数更稳健的损失函数（第 10.6 节）。
* 在数据挖掘的增强方法应用中，决策树是理想的基学习器（第 10.7 和 10.9 节）。
* 梯度增强模型（第 10.10 节）
* 强调“慢速学习”的重要性，从实践上采取对模型中每一个项使用 shrinkage 的方法（第 10.12 节），以及随机采样的方法（第 10.12.2 节）。
* 模型拟合的工具（第 10.13 节）。

Here is an outline of the developments in this chapter:

* We show that AdaBoost fits an additive model in a base learner, optimizing a novel exponential loss function. This loss function is very similar to the (negative) binomial log-likelihood (Sections 10.2–10.4).
* The population minimizer of the exponential loss function is shown to be the log-odds of the class probabilities (Section 10.5).
* We describe loss functions for regression and classification that are more robust than squared error or exponential loss (Section 10.6).
* It is argued that decision trees are an ideal base learner for data mining applications of boosting (Sections 10.7 and 10.9).
* We develop a class of gradient boosted models (GBMs), for boosting trees with any loss function (Section 10.10).
* The importance of “slow learning” is emphasized, and implemented by shrinkage of each new term that enters the model (Section 10.12), as well as randomization (Section 10.12.2).
* Tools for interpretation of the fitted model are described (Section 10.13).

[^1]: 中文维基页：https://zh.wikipedia.org/wiki/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95
[^2]: 中文维基页：https://zh.wikipedia.org/wiki/AdaBoost 


