+++
title = "ESL-10.1 提升方法"
summary = """
统计学习基础（译注）第十章第一节，第 337-341 页。
提升方法在迭代过程中不断地调整样本的权重，
提高拟合差的样本对基模型的影响力，
再将所得到的一系列基模型的加权投票作为最终结果。
本节以自适应增强为例演示了增强方法的效果。
"""

date = 2019-01-17T20:40:00+08:00
lastmod = 2019-01-17T20:40:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

**提升方法**[^1]（boosting method）
是近二十年提出的最有影响的统计学习概念之一。
此方法原本为分类问题而设计，但也可以应用在回归问题中，本章会加以说明。
提升方法的思路是将很多能力“弱”的分类器的结果结合在一起，
形成一个强大的“委员会”（committee）。
从这个角度看，提升方法与自助聚合（bagging）以及其他“委员会”类型的方法
（[第 8.8 节]({{< ref "/post/eslii/ch08/ch08_08.zh.md" >}})）
有相似之处。
然而两者之间的联系最多只是表面上的，提升方法在本质上是不同的。

我们先介绍最流行的提升算法，
由 Freund and Schapire (1997) 提出的
“**自适应增强 M1**”[^2]（adaboost, adaptive boost）。
考虑一个二分类问题，输出变量采用编码 $Y \in \\{-1, 1\\}$。
给定一个自变量向量 $X$，一个分类器 $G(X)$
会给出一个取值在 $\\{-1, 1\\}$ 中的预测。
在训练样本上的错误率为

$$\overline{\text{err}} =
\frac{1}{N} \sum\_{i=1}^{N} I(y\_i \neq G(x\_i))$$

对未来预测（总体样本）的期望错误率为 $E\_{XY} \[I(Y \neq G(X))\]$。

一个弱分类器是错误率可能只稍好于随机猜测的模型。
提升方法则将这个弱分类算法连续重复地应用于调整后的样本数据上，
从而产生一个弱分类器的序列 $G\_m(x), m = 1,2,\cdots, M$。
再将它们输出的预测按某种权重进行投票汇总，根据投票的结果产生最终的预测：

$$ G(x) = \text{sign} \left ( \sum\_{m=1}^M \alpha\_m G\_m(x) \right ) \tag{10.1}$$

其中的 $\alpha\_1, \alpha\_2, \cdots, \alpha\_M$ 从提升算法的过程中得出，
并被用作每个相应的分类器 $G\_m(x)$ 在投票中的权重。
它们的作用是在最终投票中，给予序列中更准确的分类器更高的权重。
图10.1 展示了自适应增强的示意图。

{{< figure
  src="http://public.guansong.wang/eslii/ch10/eslii_fig_10_01.png"
  title="**图10.1**：自适应增强的示意图。每一步迭代，在被权重调整过的数据集上训练分类器，然后将这一系列分类器结合在一起得出最终的预测。"
>}}

在提升的每一步，为训练样本 $(x\_i, y\_i)$, $i = 1,2,\cdots,N$
赋予权重 $w\_1, w\_2, \cdots, w\_N$。
初始化的权重为 $w\_i = 1/N$，
所以第一步简单地相当于在训练样本上按通常的方式应用弱分类器。
后续迭代的每一步 $m = 2, 3, \cdots, M$，
会调整每一个样本的权重，并将分类算法应用于权重调整后的样本上。
在第 $m$ 步，
上一步中分类器 $G\_{m-1}(x)$ 误分类的样本会被赋予更高的权重，
而正确分类的样本会被赋予更低的权重。
因此随着不断地迭代，难以分类的样本的影响力会不断地增加。
因此过程中的每个分类器都会着重于被之前模型误分类的那部分训练样本。

----------

### 算法 10.1：自适应增强 M1

1. 初始化样本权重为 $w\_i = 1/N, i = 1, 2, \cdots, N$.
2. 对每个迭代步骤 $m = 1$ 到 $M$：
   1. 在用权重 $w\_i$ 调整过的训练样本上，训练分类器 $G\_m(x)$。
   2. 计算加权错误率
      $$\text{err}\_m = \frac{\sum\_{i=1}^N w_i I(y\_i \neq G\_m(x\_i))}{\sum\_{i=1}^N w\_i}$$
   3. 计算投票权重 $\alpha\_m = \text{log}((1-\text{err}\_m)/\text{err}\_m)$
   4. 对每个样本 $i$ 更新权重为 $w\_i \leftarrow w\_i \cdot exp[\alpha\_m \cdot I(y\_i \neq G\_m(x\_i ))], i = 1, 2, \cdots, N$.
3. 输出预测结果 $G(x) = \text{sign}\left [\sum\_{i=1}^N \alpha\_m G\_m(x) \right ]$

----------

算法 10.1 详细说明了自适应增强 M1 算法。
步骤 2.1 在加权的样本上训练当前的分类器 $G\_m(x)$。
步骤 2.2 计算所得的加权错误率。
步骤 2.3 计算分类器 $G\_m(x)$ 在最终的分类结果 $G(x)$ 中（第 3 行）所占的权重 $\alpha\_m$。
步骤 2.4 更新下一步迭代的每个样本权重。
$G\_m(x)$ 误分类的样本的权重会放大到 $exp(\alpha\_m)$ 倍，
相对地增加了它们对迭代下一步中的分类器 $G\_{m+1}(x)$ 的影响力。

因为基分类器的输出为离散的类别标签，
Friedman et al. (2000) 将自适应增强 M1 算法称为“离散自适应增强”。
如果基分类器返回的是一个实数值的预测
（例如将一个概率预测值映射到 $[-1,1]$ 区间上），
自适应增强经过相应调整后也可适用。
（参考 Friedman et al. (2000) 中的“连续自适应增强”）。

{{< figure
  src="http://public.guansong.wang/eslii/ch10/eslii_fig_10_02.png"
  title="**图10.2**：模拟数据（等式10.2）：基于决策树桩模型增强方法的测试错误率，横轴为增强的迭代次数。同时也展示了单个树桩模型和 244 个节点的分类决策树的错误率。"
>}}

图 10.2 演示了通过适应性增强极大地提高了一个很弱的分类器的表现。
特征变量 $X\_1$，……，$X\_{10}$ 服从独立的标准高斯分布（正态分布），
真实的目标变量定义如下：

$$Y =  \begin{cases}
1 & \text{当}\sum\_{j=1}^{10} X\_j^2 > \chi\_{10}^2(0.5)
\\\\-1 & \text{其它}
\end{cases}\tag{10.2}$$

其中的 $\chi^2\_{10}(0.5) = 9.34$ 为自由度 10 的卡方分布
（10 个标准高斯分布的平方和）的中位数。
训练样本大小为 2000，其中每个类别大约有 1000 个；
测试样本大小为 10,000。
这里使用的弱分类器是一个“树桩”模型：只有两个终节点的分类树。
在训练样本上训练这个简单的分类器，其在测试集上错误率为 45.8%，
对比随机猜测 50% 的错误率，可以说这是一个比较差的表现。
然而，随着增强迭代次数的增加，错误率稳步地下降，
在 400 次迭代后降到了 5.8%。
可见，基于这个非常简单的弱分类器的增强方法，
将预测错误率降低到近乎原来的八分之一[^3]。
它的表现也优于一个大的分类决策树（错误率 24.7%）。
自这个方法出现后，有很多解释自适应增强得出准确分类器的能力的文献。
其中大部分都以分类决策树作为基模型 $G(x)$，带来的效果改善也常常十分显著。
Breiman (NIPS Workshop, 1996) 称基于决策树的适应增强
为“世界上最好的现成可用的分类器”（另见 Breiman (1998)）。
在数据挖掘的领域中尤其如此，
本章第 10.7 节会更详细地讨论。

### 10.1.1 本章内容概要

以下为本章的内容概要：

* 自适应增强可以看作是拟合了基学习器的一个加性模型，对一个独特的指数损失函数进行最优化。
  这个损失函数与（负）二项分布的对数似然函数很相似（第 10.2-10.4 节）。
* 指数损失函数的样本总体最小值点，是分类概率的对数几率（第 10.5 节）。
* 在分类和回归问题中，可以选择比平方误差和指数损失函数更稳健的损失函数（第 10.6 节）。
* 在数据挖掘的增强方法应用中，决策树被认为是理想的基学习器（第 10.7 和 10.9 节）。
* 梯度增强模型（GBM）可基于任意的损失函数对树模型增强（第 10.10 节）。
* 强调了“慢速学习”的重要性，
  通过对新进入模型的每一项进行收缩（shrinkage）（第 10.12 节）
  以及随机采样（第 10.12.2 节）来实现。
* 介绍了用于理解拟合模型的工具（第 10.13 节）。

[^1]: 参考了[中文维基](https://zh.wikipedia.org/wiki/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95)中的术语。
[^2]: 参考了[中文维基](https://zh.wikipedia.org/wiki/AdaBoost)中的术语。
[^3]: 原文为“Thus, boosting this ... classifier reduces its prediction error rate by almost a factor of four”。译者认为预测误差从 45.8% 降低到 5.8% 应该是“reduced by almost a factor of eight”。