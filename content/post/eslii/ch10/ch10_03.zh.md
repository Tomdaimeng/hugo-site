+++
title = "ESL-10.3 向前分步加性模型 Forward Stagewise Additive Modeling"
summary = """
"""

date = 2018-07-31T11:52:07+08:00
lastmod = 2018-08-11T15:05:10+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = []

[header]
image = ""
caption = ""
preview = true
+++

Forward stagewise modeling approximates the solution to (10.4) by sequen-
tially adding new basis functions to the expansion without adjusting the
parameters and coefficients of those that have already been added. This is
outlined in Algorithm 10.2. At each iteration m, one solves for the optimal
basis function $b(x; \gamma\_m)$ and corresponding coefficient β m to add to the cur-
rent expansion $f\_{m−1}(x)$. This produces $f\_m(x)$, and the process is repeated.
Previously added terms are not modified.

For squared-error loss
$$L(y, f(x)) = (y - f(x))^2 \tag{10.6}$$
one has

$$\begin{align}L(y\_i, f\_{m-1}(x\_i)+\beta b(x\_i; \gamma)) &=
 (y\_i - f\_{m-1}(x\_i) - \beta b(x\_i; \gamma))^2 \\\\\\
 &= (r\_{im} - \beta b(x\_i; \gamma))^2 \tag{10.7}
 \end{align}$$
  
where $r\_{im} = y\_i − f\_{m−1}(x\_i)$ is simply the residual of the current model
on the ith observation. Thus, for squared-error loss, the term $β\_m b(x; \gamma\_m)$
that best fits the current residuals is added to the expansion at each step.
This idea is the basis for “least squares” regression boosting discussed in
Section 10.10.2. However, as we show near the end of the next section,
squared-error loss is generally not a good choice for classification; hence
the need to consider other loss criteria.
