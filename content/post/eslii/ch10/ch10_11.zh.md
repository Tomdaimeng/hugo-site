+++
title = "ESL-10.11 Right-Sized Trees for Boosting"
summary = """
统计学习基础（译注）第十章第十一节，第 361-364 页。
"""

date = 2019-01-28T13:10:00+08:00
lastmod = 2019-01-28T13:10:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = []
categories = []

[header]
image = ""
caption = ""
preview = true
+++

Historically, boosting was considered to be a technique for combining mod-
els, here trees. As such, the tree building algorithm was regarded as a
primitive that produced models to be combined by the boosting proce-
dure. In this scenario, the optimal size of each tree is estimated separately
in the usual manner when it is built (Section 9.2). A very large (oversized)
tree is first induced, and then a bottom-up procedure is employed to prune
it to the estimated optimal number of terminal nodes. This approach as-
sumes implicitly that each tree is the last one in the expansion (10.28).
Except perhaps for the very last tree, this is clearly a very poor assump-
tion. The result is that trees tend to be much too large, especially during
the early iterations. This substantially degrades performance and increases
computation.

The simplest strategy for avoiding this problem is to restrict all trees
to be the same size, J m = J ∀m. At each iteration a J-terminal node
regression tree is induced. Thus J becomes a meta-parameter of the entire
boosting procedure, to be adjusted to maximize estimated performance for
the data at hand.

One can get an idea of useful values for J by considering the properties
of the “target” function

$$\eta = \underset{f}{\text{argmin }} E\_{XY} L(Y, f(X)) \tag{10.39}$$

Here the expected value is over the population joint distribution of (X, Y ).
The target function η(x) is the one with minimum prediction risk on future
data. This is the function we are trying to approximate.

One relevant property of η(X) is the degree to which the coordinate vari-
ables X T = (X 1 , X 2 , . . . , X p ) interact with one another. This is captured
by its ANOVA (analysis of variance) expansion

$$\begin{align}
\eta(X) =& \sum\_j \eta\_j(X\_j) + \sum\_{jk} \eta\_{jk}(X\_j, X\_k)
\\\\ & + \sum\_{jkl} \eta\_{jkl}(X\_j, X\_k, X\_l) + \dots
\end{align}\tag{10.40}$$

The first sum in (10.40) is over functions of only a single predictor variable
X j . The particular functions η j (X j ) are those that jointly best approximate
η(X) under the loss criterion being used. Each such η j (X j ) is called the
“main effect” of X j . The second sum is over those two-variable functions
that when added to the main effects best fit η(X). These are called the
second-order interactions of each respective variable pair (X j , X k ). The
third sum represents third-order interactions, and so on. For many problems
encountered in practice, low-order interaction effects tend to dominate.
When this is the case, models that produce strong higher-order interaction
effects, such as large decision trees, suffer in accuracy.

The interaction level of tree-based approximations is limited by the tree
size J. Namely, no interaction effects of level greater that J − 1 are pos-
sible. Since boosted models are additive in the trees (10.28), this limit
extends to them as well. Setting J = 2 (single split “decision stump”)
produces boosted models with only main effects; no interactions are per-
mitted. With J = 3, two-variable interaction effects are also allowed, and
so on. This suggests that the value chosen for J should reflect the level
of dominant interactions of η(x). This is of course generally unknown, but
in most situations it will tend to be low. Figure 10.9 illustrates the effect
of interaction order (choice of J) on the simulation example (10.2). The
generative function is additive (sum of quadratic monomials), so boosting
models with J > 2 incurs unnecessary variance and hence the higher test
error. Figure 10.10 compares the coordinate functions found by boosted
stumps with the true functions.

{{< figure
  src="http://public.guansong.wang/eslii/ch10/eslii_fig_10_09.png"
  title="**图10.9**："
>}}
Boosting with different sized trees, applied to the example (10.2)
used in Figure 10.2. Since the generative model is additive, stumps perform the
best. The boosting algorithm used the binomial deviance loss in Algorithm 10.3;
shown for comparison is the AdaBoost Algorithm 10.1.

{{< figure
  src="http://public.guansong.wang/eslii/ch10/eslii_fig_10_10.png"
  title="**图10.10**："
>}}
Coordinate functions estimated by boosting stumps for the sim-
ulated example used in Figure 10.9. The true quadratic functions are shown for
comparison.

Although in many applications J = 2 will be insufficient, it is unlikely
that J > 10 will be required. Experience so far indicates that 4 ≤ J ≤ 8
works well in the context of boosting, with results being fairly insensitive
to particular choices in this range. One can fine-tune the value for J by
trying several different values and choosing the one that produces the low-
est risk on a validation sample. However, this seldom provides significant
improvement over using J ≃ 6.