+++
title = "ESL-10.5 Why Exponential Loss?"
summary = """
"""

date = 2018-07-31T11:52:07+08:00
lastmod = 2018-08-11T15:05:10+08:00
draft = true 
math = true

authors = ["Butters"]
tags = []
categories = []

[header]
image = ""
caption = ""
preview = true
+++

The AdaBoost.M1 algorithm was originally motivated from a very differ-
ent perspective than presented in the previous section. Its equivalence to
forward stagewise additive modeling based on exponential loss was only
discovered five years after its inception. By studying the properties of the
exponential loss criterion, one can gain insight into the procedure and dis-
cover ways it might be improved.

The principal attraction of exponential loss in the context of additive
modeling is computational; it leads to the simple modular reweighting Ad-
aBoost algorithm. However, it is of interest to inquire about its statistical
properties. What does it estimate and how well is it being estimated? The
first question is answered by seeking its population minimizer.

It is easy to show (Friedman et al., 2000) that
$$f^\*(x) = \text{arg}\min\_{f(x)} E\_{Y|x}(e^{-Y f(x)}) =
\frac{1}{2} \log\frac{Pr(Y=1|x)}{Pr(Y=-1|x)} \tag{10.16}$$
or equivalently
$$Pr(Y=1|x) = \frac{1}{1+e^{-2f^\*(x)}}$$

Thus, the additive expansion produced by AdaBoost is estimating one-
half the log-odds of $Pr(Y = 1|x)$. This justifies using its sign as the
classification rule in (10.1).

Another loss criterion with the same population minimizer is the binomial
negative log-likelihood or deviance (also known as cross-entropy),
interpreting $f$ as the logit transform. Let
$$p(x) = Pr(Y=1|x) =
\frac{e^{f(x)}}{e^{-f(x)}+e^{f(x)}}=
\frac{1}{1+e^{-2f(x)}} \tag{10.17}$$
and define $Y′ = (Y + 1)/2 \in \\{0, 1\\}$. Then the binomial log-likelihood loss
function is
$$l(Y, p(x)) = Y′ \log p(x) + (1 − Y′) \log(1 − p(x))$$
or equivalently the deviance is
$$−l(Y, f(x)) = \log(1 + e^{−2Y f(x)}) \tag{10.18}$$
Since the population maximizer of log-likelihood is at the true probabilities
$p(x) = Pr(Y = 1 | x)$, we see from (10.17) that the population minimizers of
the deviance $E\_{Y|x} [−l(Y, f(x))]$ and $E\_{Y|x} [e^{−Y f(x)}]$ are the same. Thus, using
either criterion leads to the same solution at the population level. Note that
$e^{−Yf}$ itself is not a proper log-likelihood, since it is not the logarithm of
any probability mass function for a binary random variable $Y \in \\{−1, 1\\}$.
