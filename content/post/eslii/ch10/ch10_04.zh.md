+++
title = "ESL-10.4 指数损失函数与自适应增强 Exponential Loss and AdaBoost"
summary = """
"""

date = 2018-07-31T11:52:07+08:00
lastmod = 2018-08-11T15:05:10+08:00
draft = true 
math = true

authors = ["Butters"]
tags = []
categories = []

[header]
image = ""
caption = ""
preview = true
+++

We now show that AdaBoost.M1 (Algorithm 10.1) is equivalent to forward
stagewise additive modeling (Algorithm 10.2) using the loss function
$$L(y, f(x)) = \exp(−y f(x)) \tag{10.8}$$
The appropriateness of this criterion is addressed in the next section.

For AdaBoost the basis functions are the individual classifiers $G\_m(x) \in
\\{−1, 1\\}$. Using the exponential loss function, one must solve
$$(\beta\_m, G\_m) = \text{arg}\min\_{\beta,G} \sum\_{i=1}^N
\exp[-y\_i(f\_{m-1}(x\_i) + \beta G(x\_i))]$$
for the classifier $G\_m$ and corresponding coefficient $\beta_m$ to be added at each
step. This can be expressed as
$$(\beta\_m, G\_m) = \text{arg}\min\_{\beta,G} \sum\_{i=1}^N
w\_i^{(m)}\exp(-\beta y\_i G(x\_i)) \tag{10.9}$$
with $w\_i^{(m)} = \exp(−y\_i f\_{m−1}(x\_i))$. Since each $w\_i^{(m)}$
depends neither on $\beta$
nor $G(x)$, it can be regarded as a weight that is applied to each observation.
This weight depends on $f\_{m−1}(x\_i)$, and so the individual weight values
change with each iteration $m$.

The solution to (10.9) can be obtained in two steps. First, for any value
of $\beta > 0$, the solution to (10.9) for $G\_m(x)$ is
$$G\_m = \text{arg}\min_{G} \sum\_{i=1}^N
w\_i^{(m)} I(y\_i \neq G(x\_i)) \tag{10.10}$$
which is the classifier that minimizes the weighted error rate in predicting
$y$. This can be easily seen by expressing the criterion in (10.9) as
$$e^{-\beta}\sum\_{y\_i = G(x\_i)}w\_i^{(m)} +
e^{\beta}\sum\_{y\_i \neq G(x\_i)}w\_i^{(m)}$$
which in turn can be written as
$$(e^{-\beta}-e^{\beta})\sum\_{i=1}^N w\_i^{(m)}I(y\_i \neq G(x\_i)) +
e^{-\beta}\sum\_{i=1}^Nw\_i^{(m)} \tag{10.11}$$
Plugging this $G\_m$ into (10.9) and solving for $\beta$ one obtains
$$\beta\_m = \frac{1}{2} \log\frac{1-\text{err}\_m}{\text{err}\_m}\tag{10.12}$$
where $\text{err}\_m$ is the minimized weighted error rate
$$\text{err}\_m = \frac{\sum\_{i=1}^N w\_i^{(m)}I(y\_i \neq G(x\_i))}
{\sum\_{i=1}^N w\_i^{(m)}} \tag{10.13}$$
The approximation is then updated
$f\_m(x) = f\_{m−1}(x) + \beta\_m G\_m(x)$
which causes the weights for the next iteration to be
$$w\_i^{(m+1)}= w\_i^{(m)} e^{-\beta\_m y\_i G\_m(x\_i)} \tag{10.14}$$
Using the fact that $-y\_iG\_m(x\_i) = 2I(y\_i \neq G\_m(x\_i)) - 1$,
(10.14) becomes
$$w\_i^{(m+1)}= w\_i^{(m)} e^{-\alpha\_m I(y\_i \neq G\_m(x\_i))} e^{-\beta\_m}\tag{10.15}$$
where $\alpha\_m = 2\beta\_m$ is the quantity defined at line 2(c) of AdaBoost.M1
(Algorithm 10.1). The factor $e^{−\beta\_m}$ in (10.15) multiplies all weights by the
same value, so it has no effect. Thus (10.15) is equivalent to line 2(d) of
Algorithm 10.1.

One can view line 2(a) of the Adaboost.M1 algorithm as a method for
approximately solving the minimization in (10.11) and hence (10.10). Hence
we conclude that AdaBoost.M1 minimizes the exponential loss criterion
(10.8) via a forward-stagewise additive modeling approach.

Figure 10.3 shows the training-set misclassification error rate and aver-
age exponential loss for the simulated data problem (10.2) of Figure 10.2.
The training-set misclassification error decreases to zero at around 250 it-
erations (and remains there), but the exponential loss keeps decreasing.
Notice also in Figure 10.2 that the test-set misclassification error continues
to improve after iteration 250. Clearly Adaboost is not optimizing training-
set misclassification error; the exponential loss is more sensitive to changes
in the estimated class probabilities.
