+++
title = "ESL-10.6 Loss Functions and Robustness"
summary = """
统计学习基础（译注）第十章第五节，第 346-350 页。
"""

date = 2019-01-22T12:49:00+08:00
lastmod = 2019-01-22T12:49:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = []
categories = []

[header]
image = ""
caption = ""
preview = true
+++

In this section we examine the different loss functions for classification and
regression more closely, and characterize them in terms of their robustness
to extreme data.

#### Robust Loss Functions for Classification

Although both the exponential (10.8) and binomial deviance (10.18) yield
the same solution when applied to the population joint distribution, the
same is not true for finite data sets. Both criteria are monotone decreasing
functions of the “margin” yf (x). In classification (with a −1/1 response)
the margin plays a role analogous to the residuals y−f (x) in regression. The
classification rule G(x) = sign[f (x)] implies that observations with positive
margin y i f (x i ) > 0 are classified correctly whereas those with negative
margin y i f (x i ) < 0 are misclassified. The decision boundary is defined by
f (x) = 0. The goal of the classification algorithm is to produce positive
margins as frequently as possible. Any loss criterion used for classification
should penalize negative margins more heavily than positive ones since
positive margin observations are already correctly classified.

{{< figure
  src="http://public.guansong.wang/eslii/ch10/eslii_fig_10_04.png"
  title="**图10.4**："
>}}
Loss functions for two-class classification. The response is
y = ±1; the prediction is f , with class prediction sign(f ). The losses are
misclassification: I(sign(f ) 6 = y); exponential: exp(−yf ); binomial deviance:
log(1 + exp(−2yf )); squared error: (y − f ) 2 ; and support vector: (1 − yf ) + (see
Section 12.3). Each function has been scaled so that it passes through the point
(0, 1).

Figure 10.4 shows both the exponential (10.8) and binomial deviance
criteria as a function of the margin y · f (x). Also shown is misclassification
loss L(y, f (x)) = I(y · f (x) < 0), which gives unit penalty for negative mar-
gin values, and no penalty at all for positive ones. Both the exponential
and deviance loss can be viewed as monotone continuous approximations
to misclassification loss. They continuously penalize increasingly negative
margin values more heavily than they reward increasingly positive ones.
The difference between them is in degree. The penalty associated with bi-
nomial deviance increases linearly for large increasingly negative margin,
whereas the exponential criterion increases the influence of such observa-
tions exponentially.

At any point in the training process the exponential criterion concen-
trates much more influence on observations with large negative margins.
Binomial deviance concentrates relatively less influence on such observa-
tions, more evenly spreading the influence among all of the data. It is
therefore far more robust in noisy settings where the Bayes error rate is
not close to zero, and especially in situations where there is misspecification
of the class labels in the training data. The performance of AdaBoost has
been empirically observed to dramatically degrade in such situations.

Also shown in the figure is squared-error loss. The minimizer of the cor-
responding risk on the population is

$$\begin{align}
f^\*(x) &= \underset{f(x)}{\text{argmin}} E\_{Y|x} (Y-f(x))^2 =
E(Y|x) \\\\ &= 2 \cdot \text{Pr}(Y=1|x) - 1
\end{align}\tag{10.19}$$

As before the classification rule is G(x) = sign[f (x)]. Squared-error loss
is not a good surrogate for misclassification error. As seen in Figure 10.4, it
is not a monotone decreasing function of increasing margin yf (x). For mar-
gin values y i f (x i ) > 1 it increases quadratically, thereby placing increasing
influence (error) on observations that are correctly classified with increas-
ing certainty, thereby reducing the relative influence of those incorrectly
classified y i f (x i ) < 0. Thus, if class assignment is the goal, a monotone de-
creasing criterion serves as a better surrogate loss function. Figure 12.4 on
page 426 in Chapter 12 includes a modification of quadratic loss, the “Hu-
berized” square hinge loss (Rosset et al., 2004b), which enjoys the favorable
properties of the binomial deviance, quadratic loss and the SVM hinge loss.
It has the same population minimizer as the quadratic (10.19), is zero for
y · f (x) > 1, and becomes linear for y · f (x) < −1. Since quadratic functions
are easier to compute with than exponentials, our experience suggests this
to be a useful alternative to the binomial deviance.

With K-class classification, the response Y takes values in the unordered
set G = {G 1 , . . . , G k } (see Sections 2.4 and 4.4). We now seek a classifier
G(x) taking values in G. It is sufficient to know the class conditional proba-
bilities p k (x) = Pr(Y = G k |x), k = 1, 2, . . . , K, for then the Bayes classifier
is

$$G(x) = \mathcal{G}\_k \text{ 其中 } k =
\underset{l}{\text{argmax }} p\_l(x) \tag{10.20}$$

In principal, though, we need not learn the p k (x), but simply which one is
largest. However, in data mining applications the interest is often more in
the class probabilities p l (x), l = 1, . . . , K themselves, rather than in per-
forming a class assignment. As in Section 4.4, the logistic model generalizes
naturally to K classes,

$$p\_k(x) = \frac{e^{f\_k(x)}}{\sum\_{l=1}^K e^{f\_l(x)}} \tag{10.21}$$

which ensures that 0 ≤ p k (x) ≤ 1 and that they sum to one. Note that
here we have K different functions, one per class. There is a redundancy
in the functions f k (x), since adding an arbitrary h(x) to each leaves the
model unchanged. Traditionally one of them is set to zero: for example,
f K (x) = 0, as in (4.17). Here we prefer to retain the symmetry, and impose
the constraint k=1 f k (x) = 0. The binomial deviance extends naturally
to the K-class multinomial deviance loss function:

$$\begin{align}
L(y, p(x)) &= - \sum\_{k=1}^K I(y=\mathcal{G}\_k) \log p\_k(x) \\\\ &=
- \sum\_{k=1}^K I(y=\mathcal{G}\_k) f\_k(x) +
\log \left( \sum\_{l=1}^K e^{f\_l(x)} \right)
\tag{10.22}\end{align}$$

As in the two-class case, the criterion (10.22) penalizes incorrect predictions
only linearly in their degree of incorrectness.

Zhu et al. (2005) generalize the exponential loss for K-class problems.
See Exercise 10.5 for details.

#### Robust Loss Functions for Regression

In the regression setting, analogous to the relationship between exponential
loss and binomial log-likelihood is the relationship between squared-error
loss L(y, f (x)) = (y −f (x)) 2 and absolute loss L(y, f (x)) = | y −f (x) |. The
population solutions are f (x) = E(Y |x) for squared-error loss, and f (x) =
median(Y |x) for absolute loss; for symmetric error distributions these are
the same. However, on finite samples squared-error loss places much more
emphasis on observations with large absolute residuals | y i − f (x i ) | during
the fitting process. It is thus far less robust, and its performance severely
degrades for long-tailed error distributions and especially for grossly mis-
measured y-values (“outliers”). Other more robust criteria, such as abso-
lute loss, perform much better in these situations. In the statistical ro-
bustness literature, a variety of regression loss criteria have been proposed
that provide strong resistance (if not absolute immunity) to gross outliers
while being nearly as efficient as least squares for Gaussian errors. They
are often better than either for error distributions with moderately heavy
tails. One such criterion is the Huber loss criterion used for M-regression
(Huber, 1964)

$$L(y, f(x)) = \begin{cases}
[y-f(x)]^2 & \text{如果} |y-f(x)| \leq \delta
\\\\ 2\delta |y-f(x)| - \delta^2 & \text{其他}
\end{cases}\tag{10.23}$$

Figure 10.5 compares these three loss functions.

{{< figure
  src="http://public.guansong.wang/eslii/ch10/eslii_fig_10_05.png"
  title="**图10.5**："
>}}
A comparison of three loss functions for regression, plotted as a
function of the margin y−f . The Huber loss function combines the good properties
of squared-error loss near zero and absolute error loss when |y − f | is large.

These considerations suggest than when robustness is a concern, as is
especially the case in data mining applications (see Section 10.7), squared-
error loss for regression and exponential loss for classification are not the
best criteria from a statistical perspective. However, they both lead to the
elegant modular boosting algorithms in the context of forward stagewise
additive modeling. For squared-error loss one simply fits the base learner
to the residuals from the current model y i − f m−1 (x i ) at each step. For
exponential loss one performs a weighted fit of the base learner to the
output values y i , with weights w i = exp(−y i f m−1 (x i )). Using other more
robust criteria directly in their place does not give rise to such simple
feasible boosting algorithms. However, in Section 10.10.2 we show how one
can derive simple elegant boosting algorithms based on any differentiable
loss criterion, thereby producing highly robust boosting procedures for data
mining.