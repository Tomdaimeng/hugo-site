+++
title = "ESL-10.2 Boosting Fits an Additive Model"
summary = """
"""

date = 2018-07-16T11:52:07+08:00
lastmod = 2018-08-11T15:05:10+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = []

[header]
image = ""
caption = ""
preview = true
+++

增强方法十分有效，但原理并不神秘，其关键在于等式（10.1）。
增强可以看作是在拟合在一组基函数元素上的加性拓展（additive expansion）[^1]。
这里的基函数为个体分类器 $G\_m(x) \in \\{-1, 1\\}$。
从更广泛的角度看，基函数拓展的形式如下

The success of boosting is really not very mysterious.
The key lies in expression (10.1).
Boosting is a way of fitting an additive expansion in a set
of elementary “basis” functions. Here the basis functions are the individual
classifiers $G\_m(x) \in {−1, 1}$.
More generally, basis function expansions take the form

$$f(x) = \sum\_{m=1}^M \beta\_m b(x; \gamma\_m) \tag{10.3}$$

这里的 $β\_m , m = 1, 2, . . . , M$ 为拓展系数，
$b(x; γ) \in \mathcal{R}$ 通常为输入多维向量 $x$ 的简单的函数，含有一组参数 $\gamma$。
在本书第 5 章有更多关于基函数拓展的细节。

where $β\_m , m = 1, 2, . . . , M$ are the expansion coefficients, and $b(x; γ) \in \mathcal{R}$
are usually simple functions of the multivariate argument x, characterized
by a set of parameters $\gamma$. We discuss basis expansions in some detail in
Chapter 5.

类似这样的加性拓展是本书中很多统计学习技术的核心：

- 单隐层神经网络

- 信号处理

Additive expansions like this are at the heart of many of the learning
techniques covered in this book:

- In single-hidden-layer neural networks (Chapter 11),
  $b(x; \gamma) = \sigma(\gamma\_0 + \gamma\_1^T x)$,
  where $\sigma(t) = 1/(1 + e^{−t})$ is the sigmoid function,
  and $\gamma$ parameterizes a linear combination of the input variables.
  
- In signal processing, wavelets (Section 5.9.1) are a popular choice with
  $\gamma$ parameterizing the location and scale shifts of a “mother” wavelet.

- Multivariate adaptive regression splines (Section 9.4) uses truncated-
power spline basis functions where $\gamma$ parameterizes the variables and
values for the knots.

- For trees, $\gamma$ parameterizes the split variables and split points at the
internal nodes, and the predictions at the terminal nodes.

通常这些模型通过对训练样本上的平均损失函数最小化进行拟合，例如平方误差或似然损失函数。

Typically these models are fit by minimizing a loss function averaged
over the training data, such as the squared-error or a likelihood-based loss
function,

$$\min\_{\\{\beta\_m, \gamma\_m\\}\_1^M}
\sum\_{i=1}^N L \left ( y\_i, \sum\_{m=1}^M\beta\_m b(x\_i; \gamma_m) \right ) \tag{10.4}$$

对于很多损失函数 $L(y, f (x))$ 和基函数 $b(x; \gamma)$，这种拟合的数值最优化过程需要消耗大量的计算能力。
不过当单个基函数的最优化问题存在快速的解法时，可以采用一个简单的替代算法。

For many loss functions $L(y, f (x))$ and/or basis functions $b(x; γ)$, this re-
quires computationally intensive numerical optimization techniques. However,
a simple alternative often can be found when it is feasible to rapidly
solve the subproblem of fitting just a single basis function,

$$\min\_{\beta, \gamma} \sum\_{i=1}^N L(y\_i, \beta b(x\_i; \gamma)) \tag{10.5}$$

[^1]: 即这些基函数的线性组合。
