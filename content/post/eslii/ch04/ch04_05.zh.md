+++
title = "ESL-4.5 Separating Hyperplanes"
summary = """
统计学习基础（译注）第四章第五节，第 129-135 页。
"""

date = 2018-10-09T19:25:00+08:00
lastmod = 2018-10-09T19:25:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

We have seen that linear discriminant analysis and logistic regression both
estimate linear decision boundaries in similar but slightly different ways.
For the rest of this chapter we describe separating hyperplane classifiers.
These procedures construct linear decision boundaries that explicitly try
to separate the data into different classes as well as possible. They provide
the basis for support vector classifiers, discussed in Chapter 12. The math-
ematical level of this section is somewhat higher than that of the previous
sections.

Figure 4.14 shows 20 data points in two classes in IR 2 . These data can be
separated by a linear boundary. Included in the figure (blue lines) are two
of the infinitely many possible separating hyperplanes. The orange line is
the least squares solution to the problem, obtained by regressing the −1/1
response Y on X (with intercept); the line is given by

$$\\{x:\hat{\beta}\_0 + \hat{\beta}\_1 x\_1 + \hat{\beta}\_2 x\_2 = 0
\\}\tag{4.39}$$

This least squares solution does not do a perfect job in separating the
points, and makes one error. This is the same boundary found by LDA,
in light of its equivalence with linear regression in the two-class case (Sec-
tion 4.3 and Exercise 4.2).

Classifiers such as (4.39), that compute a linear combination of the input
features and return the sign, were called perceptrons in the engineering liter-
ature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations
for the neural network models of the 1980s and 1990s.

Before we continue, let us digress slightly and review some vector algebra.
Figure 4.15 depicts a hyperplane or affine set L defined by the equation
f (x) = β 0 + β T x = 0; since we are in IR 2 this is a line.

Here we list some properties:

1. For any two points x 1 and x 2 lying in L, β T (x 1 − x 2 ) = 0, and hence
  β ∗ = β/||β|| is the vector normal to the surface of L.
2. For any point x 0 in L, β T x 0 = −β 0 .
3. The signed distance of any point x to L is given by
  $$\begin{align}
  {\beta^*}^T (x-x\_0) &=
    \frac{1}{\\|\beta\\|} (\beta^T x + \beta\_0) \\\\ &=
    \frac{1}{\\|f'(x)\\|} f(x)
  \tag{4.40}\end{align}$$

Hence f (x) is proportional to the signed distance from x to the hyperplane
defined by f (x) = 0.

### 4.5.1 Rosenblatt 感知器学习算法
### 4.5.1 Rosenblatt’s Perceptron Learning Algorithm

The perceptron learning algorithm tries to find a separating hyperplane by
minimizing the distance of misclassified points to the decision boundary. If
a response y i = 1 is misclassified, then x Ti β + β 0 < 0, and the opposite for
a misclassified response with y i = −1. The goal is to minimize

$$D(\beta, \beta\_0) = -\sum\_{i\in\mathcal{M}} y\_i(x\_i^T\beta + \beta\_0)
\tag{4.41}$$

where M indexes the set of misclassified points. The quantity is non-
negative and proportional to the distance of the misclassified points to
the decision boundary defined by β T x + β 0 = 0. The gradient (assuming
M is fixed) is given by

$$\begin{align}
\frac{\partial D(\beta, \beta\_0)}{\partial \beta} &=
-\sum\_{i\in\mathcal{M}} y\_ix\_i 
\tag{4.42} \\\\ \frac{\partial D(\beta, \beta\_0)}{\partial \beta\_0} &=
-\sum\_{i\in\mathcal{M}} y\_i \tag{4.43}
\end{align}$$

The algorithm in fact uses stochastic gradient descent to minimize this
piecewise linear criterion. This means that rather than computing the sum
of the gradient contributions of each observation followed by a step in the
negative gradient direction, a step is taken after each observation is visited.
Hence the misclassified observations are visited in some sequence, and the
parameters β are updated via

$$\begin{pmatrix} \beta \\\\ \beta\_0 \end{pmatrix}
\leftarrow
\begin{pmatrix} \beta \\\\ \beta\_0 \end{pmatrix} +
\rho \begin{pmatrix} y\_i x\_i \\\\ y\_i \end{pmatrix}
\tag{4.44}$$

Here ρ is the learning rate, which in this case can be taken to be 1 without
loss in generality. If the classes are linearly separable, it can be shown that
the algorithm converges to a separating hyperplane in a finite number of
steps (Exercise 4.6). Figure 4.14 shows two solutions to a toy problem, each
started at a different random guess.

There are a number of problems with this algorithm, summarized in
Ripley (1996):

* When the data are separable, there are many solutions, and which
one is found depends on the starting values.
* The “finite” number of steps can be very large. The smaller the gap,
the longer the time to find it.
* When the data are not separable, the algorithm will not converge,
and cycles develop. The cycles can be long and therefore hard to
detect.

The second problem can often be eliminated by seeking a hyperplane not
in the original space, but in a much enlarged space obtained by creating
many basis-function transformations of the original variables. This is anal-
ogous to driving the residuals in a polynomial regression problem down
to zero by making the degree sufficiently large. Perfect separation cannot
always be achieved: for example, if observations from two different classes
share the same input. It may not be desirable either, since the resulting
model is likely to be overfit and will not generalize well. We return to this
point at the end of the next section.

A rather elegant solution to the first problem is to add additional con-
straints to the separating hyperplane.

### 4.5.2 Optimal Separating Hyperplanes

The optimal separating hyperplane separates the two classes and maximizes
the distance to the closest point from either class (Vapnik, 1996). Not only
does this provide a unique solution to the separating hyperplane problem,
but by maximizing the margin between the two classes on the training data,
this leads to better classification performance on test data.
We need to generalize criterion (4.41). Consider the optimization problem

$$\begin{gather}
\max\_{\beta, \beta\_0, \\|\beta\\|=1} M \\\\ \text{subjext to }
y\_i(x\_i^T\beta + \beta\_0) \geq M, i = 1,\dots, N
\end{gather}\tag{4.45}$$

The set of conditions ensure that all the points are at least a signed
distance M from the decision boundary defined by β and β 0 , and we seek
the largest such M and associated parameters. We can get rid of the ||β|| =
1 constraint by replacing the conditions with

$$ \frac{1}{\\|\beta\\|} y\_i(x\_i^T\beta + \beta\_0) \geq M
\tag{4.46}$$

(which redefines β 0 ) or equivalently

$$ y\_i(x\_i^T \beta + \beta\_0) \geq M \\|\beta\\|
\tag{4.47}$$

Since for any β and β 0 satisfying these inequalities, any positively scaled
multiple satisfies them too, we can arbitrarily set ||β|| = 1/M . Thus (4.45)
is equivalent to

$$\begin{gather}
\max\_{\beta, \beta\_0} \frac{1}{2}\\|\beta\\|^2 \\\\ \text{subjext to }
y\_i(x\_i^T\beta + \beta\_0) \geq 1, i = 1,\dots, N
\end{gather}\tag{4.48}$$

In light of (4.40), the constraints define an empty slab or margin around the
linear decision boundary of thickness 1/||β||. Hence we choose β and β 0 to
maximize its thickness. This is a convex optimization problem (quadratic
criterion with linear inequality constraints). The Lagrange (primal) func-
tion, to be minimized w.r.t. β and β 0 , is

$$ L\_P = \frac{1}{2}\\|\beta\\|^2 -
  \sum\_{i=1}^N \alpha\_i[y\_i(x\_i^T\beta + \beta\_0) - 1]
\tag{4.49}$$

Setting the derivatives to zero, we obtain:

$$\begin{align}
\beta &= \sum\_{i=1}^N \alpha\_i y\_i x\_i
\tag{4.50} \\\\ 0 &=
\sum\_{i=1}^N \alpha\_i y\_i \tag{4.51}
\end{align}$$

and substituting these in (4.49) we obtain the so-called Wolfe dual

$$\begin{align}
L\_D =& \sum\_{i=1}^N \alpha\_i -
  \sum\_{i=1}^N\sum\_{k=1}^N \alpha\_i \alpha\_k y\_i y\_k x\_i^T x\_k \\\\ &
\text{subject to } \alpha\_i \geq 0 \text{ and } \sum\_{i=1}^N \alpha\_i y\_i = 0
\tag{4.52}\end{align}$$

The solution is obtained by maximizing L D in the positive orthant, a sim-
pler convex optimization problem, for which standard software can be used.
In addition the solution must satisfy the Karush–Kuhn–Tucker conditions,
which include (4.50), (4.51), (4.52) and

$$ \alpha\_i[y\_i(x\_i^T\beta + \beta\_0) - 1] = 0  \forall i
\tag{4.53}$$

From these we can see that

* if α i > 0, then y i (x Ti β + β 0 ) = 1, or in other words, x i is on the
boundary of the slab;
* if y i (x Ti β + β 0 ) > 1, x i is not on the boundary of the slab, and α i = 0.

From (4.50) we see that the solution vector β is defined in terms of a linear
combination of the support points x i —those points defined to be on the
boundary of the slab via α i > 0. Figure 4.16 shows the optimal separating
hyperplane for our toy example; there are three support points. Likewise,
β 0 is obtained by solving (4.53) for any of the support points.

The optimal separating hyperplane produces a function f ˆ (x) = x T β̂ + β̂ 0
for classifying new observations:

$$ \hat{G}(x) = \text{sign}(\hat{f}(x))
\tag{4.54}$$

Although none of the training observations fall in the margin (by con-
struction), this will not necessarily be the case for test observations. The
intuition is that a large margin on the training data will lead to good
separation on the test data.

The description of the solution in terms of support points seems to sug-
gest that the optimal hyperplane focuses more on the points that count,
and is more robust to model misspecification. The LDA solution, on the
other hand, depends on all of the data, even points far away from the de-
cision boundary. Note, however, that the identification of these support
points required the use of all the data. Of course, if the classes are really
Gaussian, then LDA is optimal, and separating hyperplanes will pay a price
for focusing on the (noisier) data at the boundaries of the classes.

Included in Figure 4.16 is the logistic regression solution to this prob-
lem, fit by maximum likelihood. Both solutions are similar in this case.
When a separating hyperplane exists, logistic regression will always find
it, since the log-likelihood can be driven to 0 in this case (Exercise 4.5).
The logistic regression solution shares some other qualitative features with
the separating hyperplane solution. The coefficient vector is defined by a
weighted least squares fit of a zero-mean linearized response on the input
features, and the weights are larger for points near the decision boundary
than for those further away.

When the data are not separable, there will be no feasible solution to
this problem, and an alternative formulation is needed. Again one can en-
large the space using basis transformations, but this can lead to artificial
separation through over-fitting. In Chapter 12 we discuss a more attractive
alternative known as the support vector machine, which allows for overlap,
but minimizes a measure of the extent of this overlap.