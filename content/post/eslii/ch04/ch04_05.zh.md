+++
title = "ESL-4.5 分离超平面"
summary = """
统计学习基础（译注）第四章第五节，第 129-135 页。
当训练样本在输入空间上可以被线性的超平面完全分开，则存在分离超平面；其中让不同类型之间的边界区域尽可能大的为最优分离超平面。
"""

date = 2018-10-12T18:10:00+08:00
lastmod = 2018-10-12T18:10:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

本章之前所介绍的线性判别分析和对数几率回归，
都针对线性判定边界进行估计，估计的方法略有不同。
本节介绍的是分类超平面分类器。
这类算法直接寻找可最大程度区分不同类型的的线性判定边界。
第十二章中的支持向量机分类器即以此方法为基础。
相较于本章其他部分，本节所涉及的数学知识稍微高一些。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_14.png"
  title="**图4.14**：可被超平面分离的两个类型的虚拟例子。橙色直线为最小二乘解的边界，有一个点被误分类。两条蓝色的线为不同随机初始点的感知器学习分类器生成的分离超平面。"
>}}

图 4.14 中为在 $\mathbb{R}^2$ 空间上的 20 个二分类数据。
这些样本点可完全被某个线性边界分开。
这种 *分离超平面（separating hyperplane）* 有无穷多个，图中的蓝色直线为两个例子。
橙色直线为这个数据集的最小二乘解，
即用 -1 和 1 的输出变量 $Y$ 对 $X$ 的含截距的回归，
其表达式为：

$$\\{x:\hat{\beta}\_0 + \hat{\beta}\_1 x\_1 + \hat{\beta}\_2 x\_2 = 0
\\}\tag{4.39}$$

最小二乘解没有完美地区分样本点，其中有一个误分类。
在二分类问题中，线性判别分析与线性回归是等价的
（[第 4.3 节]({{< ref "/post/eslii/ch04/ch04_03.zh.md" >}}) 以及练习 4.2），
所以这也是线性判别分析的判定边界。

等式 4.39 中的分类器根据输入特征变量的线性组合的符号进行分类，
在 1950 年代末的工程领域（Rosenblatt, 1958）称之为 *感知器（perceptrons）* 。
而感知器是 1980 和 1990 年代的神经网络模型的基础。

在继续深入之前，下面为可能会涉及到的一些向量代数基础。
图 4.15 中为一个超平面，或等式 $f(x) = \beta\_0 + \beta^T x = 0$ 定义的仿射集 $L$；
在 $\mathbb{R}^2$ 空间上其形式为一条直线。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_15.png"
  title="**图4.15**：超平面或仿射集的线性代数。"
>}}

则有以下结论：

1. 集合 $L$ 上的任意两点 $x\_1$ 和 $x\_2$，
   $\beta^T(x\_1 - x\_2) = 0$，
   因此平面 $L$ 的向量范数为 $\beta^* = \beta / \\|\beta\\|$。
2. 对于 $L$ 上的任一点 $x\_0$，
   $ \beta^T x\_0 = - \beta\_0$。
3. 任意点与 $L$ 的有方向距离为
  $$\begin{align}
  {\beta^*}^T (x-x\_0) &=
    \frac{1}{\\|\beta\\|} (\beta^T x + \beta\_0) \\\\ &=
    \frac{1}{\\|f'(x)\\|} f(x)
  \tag{4.40}\end{align}$$

因此某个点 $z$ [^1]与等式 $f(x)=0$ 所确立的超平面的有方向距离，与 $f(z)$ 的值成正比。

### 4.5.1 Rosenblatt 感知器学习算法

*感知器学习算法（perceptron learning algorithm）*
通过最小化误分类的点与判定边界之间的距离来寻找分离超平面。
若一个点的输出分类 $y\_i = 1$ 被误分类为 $-1$，
则 $x\_i^T \beta + \beta\_0 < 0 $；
对 $y\_i = -1$ 的点则相反。
最小化的目标函数为：

$$D(\beta, \beta\_0) = -\sum\_{i\in\mathcal{M}} y\_i(x\_i^T\beta + \beta\_0)
\tag{4.41}$$

其中的 $\mathcal{M}$ 代表被误分类点的集合。
这个非负的值与误分类点与判定边界 $\beta^T x + \beta\_0=0$ 的距离成正比。
梯度为（视 $\mathcal{M}$ 为给定集合）：

$$\begin{align}
\frac{\partial D(\beta, \beta\_0)}{\partial \beta} &=
-\sum\_{i\in\mathcal{M}} y\_ix\_i 
\tag{4.42} \\\\ \frac{\partial D(\beta, \beta\_0)}{\partial \beta\_0} &=
-\sum\_{i\in\mathcal{M}} y\_i \tag{4.43}
\end{align}$$

事实上这个算法通过
*随机梯度下降（stochastic gradient descent）*
来对分段线性的准则函数进行最小化。
传统的梯度下降方法在每一步先对整体样本对梯度向量的贡献求和，再向梯度向量的反方向更新参数；
而这个算法对样本中每个点依次计算并相应地更新参数。
因而误分类的点依次进入计算，参数 $\beta$ 的更新过程为：

$$\begin{pmatrix} \beta \\\\ \beta\_0 \end{pmatrix}
\leftarrow
\begin{pmatrix} \beta \\\\ \beta\_0 \end{pmatrix} +
\rho \begin{pmatrix} y\_i x\_i \\\\ y\_i \end{pmatrix}
\tag{4.44}$$

其中的 $\rho$ 为学习率，
在本例中不失一般性地设为 1。
若类型在空间中为线性可分离，
可证明这个算法在有限步骤之后收敛到一个分离超平面（练习 4.6）。
图 4.14 中展示了虚拟例子中的两个分离超平面解，
两者的初始值为不同的随机猜测。

Ripley (1996) 概括了这个算法的一些问题：

* 若数据样本的类型可分离，则解不唯一，算法的初始值决定了收敛到哪个解上。
* 虽然收敛所需的步骤是有限的，但有可能是个很大的数字。
  类型在空间上的间隙越小，所需的步骤越多。
* 当数据的类型不可分离，则算法可能不收敛，而且会形成循环的步骤。
  这个循环有可能很长，使其难以被探测到。

第二个问题通常可以通过空间的转换来解决，
即将原始的输入变量进行基函数转换，
再在扩大了很多的新输入变量空间上计算分离超平面。
这与在多项式回归模型问题中将输入变量的度量扩大后残差会接近于 0 的原理类似。
算法并不总会完全分离数据点，
一个简单的例子是当两个不同类型都包含某个输入变量完全相同的点。
另外，完全分离数据点的模型很可能存在过拟合而泛化表现差，因而并不是所追求的目标。
本节末位会重提这个观点。

第一个问题可以通过加入对分离超平面额外的约束来解决。

### 4.5.2 最优分离超平面 :scream:

*最优分离超平面（optimal separating hyperplane）*
除分离两个类型外还最大化分离超平面与所有点的距离（Vapnik, 1996）。
这不仅为分离超平面提供了唯一解，
而且将训练数据中两个分类之间的距离最大化可改善其在测试数据中的正确率。
对等式 4.41 进行推广，考虑最优化问题：

$$\begin{gather}
\max\_{\beta, \beta\_0, \\|\beta\\|=1} M \\\\ \text{subjext to }
y\_i(x\_i^T\beta + \beta\_0) \geq M, i = 1,\dots, N
\end{gather}\tag{4.45}$$

这组约束条件保证了所有的点与 $\beta$ 和 $\beta\_0$ 确定的判定边界之间的距离
都大于带符号的距离 $M$，
最优化解为可以使这个距离 $M$ 最大的相应参数。
通过重写约束条件可去掉 $\\|\beta\\|=1$ 的约束条件：

$$ \frac{1}{\\|\beta\\|} y\_i(x\_i^T\beta + \beta\_0) \geq M
\tag{4.46}$$

注意上式改变了 $\beta\_0$ 的含义。
或等价地：

$$ y\_i(x\_i^T \beta + \beta\_0) \geq M \\|\beta\\|
\tag{4.47}$$

任意满足这些不等式的 $\beta$ 和 $\beta\_0$，乘以一个正的常数后依然满足这些不等式，
因此可随意地固定 $\\|\beta\\| = 1 / M$。
则等式 4.45 可写为：

$$\begin{gather}
\max\_{\beta, \beta\_0} \frac{1}{2}\\|\beta\\|^2 \\\\ \text{subjext to }
y\_i(x\_i^T\beta + \beta\_0) \geq 1, i = 1,\dots, N
\end{gather}\tag{4.48}$$

通过等式 4.40，这些约束条件可以想象为在线性判定边界周围定义了
厚度为 $1 / \\|\beta\\|$ 的空白边际区间。
$\beta$ 和 $\beta\_0$ 的解最大化这个区间的厚度。
这是一个凸优化问题（二次项的准则和线性不等式的约束条件）。
对 $\beta$ 和 $\beta\_0$ 取最小化的拉格朗日函数（原函数，primal function）为：

$$ L\_P = \frac{1}{2}\\|\beta\\|^2 -
  \sum\_{i=1}^N \alpha\_i[y\_i(x\_i^T\beta + \beta\_0) - 1]
\tag{4.49}$$

将一阶导数设为 0：

$$\begin{align}
\beta &= \sum\_{i=1}^N \alpha\_i y\_i x\_i
\tag{4.50} \\\\ 0 &=
\sum\_{i=1}^N \alpha\_i y\_i \tag{4.51}
\end{align}$$

将结果带入到等式 4.49，即为 Wolfe 对偶问题：

$$\begin{align}
L\_D =& \sum\_{i=1}^N \alpha\_i -
  \sum\_{i=1}^N\sum\_{k=1}^N \alpha\_i \alpha\_k y\_i y\_k x\_i^T x\_k \\\\ &
\text{subject to } \alpha\_i \geq 0 \text{ and } \sum\_{i=1}^N \alpha\_i y\_i = 0
\tag{4.52}\end{align}$$

在正象限（positive orthant）上最大化 $L\_D$ 可的解，
这是一个简单的凸优化问题，大多软件都实现了求解方法。
另外，这个解须满足
卡罗需-库恩-塔克（Karush–Kuhn–Tucker）条件，
包括等式 4.50、4.51、4.52、以及

$$ \alpha\_i[y\_i(x\_i^T\beta + \beta\_0) - 1] = 0  \forall i
\tag{4.53}$$

从中可见：

* 若 $\alpha\_i > 0$，
  则 $y\_i(x\_i^T \beta + \beta\_0) = 1$，
  或者说 $x\_i$ 位于空白边际空间的边界；
* 若 $y\_i(x\_i^T \beta + \beta\_0) > 1$，
  则 $x\_i$ 不在空白边际空间的边界，且 $\alpha\_i = 0$。

从等式 4.50 可见向量 $\beta$ 解的形式为 *支持点（support point）* 的线性组合，
即有 $\alpha\_i>0$ 的在空白边际空间的边界上的点 $x\_i$。
图 4.16 以虚拟例子展示了最优分离超平面，其中有三个支持点。
$\beta\_0$ 也类似地在支持点对等式 4.53 求解得出。


{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_16.png"
  title="**图4.16**：与图 4.14 同样的数据。黄色阴影区域描述了分离两个类型的最大边际区域。共有三个支持点，落在边际区域的边界上，最优分离超平面（蓝色直线）平分这个区域。图中也包括了对数几率回归生成的判定边界（红色直线），其与最优分离超平面非常相近（见第 12.3.3 节）。"
>}}

最优分离超平面生成一个可用来对新样本分类的判定函数
$\hat{f}(x) = x^T \hat{\beta} + \hat{\beta}\_0$：

$$ \hat{G}(x) = \text{sign}(\hat{f}(x))
\tag{4.54}$$

算法从设计上保证了训练样本中没有落入边际区域的点，
但测试样本有可能落入这个区间。
若在训练数据中生成分类器有比较大的边际区域，
从直观上可认为其在测试数据中会有比较强的分类能力。

最优分离超平面的解，从设计上更依赖于边界上的支持点，
因而看起来对模型误设（misspecification）更稳健。
相比之下，线性判别分析的解建立在所有的数据之上，包含远离判定边界的点。
但需要指出的是支持点也是从所有的数据中生成的。
当然，若真实的数据为高斯分布，则线性判别分析为最优的方法，
分离超平面过于依赖于类型边界附近的数据，影响其分类效果。

图 4.16 中也加入了通过最大似然拟合的对数几率回归的解。
在这个例子中，两个解非常相似。
当存在分离超平面时，对数似然度可以取值到 0，
所以对数几率回归的解也必然是一个分离超平面（练习 4.5）。
对数几率回归的解与分离超平面的解还有另外的一些相似特性。
其通过零和并线性化的输入变量对输入特征变量的加权最小二乘拟合确定系数向量，
在判定边界附近点的权重要高于远离边界的点。

当数据中的类型无法分离，这个方法无法求解，需要对其进行修改。
用基函数转化来扩大输入变量空间同样可能解决这个问题，
但这可能会引起过拟合从而造成人为的分离。
第十二章中的支持向量机可以提供一个更好的解决方法，
允许但最小化类型的重叠区域。

[^1]: 原文里点的符号为 $x$，其与后面的 $f(x)$ 中的 $x$ 含义不同。为了避免混淆，用 $z$ 代表某个点。