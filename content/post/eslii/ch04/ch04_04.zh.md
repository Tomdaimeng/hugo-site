+++
title = "ESL-4.4 对数几率回归"
summary = """
统计学习基础（译注）第四章第三节，第 119-128 页。
"""

date = 2018-10-08T14:32:07+08:00
lastmod = 2018-10-08T14:32:07+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The logistic regression model arises from the desire to model the posterior
probabilities of the K classes via linear functions in x, while at the same
time ensuring that they sum to one and remain in [0, 1]. The model has
the form

$$\begin{align}
\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=K|X=x)} &=
\beta\_{10} + \beta\_1^T x \\\\ \log\frac{\text{Pr}(G=2|X=x)}{\text{Pr}(G=K|X=x)}  &= 
\beta\_{20} + \beta\_2^T x \\\\ &\vdots \\\\ \log\frac{\text{Pr}(G=K-1|X=x)}{\text{Pr}(G=K|X=x)}  &= 
\beta\_{(K-1)0} + \beta\_{K-1}^T x 
\end{align}\tag{4.17}$$

The model is specified in terms of K − 1 log-odds or logit transformations
(reflecting the constraint that the probabilities sum to one). Although the
model uses the last class as the denominator in the odds-ratios, the choice
of denominator is arbitrary in that the estimates are equivariant under this
choice. A simple calculation shows that

$$\begin{align}
\text{Pr}(G=k|X=x) &=
\frac{\exp(\beta\_{k0}+\beta\_k^T x)}{1+\sum\_{l=1}^{K-1}\exp(\beta\_{l0} + \beta\_l^T x)},
k = 1,\dots,K-1, \\\\ \text{Pr}(G=K|X=x) &=
\frac{1}{1+\sum\_{l=1}^{K-1}\exp(\beta\_{l0} + \beta\_l^T x)} \tag{4.18}
\end{align}$$

and they clearly sum to one. To emphasize the dependence on the entire pa-
rameter set θ = {β 10 , β 1 T , . . . , β (K−1)0 , β K−1
}, we denote the probabilities
Pr(G = k|X = x) = p k (x; θ).

When K = 2, this model is especially simple, since there is only a single
linear function. It is widely used in biostatistical applications where binary
responses (two classes) occur quite frequently. For example, patients survive
or die, have heart disease or not, or a condition is present or absent.

### 4.4.1 Fitting Logistic Regression Models

Logistic regression models are usually fit by maximum likelihood, using the
conditional likelihood of G given X. Since Pr(G|X) completely specifies the
conditional distribution, the multinomial distribution is appropriate. The
log-likelihood for N observations is

$$l(\theta) = \sum\_{i=1}^N \log p\_{g\_i}(x\_1; \theta)
\tag{4.19}$$

where p k (x i ; θ) = Pr(G = k|X = x i ; θ).

We discuss in detail the two-class case, since the algorithms simplify
considerably. It is convenient to code the two-class g i via a 0/1 response y i ,
where y i = 1 when g i = 1, and y i = 0 when g i = 2. Let p 1 (x; θ) = p(x; θ),
and p 2 (x; θ) = 1 − p(x; θ). The log-likelihood can be written

$$\begin{align}
l(\beta) &= \sum\_{i=1}^N \left\\{ y\_i \log p(x\_i; \beta) +
  (1-y\_i) \log (1 - p(x\_i;\beta)) \right\\} \\\\ &=
\sum\_{i=1}^N \left\\{ y\_i\beta^Tx\_i - \log(1+e^{\beta^T x\_i}) \right\\}
\tag{4.20}\end{align}$$

Here β = {β 10 , β 1 }, and we assume that the vector of inputs x i includes
the constant term 1 to accommodate the intercept.

To maximize the log-likelihood, we set its derivatives to zero. These score
equations are

$$\frac{\partial l(\beta)}{\partial \beta} =
\sum\_{i=1}^N x\_i (y\_i - p(x\_i; \beta)) = 0
\tag{4.21}$$

which are p + 1 equations nonlinear in β. Notice that since the P
first component of x i is 1, the first score equation specifies that i=1 y i = i=1 p(x i ; β);
the expected number of class ones matches the observed number (and hence
also class twos.)

To solve the score equations (4.21), we use the Newton–Raphson algo-
rithm, which requires the second-derivative or Hessian matrix

$$\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} =
- \sum\_{i=1}^N x\_ix\_i^T p(x\_i;\beta)(1 - p(x\_i;\beta))
\tag{4.22}$$

Starting with β old , a single Newton update is

$$\beta^{\text{new}} = \beta^{\text{old}} -
\left ( \frac{\partial^2 l(\beta)}{\partial\beta\partial\beta^T} \right )^{-1}
\frac{\partial l(\beta)}{\partial\beta}
\tag{4.23}$$

where the derivatives are evaluated at β old .

It is convenient to write the score and Hessian in matrix notation. Let
y denote the vector of y i values, X the N × (p + 1) matrix of x i values,
p the vector of fitted probabilities with ith element p(x i ; β old ) and W a
N × N diagonal matrix of weights with ith diagonal element p(x i ; β old )(1 −
p(x i ; β old )). Then we have

$$\begin{align}
\frac{\partial l(\beta)}{\partial\beta &=
\mathbf{X}^{-1}(\mathbf{y}-\mathbf{p})
\tag{4.24} \\\\ \frac{\partial^2 l(\beta)}{\partial\beta\partial\beta^T} &=
-\mathbf{X}^T \mathbf{W} \mathbf{X} \tag{4.25}
\end{align}$$

The Newton step is thus

$$\begin{align}
\beta^{\text{new}} &=
  \beta^{\text{old}} +
  (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{y}-\mathbf{p}) \\\\ &=
  (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\left (
    \mathbf{X}\beta^{\text{old}} + \mathbf{W}^{-1}(\mathbf{y}-\mathbf{p}) \right ) \\\\ &=
  (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}
\tag{4.26}\end{align}$$

In the second and third line we have re-expressed the Newton step as a
weighted least squares step, with the response

$$\mathbf{z} =
\mathbf{X}\beta^{\text{old}} + \mathbf{W}^{-1}(\mathbf{y} - \mathbf{p})
\tag{4.27}$$

sometimes known as the adjusted response. These equations get solved re-
peatedly, since at each iteration p changes, and hence so does W and z.
This algorithm is referred to as iteratively reweighted least squares or IRLS,
since each iteration solves the weighted least squares problem:

$$\beta^{\text{new}} \leftarrow
\text{arg}\min\_{\beta}
(\mathbf{z} - \mathbf{X}\beta)^T \mathbf{W} (\mathbf{z} - \mathbf{X}\beta)
\tag{4.28}$$

It seems that β = 0 is a good starting value for the iterative procedure,
although convergence is never guaranteed. Typically the algorithm does
converge, since the log-likelihood is concave, but overshooting can occur.
In the rare cases that the log-likelihood decreases, step size halving will
guarantee convergence.

For the multiclass case (K ≥ 3) the Newton algorithm can also be ex-
pressed as an iteratively reweighted least squares algorithm, but with a
vector of K −1 responses and a nondiagonal weight matrix per observation.
The latter precludes any simplified algorithms, and in this case it is numer-
ically more convenient to work with the expanded vector θ directly (Ex-
ercise 4.4). Alternatively coordinate-descent methods (Section 3.8.6) can
be used to maximize the log-likelihood efficiently. The R package glmnet
(Friedman et al., 2010) can fit very large logistic regression problems ef-
ficiently, both in N and p. Although designed to fit regularized models,
options allow for unregularized fits.

Logistic regression models are used mostly as a data analysis and infer-
ence tool, where the goal is to understand the role of the input variables
in explaining the outcome. Typically many models are fit in a search for a
parsimonious model involving a subset of the variables, possibly with some
interactions terms. The following example illustrates some of the issues
involved.

### 4.4.2 Example: South African Heart Disease

Here we present an analysis of binary data to illustrate the traditional
statistical use of the logistic regression model. The data in Figure 4.12 are a
subset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried
out in three rural areas of the Western Cape, South Africa (Rousseauw et
al., 1983). The aim of the study was to establish the intensity of ischemic
heart disease risk factors in that high-incidence region. The data represent
white males between 15 and 64, and the response variable is the presence or
absence of myocardial infarction (MI) at the time of the survey (the overall
prevalence of MI was 5.1% in this region). There are 160 cases in our data
set, and a sample of 302 controls. These data are described in more detail
in Hastie and Tibshirani (1987).

We fit a logistic-regression model by maximum likelihood, giving the
results shown in Table 4.2. This summary includes Z scores for each of the
coefficients in the model (coefficients divided by their standard errors); a
nonsignificant Z score suggests a coefficient can be dropped from the model.
Each of these correspond formally to a test of the null hypothesis that the
coefficient in question is zero, while all the others are not (also known as
the Wald test). A Z score greater than approximately 2 in absolute value
is significant at the 5% level.

There are some surprises in this table of coefficients, which must be in-
terpreted with caution. Systolic blood pressure ( sbp ) is not significant! Nor
is obesity , and its sign is negative. This confusion is a result of the corre-
lation between the set of predictors. On their own, both sbp and obesity
are significant, and with positive sign. However, in the presence of many
other correlated variables, they are no longer needed (and can even get a
negative sign).

At this stage the analyst might do some model selection; find a subset
of the variables that are sufficient for explaining their joint effect on the
prevalence of chd . One way to proceed by is to drop the least significant co-
efficient, and refit the model. This is done repeatedly until no further terms
can be dropped from the model. This gave the model shown in Table 4.3.

A better but more time-consuming strategy is to refit each of the models
with one variable removed, and then perform an analysis of deviance to
decide which variable to exclude. The residual deviance of a fitted model
is minus twice its log-likelihood, and the deviance between two models is
the difference of their individual residual deviances (in analogy to sums-of-
squares). This strategy gave the same final model as above.

How does one interpret a coefficient of 0.081 (Std. Error = 0.026) for
tobacco , for example? Tobacco is measured in total lifetime usage in kilo-
grams, with a median of 1.0kg for the controls and 4.1kg for the cases. Thus
an increase of 1kg in lifetime tobacco usage accounts for an increase in the
odds of coronary heart disease of exp(0.081) = 1.084 or 8.4%. Incorporat-
ing the standard error we get an approximate 95% confidence interval of
exp(0.081 ± 2 × 0.026) = (1.03, 1.14).

We return to these data in Chapter 5, where we see that some of the
variables have nonlinear effects, and when modeled appropriately, are not
excluded from the model.

### 4.4.3 Quadratic Approximations and Inference

The maximum-likelihood parameter estimates β̂ satisfy a self-consistency
relationship: they are the coefficients of a weighted least squares fit, where
the responses are

$$z\_i = x\_i^T\hat{\beta} +
\frac{(y\_i - \hat{p}\_i)}{\hat{p}\_i(1-\hat{p}\_i)}
\tag{4.29}$$

and the weights are w i = p̂ i (1 − p̂ i ), both depending on β̂ itself. Apart from
providing a convenient algorithm, this connection with least squares has
more to offer:

* The weighted residual sum-of-squares is the familiar Pearson chi-square statistic
  $$\sum\_{i=1}^N\frac{(y\_i-\hat{p}\_i)^2}{\hat{p}\_i(1-\hat{p}\_i)}\tag{4.30}$$
  a quadratic approximation to the deviance.
* Asymptotic likelihood theory says that if the model is correct, then
  β̂ is consistent (i.e., converges to the true β).
* A central limit theorem then shows that the distribution of β̂ con-
  verges to N (β, (X T WX) −1 ). This and other asymptotics can be de-
  rived directly from the weighted least squares fit by mimicking normal
  theory inference.
* Model building can be costly for logistic regression models, because
  each model fitted requires iteration. Popular shortcuts are the Rao
  score test which tests for inclusion of a term, and the Wald test which
  can be used to test for exclusion of a term. Neither of these require
  iterative fitting, and are based on the maximum-likelihood fit of the
  current model. It turns out that both of these amount to adding
  or dropping a term from the weighted least squares fit, using the
  same weights. Such computations can be done efficiently, without
  recomputing the entire weighted least squares fit.

Software implementations can take advantage of these connections. For
example, the generalized linear modeling software in R (which includes lo-
gistic regression as part of the binomial family of models) exploits them
fully. GLM (generalized linear model) objects can be treated as linear model
objects, and all the tools available for linear models can be applied auto-
matically.

### 4.4.4 L 1 Regularized Logistic Regression

The L 1 penalty used in the lasso (Section 3.4.2) can be used for variable
selection and shrinkage with any linear regression model. For logistic re-
gression, we would maximize a penalized version of (4.20):

$$\max\_{\beta\_0,\beta} \left \\{
  \sum\_{i=1}^N
  \left [ y\_i(\beta\_0+\beta^Tx\_i) - \log(1+e^{\beta\_0+\beta^Tx\_i}) \right ] -
  \lambda \sum\_{j=1}^p \|\beta\_j\| \right \\}
\tag{4.31}$$

As with the lasso, we typically do not penalize the intercept term, and stan-
dardize the predictors for the penalty to be meaningful. Criterion (4.31) is
concave, and a solution can be found using nonlinear programming meth-
ods (Koh et al., 2007, for example). Alternatively, using the same quadratic
approximations that were used in the Newton algorithm in Section 4.4.1,
we can solve (4.31) by repeated application of a weighted lasso algorithm.
Interestingly, the score equations [see (4.24)] for the variables with non-zero
coefficients have the form

$$\mathbf{x}\_j^T(\mathbf{y}-\mathbf{p}) =
\lambda \cdot \text{sign}(\beta\_j)
\tag{4.32}$$

which generalizes (3.58) in Section 3.4.4; the active variables are tied in
their generalized correlation with the residuals.

Path algorithms such as LAR for lasso are more difficult, because the
coefficient profiles are piecewise smooth rather than linear. Nevertheless,
progress can be made using quadratic approximations.

Figure 4.13 shows the L 1 regularization path for the South African
heart disease data of Section 4.4.2. This was produced using the R package
glmpath (Park and Hastie, 2007), which uses predictor–corrector methods
of convex optimization to identify the exact values of λ at which the active
set of non-zero coefficients changes (vertical lines in the figure). Here the
profiles look almost linear; in other examples the curvature will be more
visible.

Coordinate descent methods (Section 3.8.6) are very efficient for comput-
ing the coefficient profiles on a grid of values for λ. The R package glmnet
(Friedman et al., 2010) can fit coefficient paths for very large logistic re-
gression problems efficiently (large in N or p). Their algorithms can exploit
sparsity in the predictor matrix X, which allows for even larger problems.
See Section 18.4 for more details, and a discussion of L 1 -regularized multi-
nomial models.

### 4.4.5 Logistic Regression or LDA?

In Section 4.3 we find that the log-posterior odds between class k and K
are linear functions of x (4.9):

$$\begin{align}
\log\frac{\text{Pr}(G=k|X=x)}{\text{Pr}(G=K|X=x)} &=
  \log\frac{\pi\_k}{\pi_K} -
  \frac{1}{2}(\mu\_k+\mu\_K)^T\mathbf{\Sigma}^{-1}(\mu\_k-\mu\_K) \\\\ &+
  x^T \mathbf{\Sigma}^{-1}(\mu\_k - \mu\_K) \\\\ &=
  \alpha\_{k0} + \alpha\_k^T x
\tag{4.33}\end{align}$$

This linearity is a consequence of the Gaussian assumption for the class
densities, as well as the assumption of a common covariance matrix. The
linear logistic model (4.17) by construction has linear logits:

$$
\log\frac{\text{Pr}(G=k|X=x)}{\text{Pr}(G=K|X=x)} =
\beta\_{k0} + \beta\_k^T x
\tag{4.34}$$

It seems that the models are the same. Although they have exactly the same
form, the difference lies in the way the linear coefficients are estimated. The
logistic regression model is more general, in that it makes less assumptions.
We can write the joint density of X and G as

$$\text{Pr}(X, G=k) = \text{Pr}(X)\text{Pr}(G=k|X)
\tag{4.35}$$

where Pr(X) denotes the marginal density of the inputs X. For both LDA
and logistic regression, the second term on the right has the logit-linear
form

$$\text{Pr}(G=k|X=x) =
\frac{e^{\beta\_{k0}+\beta\_k^T x}}{1+\sum\_{l=1}^{K-1} e^{\beta\_{l0}+\beta\_l^Tx}}
\tag{4.36}$$

where we have again arbitrarily chosen the last class as the reference.

The logistic regression model leaves the marginal density of X as an arbi-
trary density function Pr(X), and fits the parameters of Pr(G|X) by max-
imizing the conditional likelihood—the multinomial likelihood with proba-
bilities the Pr(G = k|X). Although Pr(X) is totally ignored, we can think
of this marginal density as being estimated in a fully nonparametric and
unrestricted fashion, using the empirical distribution function which places
mass 1/N at each observation.

With LDA we fit the parameters by maximizing the full log-likelihood,
based on the joint density

$$\text{Pr}(X,G=k) = \phi(X;\mu\_k,\mathbf{\Sigma})\pi\_k
\tag{4.37}$$

where φ is the Gaussian density function. Standard normal theory leads
easily to the estimates μ̂ k , Σ̂, and π̂ k given in Section 4.3. Since the linear
parameters of the logistic form (4.33) are functions of the Gaussian param-
eters, we get their maximum-likelihood estimates by plugging in the corre-
sponding estimates. However, unlike in the conditional case, the marginal
density Pr(X) does play a role here. It is a mixture density

$$\text{Pr}(X) = \sum\_{k=1}^K \pi\_k \phi(X;\mu\_k, \mathbf{\Sigma})
\tag{4.38}$$

which also involves the parameters.

What role can this additional component/restriction play? By relying
on the additional model assumptions, we have more information about the
parameters, and hence can estimate them more efficiently (lower variance).
If in fact the true f k (x) are Gaussian, then in the worst case ignoring this
marginal part of the likelihood constitutes a loss of efficiency of about 30%
asymptotically in the error rate (Efron, 1975). Paraphrasing: with 30%
more data, the conditional likelihood will do as well.

For example, observations far from the decision boundary (which are
down-weighted by logistic regression) play a role in estimating the common
covariance matrix. This is not all good news, because it also means that
LDA is not robust to gross outliers.

From the mixture formulation, it is clear that even observations without
class labels have information about the parameters. Often it is expensive
to generate class labels, but unclassified observations come cheaply. By
relying on strong model assumptions, such as here, we can use both types
of information.

The marginal likelihood can be thought of as a regularizer, requiring
in some sense that class densities be visible from this marginal view. For
example, if the data in a two-class logistic regression model can be per-
fectly separated by a hyperplane, the maximum likelihood estimates of the
parameters are undefined (i.e., infinite; see Exercise 4.5). The LDA coeffi-
cients for the same data will be well defined, since the marginal likelihood
will not permit these degeneracies.

In practice these assumptions are never correct, and often some of the
components of X are qualitative variables. It is generally felt that logistic
regression is a safer, more robust bet than the LDA model, relying on fewer
assumptions. It is our experience that the models give very similar results,
even when LDA is used inappropriately, such as with qualitative predictors.