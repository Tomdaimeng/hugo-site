+++
title = "ESL-4.3 Linear Discriminant Analysis"
summary = """
统计学习基础（译注）第四章第三节，第 106-119 页。
"""

date = 2018-10-05T22:32:07+08:00
lastmod = 2018-10-05T22:32:07+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Decision theory for classification (Section 2.4) tells us that we need to know
the class posteriors Pr(G|X) for optimal classification. Suppose f k (x) is
the class-conditional density of X in class G = k, and let π k be the prior
probability of class k, with k=1 π k = 1. A simple application of Bayes
theorem gives us

$$\text{Pr}(G=k|X=x) = \frac{f\_k(x)\pi\_k}
{\sum\_{l=1}^K f\_l(x)\pi\_l}
\tag{4.7}$$

We see that in terms of ability to classify, having the f k (x) is almost equiv-
alent to having the quantity Pr(G = k|X = x).

Many techniques are based on models for the class densities:

* linear and quadratic discriminant analysis use Gaussian densities;
* more flexible mixtures of Gaussians allow for nonlinear decision bound-
aries (Section 6.8);
* general nonparametric density estimates for each class density allow
the most flexibility (Section 6.6.2);
* Naive Bayes models are a variant of the previous case, and assume
that each of the class densities are products of marginal densities;
that is, they assume that the inputs are conditionally independent in
each class (Section 6.6.3).

Suppose that we model each class density as multivariate Gaussian

$$f\_k(x) =
\frac{1}{(2\pi)^{p/2}\|\mathbf{\Sigma}\_k\|^{1 / 2}}
e^{-\frac{1}{2}(x-\mu\_k)^T\mathbf{\Sigma}\_k^{-1}(x-\mu\_k)}
\tag{4.8}$$

Linear discriminant analysis (LDA) arises in the special case when we
assume that the classes have a common covariance matrix Σ k = Σ ∀k. In
comparing two classes k and l, it is sufficient to look at the log-ratio, and
we see that

$$\begin{align}
\log \frac{\text{Pr}(G=k | X=x)}{\text{Pr}(G=l | X=x)} &=
\log \frac{f\_k(x)}{f\_l(x)} + \log \frac{\pi\_k}{\pi\_l} \\\\ &=
\log \frac{\pi\_k}{\pi\_l} -
\frac{1}{2}(\mu\_k+\mu\_l)^T\mathbf{\Sigma}^{-1}(\mu\_k - \mu\_l) \\\\ &+
x^T\mathbf{\Sigma}^{-1}(\mu\_k - \mu\_l)
\end{align}\tag{4.9}$$

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_05.png"
  title="**图4.5**："
>}}

an equation linear in x. The equal covariance matrices cause the normal-
ization factors to cancel, as well as the quadratic part in the exponents.
This linear log-odds function implies that the decision boundary between
classes k and l—the set where Pr(G = k|X = x) = Pr(G = l|X = x)—is
linear in x; in p dimensions a hyperplane. This is of course true for any pair
of classes, so all the decision boundaries are linear. If we divide IR p into
regions that are classified as class 1, class 2, etc., these regions will be sep-
arated by hyperplanes. Figure 4.5 (left panel) shows an idealized example
with three classes and p = 2. Here the data do arise from three Gaus-
sian distributions with a common covariance matrix. We have included in
the figure the contours corresponding to 95% highest probability density,
as well as the class centroids. Notice that the decision boundaries are not
the perpendicular bisectors of the line segments joining the centroids. This
would be the case if the covariance Σ were spherical σ 2 I, and the class
priors were equal. From (4.9) we see that the linear discriminant functions

$$\delta\_k(x) = x^T\mathbf{\Sigma}^{-1}\mu\_k - 
\frac{1}{2}\mu\_k^T\mathbf{\Sigma}^{-1}\mu\_k + \log \pi\_k
\tag{4.10}$$

are an equivalent description of the decision rule, with G(x) = argmax k δ k (x).

In practice we do not know the parameters of the Gaussian distributions,
and will need to estimate them using our training data:

* $\hat{\pi}\_k = N\_k / N$, where N k is the number of class-k observations;
* $\hat{\mu}\_k = \sum\_{g\_i = k} x\_i / N\_k$;
* $\hat{\mathbf{\Sigma}} = \sum\_{k=1}^K \sum\_{g\_i=k} (x\_i-\hat{\mu}\_k)(x\_i - \hat{\mu}\_k)^T / (N-K)$

Figure 4.5 (right panel) shows the estimated decision boundaries based on
a sample of size 30 each from three Gaussian distributions. Figure 4.1 on
page 103 is another example, but here the classes are not Gaussian.

With two classes there is a simple correspondence between linear dis-
criminant analysis and classification by linear regression, as in (4.5). The
LDA rule classifies to class 2 if

$$ x^T\hat{\mathbf{\Sigma}}^{-1}(\hat{\mu}\_2 - \hat{\mu}\_1) >
\frac{1}{2} (\hat{\mu}\_2 + \hat{\mu}\_1)^T \hat{\mathbf{\Sigma}}^{-1} (\hat{\mu}\_2 - \hat{\mu}\_1) -
\log (N\_2 / N\_1)
\tag{4.11}$$

and class 1 otherwise. Suppose we code the targets in the two classes as +1
and −1, respectively. It is easy to show that the coefficient vector from least
squares is proportional to the LDA direction given in (4.11) (Exercise 4.2).
[In fact, this correspondence occurs for any (distinct) coding of the targets;
see Exercise 4.2]. However unless N 1 = N 2 the intercepts are different and
hence the resulting decision rules are different.

Since this derivation of the LDA direction via least squares does not use a
Gaussian assumption for the features, its applicability extends beyond the
realm of Gaussian data. However the derivation of the particular intercept
or cut-point given in (4.11) does require Gaussian data. Thus it makes
sense to instead choose the cut-point that empirically minimizes training
error for a given dataset. This is something we have found to work well in
practice, but have not seen it mentioned in the literature.

With more than two classes, LDA is not the same as linear regression of
the class indicator matrix, and it avoids the masking problems associated
with that approach (Hastie et al., 1994). A correspondence between regres-
sion and LDA can be established through the notion of optimal scoring,
discussed in Section 12.5.

Getting back to the general discriminant problem (4.8), if the Σ k are
not assumed to be equal, then the convenient cancellations in (4.9) do not
occur; in particular the pieces quadratic in x remain. We then get quadratic
discriminant functions (QDA),

$$ \delta\_k(x) = 
-\frac{1}{2} \log \|\mathbf{\Sigma}\_k\| -
\frac{1}{2} (x-\mu\_k)^T \mathbf{\Sigma}\_k^{-1} (x-\mu\_k) +
\log \pi\_k
\tag{4.12}$$

The decision boundary between each pair of classes k and l is described by
a quadratic equation {x : δ k (x) = δ l (x)}.

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_06.png"
  title="**图4.6**："
>}}

Figure 4.6 shows an example (from Figure 4.1 on page 103) where the
three classes are Gaussian mixtures (Section 6.8) and the decision bound-
aries are approximated by quadratic equations in x. Here we illustrate
two popular ways of fitting these quadratic boundaries. The right plot
uses QDA as described here, while the left plot uses LDA in the enlarged
five-dimensional quadratic polynomial space. The differences are generally
small; QDA is the preferred approach, with the LDA method a convenient
substitute 2 .

The estimates for QDA are similar to those for LDA, except that separate
covariance matrices must be estimated for each class. When p is large this
can mean a dramatic increase in parameters. Since the decision boundaries
are functions of the parameters of the densities, counting the number of
parameters must be done with care. For LDA, it seems there are (K −
1) × (p + 1) parameters, since we only need the differences δ k (x) − δ K (x)
between the discriminant functions where K is some pre-chosen class (here
we have chosen the last), and each difference requires p + 1 parameters 3 .
Likewise for QDA there will be (K − 1) × {p(p + 3)/2 + 1} parameters.
Both LDA and QDA perform well on an amazingly large and diverse set
of classification tasks. For example, in the STATLOG project (Michie et
al., 1994) LDA was among the top three classifiers for 7 of the 22 datasets,
QDA among the top three for four datasets, and one of the pair were in the
top three for 10 datasets. Both techniques are widely used, and entire books
are devoted to LDA. It seems that whatever exotic tools are the rage of the
day, we should always have available these two simple tools. The question
arises why LDA and QDA have such a good track record. The reason is not
likely to be that the data are approximately Gaussian, and in addition for
LDA that the covariances are approximately equal. More likely a reason is
that the data can only support simple decision boundaries such as linear or
quadratic, and the estimates provided via the Gaussian models are stable.
This is a bias variance tradeoff—we can put up with the bias of a linear
decision boundary because it can be estimated with much lower variance
than more exotic alternatives. This argument is less believable for QDA,
since it can have many parameters itself, although perhaps fewer than the
non-parametric alternatives.

### 4.3.1 Regularized Discriminant Analysis

Friedman (1989) proposed a compromise between LDA and QDA, which
allows one to shrink the separate covariances of QDA toward a common
covariance as in LDA. These methods are very similar in flavor to ridge
regression. The regularized covariance matrices have the form

$$\hat{\mathbf{\Sigma}}\_k(\alpha) =
\alpha \hat{\mathbf{\Sigma}}\_k + (1-\alpha)\hat{\mathbf{\Sigma}}
\tag{4.13}$$

where Σ̂ is the pooled covariance matrix as used in LDA. Here α ∈ [0, 1]
allows a continuum of models between LDA and QDA, and needs to be
specified. In practice α can be chosen based on the performance of the
model on validation data, or by cross-validation.

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_07.png"
  title="**图4.7**："
>}}

Figure 4.7 shows the results of RDA applied to the vowel data. Both
the training and test error improve with increasing α, although the test
error increases sharply after α = 0.9. The large discrepancy between the
training and test error is partly due to the fact that there are many repeat
measurements on a small number of individuals, different in the training
and test set.

Similar modifications allow Σ̂ itself to be shrunk toward the scalar
covariance,

$$ \hat{\mathbf{\Sigma}}(\gamma) =
\gamma\hat{\mathbf{\Sigma}} + (1-\gamma)\hat{\sigma}^2\mathbf{I}
\tag{4.14}$$

for γ ∈ [0, 1]. Replacing Σ̂ in (4.13) by Σ̂(γ) leads to a more general family
of covariances Σ̂(α, γ) indexed by a pair of parameters.

In Chapter 12, we discuss other regularized versions of LDA, which are
more suitable when the data arise from digitized analog signals and images.
In these situations the features are high-dimensional and correlated, and the
LDA coefficients can be regularized to be smooth or sparse in the original
domain of the signal. This leads to better generalization and allows for
easier interpretation of the coefficients. In Chapter 18 we also deal with
very high-dimensional problems, where for example the features are gene-
expression measurements in microarray studies. There the methods focus
on the case γ = 0 in (4.14), and other severely regularized versions of LDA.

### 4.3.2 Computations for LDA

As a lead-in to the next topic, we briefly digress on the computations
required for LDA and especially QDA. Their computations are simplified
by diagonalizing Σ̂ or Σ̂ k . For the latter, suppose we compute the eigen-
decomposition for each Σ̂ k = U k D k U Tk , where U k is p × p orthonormal,
and D k a diagonal matrix of positive eigenvalues d kl . Then the ingredients
for δ k (x) (4.12) are

* $(x-\hat{\mu}\_k)^T\hat{\mathbf{\Sigma}}\_k^{-1}(x-\hat{\mu}\_k)=
  [\mathbf{U}\_k^T(x-\hat{\mu}\_k)]^T
  \hat{\mathbf{D}}\_k^{-1}
  [\mathbf{U}\_k^T(x-\hat{\mu}\_k)]$
* $\log \|\hat{\mathbf{\Sigma}}\_k\| = \sum\_l \log d\_{kl}$

In light of the computational steps outlined above, the LDA classifier
can be implemented by the following pair of steps:

* Sphere the data with respect to the common covariance estimate Σ̂:
  X ∗ ← D − 2 U T X, where Σ̂ = UDU T . The common covariance esti-
  mate of X ∗ will now be the identity.
* Classify to the closest class centroid in the transformed space, modulo
  the effect of the class prior probabilities π k .

### 4.3.3 Reduced-Rank Linear Discriminant Analysis

So far we have discussed LDA as a restricted Gaussian classifier. Part of
its popularity is due to an additional restriction that allows us to view
informative low-dimensional projections of the data.

The K centroids in p-dimensional input space lie in an affine subspace
of dimension ≤ K − 1, and if p is much larger than K, this will be a con-
siderable drop in dimension. Moreover, in locating the closest centroid, we
can ignore distances orthogonal to this subspace, since they will contribute
equally to each class. Thus we might just as well project the X ∗ onto this
centroid-spanning subspace H K−1 , and make distance comparisons there.
Thus there is a fundamental dimension reduction in LDA, namely, that we
need only consider the data in a subspace of dimension at most K − 1.
If K = 3, for instance, this could allow us to view the data in a two-
dimensional plot, color-coding the classes. In doing so we would not have
relinquished any of the information needed for LDA classification.

What if K > 3? We might then ask for a L < K −1 dimensional subspace
H L ⊆ H K−1 optimal for LDA in some sense. Fisher defined optimal to
mean that the projected centroids were spread out as much as possible in
terms of variance. This amounts to finding principal component subspaces
of the centroids themselves (principal components are described briefly in
Section 3.5.1, and in more detail in Section 14.5.1). Figure 4.4 shows such an
optimal two-dimensional subspace for the vowel data. Here there are eleven
classes, each a different vowel sound, in a ten-dimensional input space. The
centroids require the full space in this case, since K − 1 = p, but we have
shown an optimal two-dimensional subspace. The dimensions are ordered,
so we can compute additional dimensions in sequence. Figure 4.8 shows four
additional pairs of coordinates, also known as canonical or discriminant
variables. In summary then, finding the sequences of optimal subspaces
for LDA involves the following steps:

* compute the K × p matrix of class centroids M and the common
  covariance matrix W (for within-class covariance);
* compute M ∗ = MW − 2 using the eigen-decomposition of W;
* compute B ∗ , the covariance matrix of M ∗ (B for between-class covari-
  ance), and its eigen-decomposition B ∗ = V ∗ D B V ∗ T . The columns
  v l ∗ of V ∗ in sequence from first to last define the coordinates of the
  optimal subspaces.

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_08.png"
  title="**图4.8**："
>}}

Combining all these operations the lth discriminant variable is given by
Z l = v l T X with v l = W − 2 v l ∗ .

Fisher arrived at this decomposition via a different route, without refer-
ring to Gaussian distributions at all. He posed the problem:

> Find the linear combination Z = a T X such that the between-
> class variance is maximized relative to the within-class variance.

Again, the between class variance is the variance of the class means of
Z, and the within class variance is the pooled variance about the means.
Figure 4.9 shows why this criterion makes sense. Although the direction
joining the centroids separates the means as much as possible (i.e., max-
imizes the between-class variance), there is considerable overlap between
the projected classes due to the nature of the covariances. By taking the
covariance into account as well, a direction with minimum overlap can be
found.

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_09.png"
  title="**图4.9**："
>}}

The between-class variance of Z is a T Ba and the within-class variance
a T Wa, where W is defined earlier, and B is the covariance matrix of the
class centroid matrix M. Note that B + W = T, where T is the total
covariance matrix of X, ignoring class information.

Fisher’s problem therefore amounts to maximizing the Rayleigh quotient,

$$ \max\_{a} \frac{a^T \mathbf{B} a}{a^T \mathbf{W} a}
\tag{4.15}$$

or equivalently

$$ \max\_{a} a^T \mathbf{B} a
\text{ subject to } a^T \mathbf{W} a = 1
\tag{4.16}$$

This is a generalized eigenvalue problem, with a given by the largest
eigenvalue of W −1 B. It is not hard to show (Exercise 4.1) that the optimal
a 1 is identical to v 1 defined above. Similarly one can find the next direction
a 2 , orthogonal in W to a 1 , such that a T 2 Ba 2 /a T 2 Wa 2 is maximized; the
solution is a 2 = v 2 , and so on. The a l are referred to as discriminant
coordinates, not to be confused with discriminant functions. They are also
referred to as canonical variates, since an alternative derivation of these
results is through a canonical correlation analysis of the indicator response
matrix Y on the predictor matrix X. This line is pursued in Section 12.5.

To summarize the developments so far:
* Gaussian classification with common covariances leads to linear deci-
  sion boundaries. Classification can be achieved by sphering the data
  with respect to W, and classifying to the closest centroid (modulo
  log π k ) in the sphered space.
* Since only the relative distances to the centroids count, one can con-
  fine the data to the subspace spanned by the centroids in the sphered
  space.
* This subspace can be further decomposed into successively optimal
  subspaces in term of centroid separation. This decomposition is iden-
  tical to the decomposition due to Fisher.

The reduced subspaces have been motivated as a data reduction (for
viewing) tool. Can they also be used for classification, and what is the
rationale? Clearly they can, as in our original derivation; we simply limit
the distance-to-centroid calculations to the chosen subspace. One can show
that this is a Gaussian classification rule with the additional restriction
that the centroids of the Gaussians lie in a L-dimensional subspace of IR p .
Fitting such a model by maximum likelihood, and then constructing the
posterior probabilities using Bayes’ theorem amounts to the classification
rule described above (Exercise 4.8).

Gaussian classification dictates the log π k correction factor in the dis-
tance calculation. The reason for this correction can be seen in Figure 4.9.
The misclassification rate is based on the area of overlap between the two
densities. If the π k are equal (implicit in that figure), then the optimal
cut-point is midway between the projected means. If the π k are not equal,
moving the cut-point toward the smaller class will improve the error rate.
As mentioned earlier for two classes, one can derive the linear rule using
LDA (or any other method), and then choose the cut-point to minimize
misclassification error over the training data.

As an example of the benefit of the reduced-rank restriction, we return
to the vowel data. There are 11 classes and 10 variables, and hence 10
possible dimensions for the classifier. We can compute the training and
test error in each of these hierarchical subspaces; Figure 4.10 shows the
results. Figure 4.11 shows the decision boundaries for the classifier based
on the two-dimensional LDA solution.


{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_10.png"
  title="**图4.10**："
>}}

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_11.png"
  title="**图4.11**："
>}}

There is a close connection between Fisher’s reduced rank discriminant
analysis and regression of an indicator response matrix. It turns out that
LDA amounts to the regression followed by an eigen-decomposition of
Ŷ T Y. In the case of two classes, there is a single discriminant variable
that is identical up to a scalar multiplication to either of the columns of Ŷ.
These connections are developed in Chapter 12. A related fact is that if one
transforms the original predictors X to Ŷ, then LDA using Ŷ is identical
to LDA in the original space (Exercise 4.3).