+++
title = "ESL-4.3 线性判别分析"
summary = """
统计学习基础（译注）第四章第三节，第 106-119 页。
根据比较条件于不同类型下的概率大小来分类，
在同协方差矩阵的高斯分布假设下，得到线性判别分析；
在不同协方差矩阵的假设下，得到二次判别分析。
线性判别分析同时可以应用在降维的输入向量空间上，
这也提供了分类问题可视化的工具。
本节的逻辑有些生涩，需要再次修整。
"""

date = 2018-10-08T13:32:07+08:00
lastmod = 2018-10-08T13:32:07+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

从分类问题的决策理论（第 2.4 节）可知，
最优的分类依赖于对类型的后验概率 $\text{Pr}(G|X)$。
假设 $f\_k(x)$ 为条件于类型 $G=k$ 的变量 $X$ 的分布密度，
$\pi\_k$ 为类型 k 的先验概率，使得 $\sum\_{k=1}^K \pi\_k = 1$。
则根据贝叶斯公式可得：

$$\text{Pr}(G=k|X=x) = \frac{f\_k(x)\pi\_k}
{\sum\_{l=1}^K f\_l(x)\pi\_l}
\tag{4.7}$$

可见从分类能力的角度而言，密度函数 $f\_k(x)$ 提供的信息基本等价于条件概率
$\text{Pr}(G=k|X=x)$ 可提供的信息。

很多方法都是基于对类型条件密度函数的建模：

* 使用高斯分布密度的线性和二次判别分析；
* 使用更灵活的高斯混合分布，可允许非线性判定边界（第 6.8 节）；
* 最灵活的对每个类别进行一般性的非参数密度函数估计（第 6.6.2 节）；
* 朴素贝叶斯模型（第 6.6.3 节）为上者的一个变体，假设每个类型的密度函数均为边际密度函数的乘积；
  即假设条件于各个类型时，输入变量之间相互独立（第 6.6.3 节）。

假设每个类型的条件概率密度为多元高斯分布：

$$f\_k(x) =
\frac{1}{(2\pi)^{p/2}\|\mathbf{\Sigma}\_k\|^{1 / 2}}
e^{-\frac{1}{2}(x-\mu\_k)^T\mathbf{\Sigma}\_k^{-1}(x-\mu\_k)}
\tag{4.8}$$

*线性判别分析（linear discriminant analysis，LDA）*
即为假设每个类型都有统一的协方差矩阵 $\mathbf{\Sigma}\_k = \mathbf{\Sigma}, \forall k$
的特例。
因此在对比两个类型的密度分布时，只需要对比它们的比例的对数，即：

$$\begin{align}
\log \frac{\text{Pr}(G=k | X=x)}{\text{Pr}(G=l | X=x)} &=
\log \frac{f\_k(x)}{f\_l(x)} + \log \frac{\pi\_k}{\pi\_l} \\\\ &=
\log \frac{\pi\_k}{\pi\_l} -
\frac{1}{2}(\mu\_k+\mu\_l)^T\mathbf{\Sigma}^{-1}(\mu\_k - \mu\_l) \\\\ &+
x^T\mathbf{\Sigma}^{-1}(\mu\_k - \mu\_l)
\end{align}\tag{4.9}$$

此为一个关于 $x$ 的线性函数。
其中正态分布的常数项部分被抵消掉；
两个概率中的协方差矩阵相同，导致二次项被抵消掉。
线性的对数几率函数使得类型 k 和 l 之间的判定边界对 x 为线性；
即使得 $\text{Pr}(G=k|X=x) = \text{Pr}(G=l|X=x)$
的点组成了 p 维空间中的一个超平面。
对任意两个类型，上述结论都成立，所以所有的判定边界均为线性。
若将输入变量空间 $\mathbb{R}^p$ 分成不同类型的区间，
那么这些区间的边界均为超平面。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_05.png"
  title="**图4.5**：左图为三个高斯分布，其协方差矩阵相同，均值不同。椭圆形为包围了概率为 95% 的区域的等概率密度曲线。每两个类型之间的贝叶斯判定边界为图中的虚线，三个类型之间相互的贝叶斯判定边界为图中的实线（实际为前者虚线的一部分）。右图为从三个高斯分布生成的每个类型 30 个数据点，以及线性判别分析生成的判定边界。"
>}}

图 4.5 的左图显示了 3 个类型 2 个输入变量的例子。
输入变量服从高斯分布，其协方差矩阵相同。
椭圆形为包围了概率为 95% 的区域的等概率密度曲线，
十字标记点为类型的中心点。
需要注意两个类型之间的判定边界并不是其中心点连线的垂直平分线。
只有当协方差矩阵 $\mathbf{\Sigma}$ 为 $\sigma^2\mathbf{I}$ 的形式，
并且类型的边际概率相同时，判定边界才是其中心点连线的垂直平分线。

通过等式 4.9，可证明
*线性判别函数（linear discriminant function）*

$$\delta\_k(x) = x^T\mathbf{\Sigma}^{-1}\mu\_k - 
\frac{1}{2}\mu\_k^T\mathbf{\Sigma}^{-1}\mu\_k + \log \pi\_k
\tag{4.10}$$

是另一种表达模型分类规则的方式，最终的分类为
$G(x) = \text{argmax}\_k\delta\_k(x)$。

在实际操作中，高斯分布的参数为未知参数，需要用训练数据进行估计：

* $\hat{\pi}\_k = N\_k / N$，其中 $N\_k$ 为训练样本中类型 k 的观测个数；
* $\hat{\mu}\_k = \sum\_{g\_i = k} x\_i / N\_k$；
* $\hat{\mathbf{\Sigma}} = \sum\_{k=1}^K \sum\_{g\_i=k} (x\_i-\hat{\mu}\_k)(x\_i - \hat{\mu}\_k)^T / (N-K)$。

图 4.5 的右图为建立在每个类型生成了 30 个高斯分布样本点上的估计判定边界。
图 4.1 为另一个线性判别分析的例子，其中各个类型的条件概率分布不是高斯分布。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_01.png"
  title="**图4.1**：以三个类型的数据为例。左图为线性判别分析产生的线性判定边界；右图为二次判定边界，通过对五个输入变量 $X\_1$、$X\_2$、$X\_1X\_2$、$X\_1^2$、$X\_2^2$ 产生的线性判定边界。此空间上的线性判定比较相当于原空间上的二次判定比较。"
>}}

在两个类型的问题中，线性判别分析与线性回归分类（等式 4.5）存在简单的对应关系。
线性判别分析分类为类型 2 的条件为：

$$ x^T\hat{\mathbf{\Sigma}}^{-1}(\hat{\mu}\_2 - \hat{\mu}\_1) >
\frac{1}{2} (\hat{\mu}\_2 + \hat{\mu}\_1)^T \hat{\mathbf{\Sigma}}^{-1} (\hat{\mu}\_2 - \hat{\mu}\_1) -
\log (N\_2 / N\_1)
\tag{4.11}$$

否则分类为类型 1。
假设将两个类型分别编码为 +1 和 -1，
则可证明最小二乘估计的系数向量与等式 4.11 中的线性判别分析的方向成正比
（实际上，对两个类型任意的编码方式都会产生这种对应关系，见练习 4.2）。
然而，除非 $N\_1=N\_2$，两者的截距项不同，
因此最终的判别规则也不同。

从最小二乘方法的角度来推导出的线性判别分析的方向变量并没有假设输入变量的高斯分布，
所以其应用范围可延伸到非高斯分布的样本中。
不过确定等式 4.11 中的特定截距或判别点却是依赖于高斯分布假设的。
因此不如直接根据实际样本选择最小化训练集误差的判别点。
作者在实际操作中发现这种做法比较有效，但没有在学术领域中见到有人提及。

在类型个数大于 2 时，线性判别分析与对类型的指示矩阵线性回归的方法不同，
前者避免了影响后者的类型屏蔽问题（Hastie et al., 1994）。
第 12.5 节中介绍的 *最优评分（optimal scoring）* 概念，
可建立两者之间的对比关系。

回到一般的判别问题（等式 4.8），
如果不同类型的协方差矩阵 $\mathbf{\Sigma}\_k$ 不相等，
则等式 4.9 中无法简化成最后的形式，即会留下无法抵消掉的 x 的二次项。
此时得到了
*二次判别分析（quadratic discriminant analysis，QDA）*
的判别函数：

$$ \delta\_k(x) = 
-\frac{1}{2} \log \|\mathbf{\Sigma}\_k\| -
\frac{1}{2} (x-\mu\_k)^T \mathbf{\Sigma}\_k^{-1} (x-\mu\_k) +
\log \pi\_k
\tag{4.12}$$

每两个类型 k 和 l 之间的判定边界为一个二次函数曲线
$\\{x: \delta\_k(x) = \delta\_l(x)\\}$。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_06.png"
  title="**图4.6**：二次函数边界的两种拟合方法，数据与图 4.1 相同。左图为在五维输入向量空间（$X\_1$，$X\_2$，$X\_1X\_2$，$X\_1^2$，$X\_2^2$）上的线性判别分析；右图为直接使用二次判别分析产生的判定边界。两者通常差别不大。"
>}}

图 4.6 以三类型的高斯混合分布（见第 6.8 节）为例（数据与图 4.1 相同），
用 x 的二次函数拟合判定边界。
图中展示了两种常见的二次判定边界拟合方法。
右图即为上述的二次判别分析方法拟合，
而左图为将原二维输入向量空间进行二次项拓展后的五维空间上的线性判别分析。
通常两者的差别不大，
二次判别分析为首选的方法，
而拓展后的线性判别分析方法也是一个方便的替代[^1]。

二次判别分析的估计方法与线性判别方法类似，除了需要对每个类别分别估计协方差矩阵。
当 p 比较大时，这会导致参数维度的巨大增加。
由于判定边界同时为密度函数中参数的函数，
所以有必要清楚确定算法所用到的参数个数。
线性判别分析的参数个数为 $(K-1) \times (p+1)$，
因为实际上只需要先选中一个类型后（这里选择了最后一个）
对比其他类型与其的判别函数大小 $\delta\_k(x) - \delta\_K(x)$，
而其中每个差需要 $p+1$ 个参数[^2]。
与之类似的计法，二次判别分析的参数个数为 $(K-1)\times \\{p(p+3)/2+1\\}$。
线性和二次判别分析在很多不同的分类问题中都有良好的表现。
例如，STATLOG 项目[^3]（Michie et al., 1994）的 22 个数据集中，
线性判别分析在 7 个数据集上的表现列于所有方法的前三，
二次判别分析在 4 个数据集上的表现列于前三，
在 10 个数据集中两者至少有一个列于前三。
这两个方法都被很广泛地使用，并有很多讨论线性判别分析的专著。
尽管各种新的神奇的方法源源不断地出现，
在实际问题中还是应尝试下这两种简单的方法。
也有关于为何线性和二次判别分析的表现如此良好的讨论。
实际场景中的数据不可能总会近似符合高斯分布的假设，
而不同类型的密度函数中协方差矩阵相同的假设也很不可能被满足。
一个可能的解释是真实的数据只能支持低复杂度的判定边界，例如线性或二次函数，
并且基于高斯模型的估计有比较好的稳定性。
这涉及了偏差和方差的权衡：
线性判定边界的偏差换来了比其他更神奇分类方法更小的估计方差。
二次判别分析中的参数个数并不少，所以这个解释的可信度不如线性判别分析，
但尽管如此，其参数个数仍要比其他非参数的方法少了很多。

### 4.3.1 正则化判别分析

Friedman (1989) 提出了一种线性和二次判别分析的折衷方法，
让二次判别分析中各个类型不同的协方差矩阵向一个共同的协方差矩阵进行“收缩”。
这种方法的思路与岭回归非常相似。
正则化协方差矩阵的形式为：

$$\hat{\mathbf{\Sigma}}\_k(\alpha) =
\alpha \hat{\mathbf{\Sigma}}\_k + (1-\alpha)\hat{\mathbf{\Sigma}}
\tag{4.13}$$

其中 $\hat{\mathbf{\Sigma}}$ 为线性判别分析中协方差矩阵的估计。
参数 $\alpha \in [0,1]$ 使得最终模型可以在线性和二次判别分析之间连续地变动，
并且待指定。
在实际应用中，可以根据在验证集上的表现或者交叉验证过程来确定 $\alpha$。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_07.png"
  title="**图4.7**：元音识别问题，一系列参数 $\alpha\in [0,1]$ 的正则化判别分析在训练集和测试集上的错误率。测试集错误率的最优点在 $\alpha=0.9$ 附近，对应的模型接近二次判别分析。"
>}}

图 4.7 展示了
*正则化判别分析（regularized discriminant analysis，RDA）*
在元音识别问题中的表现。
在训练集和测试集上的错误率均随着 $\alpha$ 的增大而降低，
但测试集误差在 $\alpha=0.9$ 之后迅速上升。
训练集和测试集的表现差别很大，
可能是由于样本中有很多重复的输出变量测量，而在训练集和测试集中的输出变量不同[^4]。

另一个类似的变化是让 $\hat{\mathbf{\Sigma}}$ 向独立同方差进行“收缩”：

$$ \hat{\mathbf{\Sigma}}(\gamma) =
\gamma\hat{\mathbf{\Sigma}} + (1-\gamma)\hat{\sigma}^2\mathbf{I}
\tag{4.14}$$

其中的参数 $\gamma \in [0,1]$。
用 $\hat{\mathbf{\Sigma}}(\gamma)$ 替换掉等式 4.13 中的 $\hat{\mathbf{\Sigma}}$，
得到了一个更广义的有两个参数的协方差矩阵 $\hat{\mathbf{\Sigma}}(\alpha, \gamma)$。

第十二章会介绍其他正则化版本的线性判别分析，
使之更适应于处理数字化的模拟信号和图像数据。
在这些场景中，存在大量的并且互相关联的特征变量，
正则化可以让线性判别分析的系数在原信号的定义域上或平滑或稀疏，
从而使模型的适用性更广，系数也有更好地可解释性。
第十八章会集中讨论超高维度问题，
例如微阵列研究中的基因表达测量度特征变量，
介绍一些基于 $\gamma=0$（等式 4.14）的方法，
和一些其他正则化方式的线性判别分析。

### 4.3.2 线性判别分析的计算

这里简要地讨论线性和二次判别分析的计算方法。
通过矩阵 $\hat{\mathbf{\Sigma}}$ 或 $\hat{\mathbf{\Sigma}}$ 的对角线分解
可以简化计算过程。
假设有后者的特征值分解
$\hat{\mathbf{\Sigma}}\_k = \mathbf{U}\_k \mathbf{D}\_k \mathbf{U}\_k^T$，
其中 $\mathbf{U}\_k$ 为 $p\times p$ 的标准正交矩阵，
$\mathbf{D}\_k$ 为对角矩阵，对角线元素为正的特征值 $d\_{kl}$。
则 $\delta\_k(x)$ 的项可写为（等式 4.12）：

* $(x-\hat{\mu}\_k)^T\hat{\mathbf{\Sigma}}\_k^{-1}(x-\hat{\mu}\_k)=
  [\mathbf{U}\_k^T(x-\hat{\mu}\_k)]^T
  \hat{\mathbf{D}}\_k^{-1}
  [\mathbf{U}\_k^T(x-\hat{\mu}\_k)]$
* $\log \|\hat{\mathbf{\Sigma}}\_k\| = \sum\_l \log d\_{kl}$

基于上述描述的计算过程，可以通过下面的步骤来实现线性判别分析分类算法：

* 针对共同的协方差矩阵 $\hat{\mathbf{\Sigma}}$ 对数据进行球形化（sphere）处理：
  $X^* \leftarrow \mathbf{D}^{-\frac{1}{2}} \mathbf{U}^T X$，
  其中 $\hat{\mathbf{\Sigma}} = \mathbf{U}\mathbf{D}\mathbf{U}^T$。
  转化后的 $X^*$ 数据中不同类型会有相同的协方差矩阵。
* 在转化后的空间上，一个点的分类是距离其最近的类型中心点。

### 4.3.3 降秩线性判别分析

线性判别分析是一个有约束的高斯分类器，
另外它也常被用于数据的可视化，将数据中的信息投射到低维度空间上。

在 p 维度的输入变量空间中，K 的中心点可组成一个维度不大于 $K-1$ 的仿射子空间[^5]。
若 p 远大于 K，则有可能大幅度降低问题的维度。
在寻找距离某个点最近的中心点时，可以忽略这个点与这个子空间之间的正交距离，
因为这个距离对所有中心点的距离有相同的影响[^6]。
既然如此，那么可以将这个点 $X^*$ 投影在中心点生成的子空间 $H\_{K-1}$ 上，
在这个子空间上比较投影点与各个中心点的距离。
这便是线性判别分析的维度降低，
即只需要在最高 $K-1$ 维度的空间上对数据分类。
例如当 $K=3$ 时，可以将投射到一个平面上，再用颜色标记出不同的类型。
投射后的图中包含了线性判别分析在分类问题中所需要的所有信息。

对 $K>3$ 的情况，可能会在更低维度的 $L < K-1$ 子空间上投射数据，
通过某种最优条件选取线性判别分析的空间 $H\_L \subseteq H\_{K-1}$ 。
费雪（Ronald Aylmer Fisher）将“最优条件”的定义为，
使中心点的投影尽可能地分散，即这几个点的方差最大。
这实际上为寻找中心点的主成分子空间
（第 3.5.1 节简要地介绍了主成分分析，在第 14.5.1 会更深入讨论）。
图 4.4 为元音识别数据在最优的二维子空间的投影图。
其中的数据有 11 个类型，每个类型为一个元音，输入空间为 10 维。
这个例子中 $K-1 = p$，所以中心点的子空间实际为整个输入空间，
图中所示的为根据主成分选择的最优的二维子空间。
维度是有序的（主成分顺序），因此可以依次地计算各个维度。
图 4.8 为另外四对坐标维度的投影图，也称之为
*典型（canonical）* 或 *判别（discriminant）* 变量。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_04.png"
  title="**图4.4**：元音识别训练数据的二维投影图。共有 11 个类型，输入变量空间为 $X\in \mathbb{R}^{10}$，这是线性判别模型中最优的二维投影子空间（见第 4.3.3 节）。加粗的圆点为每个类型的均值中心向量在这个空间的投射点。图中各个类型有明显的重叠。"
>}}

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_08.png"
  title="**图4.8**：数据在四对典型变量构成的平面上的投影。注意随着典型变量排序的变大，中心点越来越集中。在右下图中，所有中心点几乎重叠在一起，而投影点的分类也最没有规律。"
>}}

综上所述，计算线性判别分析的最优子空间序列的步骤为： 

* 计算 $K\times p$ 的类型中心点矩阵 $\mathbf{M}$，以及
  样本总体的共同协方差矩阵 $\mathbf{W}$（“W”取自于组内“**w**ithin-class”之意）。
* 利用矩阵 $\mathbf{W}$ 的特征值分解，计算
  $\mathbf{M}^\* = \mathbf{M}\mathbf{W}^{-\frac{1}{2}}$。
* 计算 $\mathbf{M}^\*$ 的协方差矩阵 $\mathbf{B}^\*$（“B”取自于组间“**b**etween-class”之意），
  以及其特征值分解
  $\mathbf{B}^\* = \mathbf{V}^\* \mathbf{D}\_B {\mathbf{V}^\*}^T$。
  矩阵 $\mathbf{V}^\*$ 的列向量 $v^\*\_l$ 从左到右依次为最优子空间的坐标向量。

从中得到的第 l 个判别变量为 $Z\_l = v\_l^T X$，其中 $v\_l = \mathbf{W}^{-\frac{1}{2}}v\_l^\*$。

费雪从另一种方式得到了这个分解，而没有利用高斯分布的性质。
他从下面这个问题入手：

> 寻找相对于组内方差最大化组间方差的线性组合 $Z=a^TX$。

组间（between class）方差指的是类型均值 $Z$ 的方差，
组内（within class）方差指的是所有类型样本混合在一起的方差。
图 4.9 说明了这个最优化准则的合理性。
尽管两个中心点的连线方向上两个分布的均值距离最远（即最大化组间方差），
但由于其协方差的性质它们的分布在这个方向的投射上有比较大的重叠。
在考虑了组内协方差后，可得到使重叠区域最小的方向。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_09.png"
  title="**图4.9**：尽管在连接两个中心点的方向上两个中心点距离最远，但其协方差性质导致投影后有比较大的重叠区域（左图）。判别方向为对高斯分布数据最小化重叠区域的方向（右图）。"
>}}

$Z$ 的组间方差为 $a^T\mathbf{B}a$，组内方差为 $a^T\mathbf{B}a$，
其中的 $\mathbf{W}$ 和 $\mathbf{B}$ 分别如之前的定义。
两者的和为 $\mathbf{B} + \mathbf{W} = \\mathbf{T}$，
输入变量 $X$ 的忽略其分类的整体协方差函数。

因此费雪问题可视为对 *瑞利商（Rayleigh quotient）* 的最大化问题：

$$ \max\_{a} \frac{a^T \mathbf{B} a}{a^T \mathbf{W} a}
\tag{4.15}$$

或等价为：

$$ \max\_{a} a^T \mathbf{B} a
\text{ subject to } a^T \mathbf{W} a = 1
\tag{4.16}$$

这是一个广义的特征值求解问题，
$a$ 等于 $\mathbf{W}^{-1}\mathbf{B}$ 的第一个特征向量[^7]。
可证明（练习 4.1）最优解 $a\_1$ 与第一个特征向量 $v\_1$ 相等。
类似也可以得到第二个方向向量 $a\_2$，
为在 $\mathbf{W}$ 空间上与 $a\_1$ 正交的方向中使
$a\_2^T\mathbf{B}a\_2 / a\_2^T\mathbf{W}a\_2$ 最大的向量，
同样其解为 $a\_2 = v\_2$，以此类推。
向量 $a\_l$ 被称为 *判别坐标（discriminant coordinate）* ，
其与判别函数为不同的概念；
也被称为 *典型变量（canonical variate）* ，
因为也可通过对指示输出变量矩阵 $\mathbf{Y}$ 与 自变量矩阵 $\mathbf{X}$ 的典型相关分析推导出。
第 12.5 节会详细介绍。

本节的讨论大致可总结如下：

* 在同协方差矩阵的高斯分布假设下，分类问题可推导出线性的判定边界。
  分类时先将原始数据根据 $\mathbf{W}$ 球形化，
  再将球形空间上的点分类到与其最近（模除 $\log \pi\_k$）的中心点。
* 因为在分类时只需要对比这个点与各个类型中心点的相对距离，
  所以可以将这个点投射到类型中心点生成的子空间上进行对比。
* 这个子空间可以进一步降低维度，根据中心点的分散程度而选择最优的降维方向。
  这种维度分解与费雪提出的分解是等价的。

降维子空间的最初是作为数据降维（可视化）的工具，
但也可用于解决分类问题。
在上述的分类问题解决步骤中，
可将与类型中心点之间距离的比较限制在指定的子空间中。
可证明这实际上等同于
将类型中心点限制在输入变量空间 $\mathbb{R}^p$ 上的 $L$ 维度子空间的假设下的
高斯分类规则。
用最大似然估计拟合这个模型，再用贝叶斯公式计算后验概率，则可得到上述的分类规则（练习 4.8）。

高斯分类规则决定了在距离计算中对 $\log \pi_k$ 的修正因子。
图 4.9 显示了这个修正的合理性。
分类的误判率决定于两个密度函数的重叠区域大小。
当 $\pi_k$ 都相等时（即图中的情况），则最优的判定点为两个均值投影点的中间。
当 $\pi_k$ 不同时，将判定点偏向于少的类型会降低误判率。
类似于之前对两个类型的讨论，可以先用线性判别分析或其他方法得到线性的判别规则，
再选择最小化训练集的误判率的判定点。

以元音识别数据为例，可说明降维限制对分类问题的作用。
数据中有 11 个类型和 10 个输入变量，因此分类分析的空间有 10 个维度。
在这些维度组成的子空间序列上计算训练集和测试集的误判率，图 4.10 为结果曲线。
图 4.11 展示了在二维线性判别分析中的判定边界。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_10.png"
  title="**图4.10**：元音识别数据的训练集和测试集误差曲线，横轴为判别子空间的维度。此例中最优的测试集误差在维度为 2 时。图 4.11 为这个二维空间上的判定边界。"
>}}

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_11.png"
  title="**图4.11**：元音识别数据在前两个典型变量生成的二维子空间上的判定边界。注意在更高维的子空间上，判定边界为高于一维的仿射平面，无法用线来可视化。"
>}}

费雪降秩判别分析与指示输出向量的回归模型有紧密的联系。
线性判别分析可看作是回归之后对 $\hat{\mathbf{Y}}^T\mathbf{Y}$ 的特征值分解。
在两个类型的问题中，判别变量中存在一个与 $\hat{\mathbf{Y}}$ 的某一列向量完全成比例。
第 12 章会推导两者的关系。
另外，在回归生成的拟合 $\hat{\mathbf{Y}}$ 进行线性判别分析，
其结果与在原数据中的线性判别分析一致（练习 4.3）。

[^1]: 原文脚注 2：本图和此书中其他类似图中的判定边界，由穷举轮廓方法（exhaustive contouring method）计算得出：在密集的点阵上计算每个点的分类，然后用轮廓（contouring）算法来计算边界。
[^2]: 原文脚注 3：尽管在计算线性判别分析的判别函数时需要协方差矩阵的估计 $\hat{\mathbf{\Sigma}}$，但如果只为了估计计算判定边界所必须的 $O(p)$ 个参数，可以使用一个更简化的函数。
[^3]: STATLOG 是由欧盟在 90 年代资助的科研项目，目的在于审视当时领域中的各种分类方法，在多样的数据集中比较它们的分类表现，从而评估它们解决实际问题的能力。[UCI 机器学习资源库](http://archive.ics.uci.edu/ml/datasets/Statlog+project) 提供了这些数据集的说明和下载链接。
[^4]: 译者的理解是，元音识别的数据中，有很多类似的读音但对应了不同的元音分类，导致一部分输入空间上的点对应着不同的类型，导致训练集和测试集中相近的点却对应了不同的类型。
[^5]: 空间中的 $K$ 个点可组成 $K-1$ 维度的空间，例如 3 个点可形成一个平面（二维空间）。并且如果这 $K$ 个点中存在线性组合关系，则组成的子空间维度小于 $K-1$，例如 3 个在一条直线上的点，只能形成一维的子空间。
[^6]: 所以在比较距离大小时，是否包括这个点与子空间之间的距离不会影响其大小关系。
[^7]: 原文为“特征值”，但 $a$ 应为一个向量。