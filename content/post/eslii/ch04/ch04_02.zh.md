+++
title = "ESL-4.2 Linear Regression of an Indicator Matrix"
summary = """
统计学习基础（译注）第四章第二节，第 103-106 页。
"""

date = 2018-10-04T16:22:07+08:00
lastmod = 2018-10-04T16:22:07+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Here each of the response categories are coded via an indicator variable.
Thus if G has K classes, there will be K such indicators Y k , k = 1, . . . , K,
with Y k = 1 if G = k else 0. These are collected together in a vector
Y = (Y 1 , . . . , Y K ), and the N training instances of these form an N × K
indicator response matrix Y. Y is a matrix of 0’s and 1’s, with each row
having a single 1. We fit a linear regression model to each of the columns
of Y simultaneously, and the fit is given by

$$\hat{\mathbf{Y}} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\tag{4.3}$$

Chapter 3 has more details on linear regression. Note that we have a coeffi-
cient vector for each response column y k , and hence a (p+1)×K coefficient
matrix B̂ = (X T X) −1 X T Y. Here X is the model matrix with p+1 columns
corresponding to the p inputs, and a leading column of 1’s for the intercept.
A new observation with input x is classified as follows:

* compute the fitted output f ˆ (x) T = (1, x T ) B̂, a K vector;
* identify the largest component and classify accordingly:

$$\hat{G}(x) = \underset{k\in\mathcal{G}}{\text{argmax}} \hat{f}\_k(x)
\tag{4.4}$$

What is the rationale for this approach? One rather formal justification
is to view the regression as an estimate of conditional expectation. For the
random variable Y k , E(Y k |X = x) = Pr(G = k|X = x), so conditional
expectation of each of the Y k seems a sensible goal. The real issue is: how
good an approximation to conditional expectation is the rather rigid linear
regression model? Alternatively, are the f ˆ k (x) reasonable estimates of the
posterior probabilities Pr(G = k|X = x), and more importantly, does this
matter?

It is quite straightforward to verify that k∈G f ˆ k (x) = 1 for any x, as
long as there is an intercept in the model (column of 1’s in X). However,
the f ˆ k (x) can be negative or greater than 1, and typically some are. This
is a consequence of the rigid nature of linear regression, especially if we
make predictions outside the hull of the training data. These violations in
themselves do not guarantee that this approach will not work, and in fact
on many problems it gives similar results to more standard linear meth-
ods for classification. If we allow linear regression onto basis expansions
h(X) of the inputs, this approach can lead to consistent estimates of the
probabilities. As the size of the training set N grows bigger, we adaptively
include more basis elements so that linear regression onto these basis func-
tions approaches conditional expectation. We discuss such approaches in
Chapter 5.

A more simplistic viewpoint is to construct targets t k for each class,
where t k is the kth column of the K × K identity matrix. Our prediction
problem is to try and reproduce the appropriate target for an observation.
With the same coding as before, the response vector y i (ith row of Y) for
observation i has the value y i = t k if g i = k. We might then fit the linear
model by least squares:

$$\min\_{\mathbf{B}} \sum\_{i=1}^N \\| y\_i - [(1,x\_i^T)\mathbf{B}]^T \\|^2
\tag{4.5}$$

The criterion is a sum-of-squared Euclidean distances of the fitted vectors
from their targets. A new observation is classified by computing its fitted
vector f ˆ (x) and classifying to the closest target:

$$\hat{G}(x) =
\underset{k}{\text{argmin}} \\|\hat{f}(x) - t\_k\\|^2
\tag{4.6}$$

This is exactly the same as the previous approach:

* The sum-of-squared-norm criterion is exactly the criterion for multi-
ple response linear regression, just viewed slightly differently. Since
a squared norm is itself a sum of squares, the components decouple
and can be rearranged as a separate linear model for each element.
Note that this is only possible because there is nothing in the model
that binds the different responses together.
* The closest target classification rule (4.6) is easily seen to be exactly
the same as the maximum fitted component criterion (4.4).

There is a serious problem with the regression approach when the number
of classes K ≥ 3, especially prevalent when K is large. Because of the rigid
nature of the regression model, classes can be masked by others. Figure 4.2
illustrates an extreme situation when K = 3. The three classes are perfectly
separated by linear decision boundaries, yet linear regression misses the
middle class completely.

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_02.png"
  title="**图4.2**："
>}}
The data come from three classes in IR 2 and are easily separated
by linear decision boundaries. The right plot shows the boundaries found by linear
discriminant analysis. The left plot shows the boundaries found by linear regres-
sion of the indicator response variables. The middle class is completely masked
(never dominates).


{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_03.png"
  title="**图4.3**："
>}}
The effects of masking on linear regression in IR for a three-class
problem. The rug plot at the base indicates the positions and class membership of
each observation. The three curves in each panel are the fitted regressions to the
three-class indicator variables; for example, for the blue class, y blue is 1 for the
blue observations, and 0 for the green and orange. The fits are linear and quadratic
polynomials. Above each plot is the training error rate. The Bayes error rate is
0.025 for this problem, as is the LDA error rate.

In Figure 4.3 we have projected the data onto the line joining the three
centroids (there is no information in the orthogonal direction in this case),
and we have included and coded the three response variables Y 1 , Y 2 and
Y 3 . The three regression lines (left panel) are included, and we see that
the line corresponding to the middle class is horizontal and its fitted values
are never dominant! Thus, observations from class 2 are classified either
as class 1 or class 3. The right panel uses quadratic regression rather than
linear regression. For this simple example a quadratic rather than linear
fit (for the middle class at least) would solve the problem. However, it
can be seen that if there were four rather than three classes lined up like
this, a quadratic would not come down fast enough, and a cubic would
be needed as well. A loose but general rule is that if K ≥ 3 classes are
lined up, polynomial terms up to degree K − 1 might be needed to resolve
them. Note also that these are polynomials along the derived direction
passing through the centroids, which can have arbitrary orientation. So in
p-dimensional input space, one would need general polynomial terms and
cross-products of total degree K − 1, O(p K−1 ) terms in all, to resolve such
worst-case scenarios.

The example is extreme, but for large K and small p such maskings
naturally occur. As a more realistic illustration, Figure 4.4 is a projection
of the training data for a vowel recognition problem onto an informative
two-dimensional subspace. There are K = 11 classes in p = 10 dimensions.
This is a difficult classification problem, and the best methods achieve
around 40% errors on the test data. The main point here is summarized in
Table 4.1; linear regression has an error rate of 67%, while a close relative,
linear discriminant analysis, has an error rate of 56%. It seems that masking
has hurt in this case. While all the other methods in this chapter are based
on linear functions of x as well, they use them in such a way that avoids
this masking problem.