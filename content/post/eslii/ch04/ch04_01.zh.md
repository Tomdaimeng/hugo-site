+++
title = "ESL-4.1 引言"
summary = """
统计学习基础（译注）第四章第一节，第 101-56 页。
"""

date = 2018-09-25T00:22:07+08:00
lastmod = 2018-09-25T00:22:07+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In this chapter we revisit the classification problem and focus on linear
methods for classification. Since our predictor G(x) takes values in a dis-
crete set G, we can always divide the input space into a collection of regions
labeled according to the classification. We saw in Chapter 2 that the bound-
aries of these regions can be rough or smooth, depending on the prediction
function. For an important class of procedures, these decision boundaries
are linear; this is what we will mean by linear methods for classification.

There are several different ways in which linear decision boundaries can
be found. In Chapter 2 we fit linear regression models to the class indicator
variables, and classify to the largest fit. Suppose there are K classes, for
convenience labeled 1, 2, . . . , K, and the fitted linear model for the kth
indicator response variable is f ˆ k (x) = β̂ k0 + β̂ k T x. The decision boundary
between class k and l is that set of points for which f ˆ k (x) = f ˆ l (x), that is,
the set {x : ( β̂ k0 − β̂ l0 ) + ( β̂ k − β̂ l ) T x = 0}, an affine set or hyperplane. 1
Since the same is true for any pair of classes, the input space is divided
into regions of constant classification, with piecewise hyperplanar decision
boundaries. This regression approach is a member of a class of methods
that model discriminant functions δ k (x) for each class, and then classify x
to the class with the largest value for its discriminant function. Methods
that model the posterior probabilities Pr(G = k|X = x) are also in this
class. Clearly, if either the δ k (x) or Pr(G = k|X = x) are linear in x, then
the decision boundaries will be linear.

Actually, all we require is that some monotone transformation of δ k or
Pr(G = k|X = x) be linear for the decision boundaries to be linear. For
example, if there are two classes, a popular model for the posterior proba-
bilities is

$$\begin{align}
\text{Pr}(G=1|X=x) &=
\frac{\exp(\beta\_0 + \beta^T x)}
{1 + \exp(\beta\_0 + \beta^T x)} \\\\ \text{Pr}(G=2|X=x) &=
\frac{1}{1 + \exp(\beta\_0 + \beta^T x)}
\end{align}\tag{4.1}$$

Here the monotone transformation is the logit transformation: log[p/(1−p)],
and in fact we see that

$$\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=2|X=x)} =
\beta\_0 + \beta^T x
\tag{4.2}$$

The decision boundary is the set of  points for which the log-odds are zero,
and this is a hyperplane defined by x|β 0 + β T x = 0 . We discuss two very
popular but different methods that result in linear log-odds or logits: linear
discriminant analysis and linear logistic regression. Although they differ in
their derivation, the essential difference between them is in the way the
linear function is fit to the training data.

A more direct approach is to explicitly model the boundaries between
the classes as linear. For a two-class problem in a p-dimensional input
space, this amounts to modeling the decision boundary as a hyperplane—in
other words, a normal vector and a cut-point. We will look at two methods
that explicitly look for “separating hyperplanes.” The first is the well-
known perceptron model of Rosenblatt (1958), with an algorithm that finds
a separating hyperplane in the training data, if one exists. The second
method, due to Vapnik (1996), finds an optimally separating hyperplane if
one exists, else finds a hyperplane that minimizes some measure of overlap
in the training data. We treat the separable case here, and defer treatment
of the nonseparable case to Chapter 12.

While this entire chapter is devoted to linear decision boundaries, there is
considerable scope for generalization. For example, we can expand our vari-
able set X 1 , . . . , X p by including their squares and cross-products X 1 2 , X 2 2 , . . . ,
X 1 X 2 , . . ., thereby adding p(p + 1)/2 additional variables. Linear functions
in the augmented space map down to quadratic functions in the original
space—hence linear decision boundaries to quadratic decision boundaries.
Figure 4.1 illustrates the idea. The data are the same: the left plot uses
linear decision boundaries in the two-dimensional space shown, while the
right plot uses linear decision boundaries in the augmented five-dimensional
space described above. This approach can be used with any basis transfor-
mation h(X) where h : IR p 7→ IR q with q > p, and will be explored in later
chapters.

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_01.png"
  title="**图4.1**："
>}}