+++
title = "ESL-4.1 引言"
summary = """
统计学习基础（译注）第四章第一节，第 101-103 页。
从基本思路上介绍分类问题的线性方法，
分类模型可视为对输入变量空间的某种划分方式，
“线性”指的是划分边界呈线性，或某种变换之后呈线性。
"""

date = 2018-10-04T16:12:07+08:00
lastmod = 2018-10-04T16:12:07+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

本章回到分类问题，并集中讨论线性的分类方法。
在分类问题中，模型的预测 $G(x)$ 取值范围为一个离散的集合 $\mathcal{G}$。
所以模型的预测可以被视为一种对输入变量空间的区域划分并附上类型标签。
如第二章中的介绍，这些划分区域的边界，根据其具体的预测函数形式，可能是光滑的或粗糙的。
线性即为其中一种重要的函数类型，而所谓分类问题的线性方法，
所指的就是用线性函数来划分分类区域。

有很多种计算线性判别边界（decision boundary）的方法。
在第二章中提及了使用线性回归模型对各个类型的指示变量进行拟合的做法，
模型采纳拟合度最大的那个分类。
具体来说，假设有 $K$ 个类型，方便起见标记为 1，2，……，K，
对第 k 个指示输出变量拟合线性模型
$\hat{f}\_k(x) = \hat{\beta}\_{k0} + \hat{\beta}\_k^T x$。
类型 k 和类型 l 之间的判别边界，
是所有使得 $\hat{f}\_k(x) = \hat{f}\_l(x)$ 的点的集合，
即为集合 $\\{x: (\hat{\beta}\_{k0} - \hat{\beta}\_{l0}) + (\hat{\beta}\_k - \hat{\beta}\_l)^T x = 0\\}$，
一个仿射集（affine set）[^1]或超平面（hyperplane）[^2]。
对任意两个不同的类型，都有以上的结果，
因此输入变量空间被很多超平面的判别边界分成一段段的区域，
每个区域被赋予一个类型。
这属于用回归来进行分类的方法，
通常对每个类型生成 *判别函数（discriminant function）* $\delta\_k(x)$，
对输入变量 $x$ 的分类结果为对应判别函数最大的类型。
对后验概率 $\text{Pr}(G = k|X=x)$ 的建模的方法也是类似的方法。
显然，若 $\delta\_k(x)$ 或 $\text{Pr}(G = k|X=x)$ 为 $x$ 的线性函数，
则判别边界也呈线性。

实际只要 $\delta\_k$ 或 $\text{Pr}(G = k|X=x)$ 的经过某种单调性的转换后呈线性，
即可有线性的判别边界。
以两个类型为例，一个普遍使用的后验概率模型为：

$$\begin{align}
\text{Pr}(G=1|X=x) &=
\frac{\exp(\beta\_0 + \beta^T x)}
{1 + \exp(\beta\_0 + \beta^T x)} \\\\ \text{Pr}(G=2|X=x) &=
\frac{1}{1 + \exp(\beta\_0 + \beta^T x)}
\end{align}\tag{4.1}$$

其中的单调性转换为 *分对数（logit）* 转换：
$\log[p / (1-p)]$，
并且有：

$$\log\frac{\text{Pr}(G=1|X=x)}{\text{Pr}(G=2|X=x)} =
\beta\_0 + \beta^T x
\tag{4.2}$$

此时判别边界为所有使 *对数几率（log-odds）* 为零的点，
即超平面 $\\{x|\beta\_0+\beta^T x = 0\\}。
本章介绍两个很常见的生成线性对数几率或分对数的不同方法：
线性判别分析和线性逻辑回归。
这两种方法的推导过程不同，
其本质的区别在于线性函数对训练数据集拟合的方式。

另一个很直接的方法为明确地建立线性的边界来区分不同类型。
对于 p 维输入变量空间的二分类问题，
即为寻找作为判别边界的最佳的超平面；
换言之，要确定一个法线向量（normal vector）和临界点。
我们会介绍两个直接对“分界超平面”建模的方法。
第一个为 Rosenblatt (1958) 提出的
*感知器（perceptron）* 模型，
其算法利用训练数据集生成分界超平面。
另一个由 Vapnik (1996) 提出，
在可能的情况下计算最优的分解超平面，否则通过最小化训练集的分类交叉区域生成超平面。
在本章中我们只考虑可分类的场景，
第十二章会讨论对于无法分类的场景的处理。

虽然整章都在讨论线性的判别边界，但可推广为很大范围的模型。
例如，可以拓展到输入变量集合 $X\_1$、……、$X\_p$ 的平方项和交叉项
$X\_1^2$、$X\_2^2$、……、$X\_1X\_2$、……，
则额外增加了 $p(p+1)/2$ 个变量。
拓展后的输入空间的线性函数对应着原输入空间的二次函数，
因此拓展后的线性判别边界实际上是原空间的二次判别边界。
图 4.1 描绘了这种拓展方式。
左图和右图使用了相同的训练数据，
左图的分类模型为二维空间上的线性判别边界，
而右图为在拓展后的五维空间上的线性判别边界。
类似地，也可对输入变量空间利用基函数转换 $h(X)$ 进行拓展，
其中 $h:\mathbb{R}^p \mapsto \mathbb{R}^q$，并且 $q > p$，
会在后续章节介绍。

{{< figure src="http://public.guansong.wang/eslii/ch04/eslii_fig_04_01.png"
  title="**图4.1**：以三个类型的数据为例。左图为线性判别分析产生的线性判别边界；右图为二次判别边界，通过对五个输入变量 $X\_1$、$X\_2$、$X\_1X\_2$、$X\_1^2$、$X\_2^2$ 产生的线性判别边界。此空间上的线性判定比较相当于原空间上的二次判定比较。"
>}}

[^1]: 译者一直简单地将“affine”理解为“线性”。
[^2]: 原文脚注 1：严格来说，超平面要包含原点，而仿射集不需要。本书中通常忽略这个区别，默认为超平面。（通常会对变量进行中心化处理，所以可以合理地默认包含原点。）