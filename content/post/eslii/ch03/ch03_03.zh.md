+++
title = "ESL-3.3 变量子集选择"
summary = """
统计学习基础（译注）第三章第三节，第 57-61 页。
为了控制方差，在高维问题中通常在模型中只包含一小部分强有效的输入变量。
本节介绍了以线性模型为基础的几种分步式的变量选择方法。
"""

date = 2018-09-18T11:52:07+08:00
lastmod = 2018-09-18T11:52:07+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

最小二乘估计（等式 3.6）有两个不足之处：

- 预测准确度，最小二乘估计通常偏差小而方差大。
  用增加少许偏差的代价可能会减少预测的方差，比如通过收缩（shrinkage）或限制某些系数为 0，
  从而改进预测的准确度。
- 可解释性。
  比起大量的特征变量，我们更希望可以找到其中作用最强的一个小子集。
  去掉过于细枝末节的信息，可以使我们对模型有更清晰的认识。

本节介绍几个线性回归问题中的变量子集选择的方法。
稍后章节会继续介绍收缩方法、一些控制方差的混合方法、以及其他降维的策略。
这些都归属于 *模型选择（model selection）* 的范畴。
第七章会更深入介绍在广义场景下的模型选择的话题。

子集选择会保留所有输入变量中的一个子集，从模型中排除子集外的其他变量。
然后再使用最小二乘回归来估计保留的变量的系数。
子集的选择有很多种不同的方法。

### 3.3.1 最优子集选择

最优子集回归的基本做法是对不同的子集大小 $k\in\\{0,1,2,\dots,p\\}$，
寻找限制了变量个数后，可以产生最小的残差平方和（等式 3.2）的那组变量。

一个被称为 *跃进（leap and bounds）* 的高效算法（Furnival and Wilson，1974）
能够使得 30 至 40 个变量个数 p 的计算是可行的。
图 3.5 以前列腺癌症数据为例，显示了每个变量个数下的所有变量子集的表现。
所有点的下边界即为最优子集方法所得到的备选子集。
需要注意的是，最优子集方法并不要求大最优子集包含小最优子集，
例如个数为 2 的最优子集不需要包含个数为 1 的最优子集中的变量。
（但此例中所有最优子集都是嵌套的。）
最优子集曲线（图 3.5 中的红色下边界）必然是单调递减的，
因此无法通过曲线最低点来确定最终模型的子集大小 k。
对 k 的选择意味着对偏差和方差的权衡，
同时也有对简洁模型的主观偏好。
有多种可用来选择 k 的准则，
通常的方法是选择最小化期望预测误差的某种估计量的模型中最简单的一个。

{{< figure src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_05.png"
  title="**图3.5**：前列腺癌症数据中的所有变量子集，图中的点为每组子集的模型的残常平方和。"
>}}

本节后面介绍的几个方法都与最优子集方法类似，
通过训练数据集生成由单独的复杂度参数控制的一系列模型。
下一节将用使用交叉验证来估计预测误差并选择合适的 k，
AIC 是另外一个常用的选择 k 的准则。
第七章会集中地介绍类似的方法。

### 3.3.2 前向/后向逐步选择

从所有的变量子集中寻找最优，在 p 大于 40 后会在计算量上不可行。
与其从全部子集中穷举最优，我们可以遵循某种搜索路径。
*前向逐步选择（forward-stepwise selection）* 从截距项出发，
相继地加入剩余的输入变量中最大幅度提升模型拟合的那个。
在输入变量个数很大时，这个方法也同样面对着计算可行性的挑战；
然后，一些利用当前最小二乘拟合的 QR 分解的更新算法可以快速地计算下一步拟合（练习 3.9）。
和最优子集回归一样，前向逐步选择一系列以子集变量个数 k 为参数的模型，
而 k 也是待确定的。

前向逐步选择是一种 *贪心算法（greedy algorithm）*，生成了一系列嵌套的模型。
对模型的嵌套性的限制导致它相较于之前的最优子集选择来说是“次优”的方法。
但它有以下的优势：

- 计算性质：在 p 比较大时，最优子集的模型序列不容易计算，
  但前向逐步选择的模型序列却总是可以计算的（甚至当 p 大于 N 时）。
- 统计性质：最优子集方法会带来预测方差的增加；
  前向逐步选择的限制更强，与前者相比会有方差更小但可能偏差会增大。

*后向逐步选择（backward-stepwise selection）* 从完整的所有输入变量出发，
相继地去除模型中对拟合的影响最小的输入变量，
也就是拟合中 Z 分数最小的那个输入变量（练习 3.10）。
后向逐步选择数据集满足条件 $N>p$，然而前向逐步选择没有对 p 的要求。

图 3.6 展示了在模拟生成数据中的最优子集、前向和后向逐步选择的比较。
通常它们的表现比较相似。
图中也包括了下面将介绍的前向分段回归，它的误差收敛到最低水平的速度要慢一些。

在前列腺癌症的数据中，最优子集、前向和后向逐步选择都给出了完全一样的子集选择。

{{< figure src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_06.png"
  title="**图3.6**：模拟线性回归 $Y=X^T\beta+\varepsilon$ 中的四个子集选择方法的比较。样本量为 $N=300$，输入变量个数为 $p=31$，输入变量均服从标准高斯（正态）分布，两两之间的相关性均为 0.85。在数据生成模型中，选取其中 10 个输入变量为有效变量，其系数由分布 $\mathcal{N}(0, 0.4)$ 生成；其他输入参数的系数为 0。扰动项分布为 $\varepsilon\sim\mathcal{N}(0, 6.25)$，故输出变量中的信噪比为 0.64。这个过程模拟 50 次，将每次的拟合结果取平均。图中的点为每一步的估计系数 $\hat{\beta}(k)$ 与真实系数 $\beta$ 之间的均方误差。"
>}}

一些软件中会提供一种混合的逐步选择方法，
即在每一步同时考虑前向和后向的操作，从两者中选择最佳的方向。
例如 R 中的 `step` 函数会根据 AIC 准则（会对模型中加入过多变量有惩罚）来选择方向，
在每一步会选择加入某个变量或排除某个变量来最小化 AIC 的值。

也有其他一些传统的软件会根据 F 统计量进行变量选择，
即加入“显著”的变量，排除“不显著”的变量。
由于这种方法没有考虑到多项检验的问题，所以通常不再使用。
另外，在选择模型后经常会考察该模型的估计值摘要，类似于表 3.2，
然后需要指出这里的标准误差没有反应出变量选择的过程，在检验中是无效的。
第 8.2 节介绍的自助采样法可用来解决这个问题。

有时输入变量会成组地出现，比如一个多级的分类变量产生的多个哑变量。
实现逐步选择的软件，比如 R 中的 `step` 函数，会智能地选择或排除整组变量，
并相应地正确计算模型的自由度。

### 3.3.3 前向分段回归

*前向分段回归（Forward-stagewise regression）* 比前向逐步回归的限制更强。
与逐步回归类似，它从一个截距项开始，即 $\bar{y}$，其他所有输入变量的系数均设为 0。
在每个阶段，寻找所有的输入变量中与上一阶段产生的残差最相关的变量。
之后进行残差对这个变量进行简单（单变量）线性回归，
将估计的系数与上阶段中的该变量的系数（初始化为 0）相加。
重复这个过程，直到产生的残差与任何输入变量都不存在相关性，
这也是当 $N>p$ 时最小二乘拟合结果的一个必要条件。

与前向逐步回归不同的是，在每个阶段中只有一个系数会被调整，其他 $p-1$ 个不会改变。
其造成的结果是前向分段要得到最小二乘拟合结果需要大于 $p$ 个阶段，
也因此在之前的统计学中被认为是一种低效率的方法。
然而在今天看来，这种“慢拟合”的过程在高维问题中展现出了它的优势。
在第 3.8.1 节会说明在非常高维度的问题中，前向分段以及它的一个收敛更慢的变体是很有竞争力的选择。

图 3.6 中包括了前向分段回归的表现。
在这个例子中，前向分段在 1000 个阶段后，所有的输入向量和残差的相关性低于 $10^{-4}$。
图中的 k 对应的点，为最后一个有 k 个非零系数的阶段的估计系数与真实值的均方误差。
最终其均方误差收敛到最优子集的水平，但与逐步的方法比要经过更多的 k。

### 3.3.4 前列腺癌症数据（续）

表 3.3 为一些不同的变量子集选择和收缩方法的估计系数。
其中 LS 为最小二乘法，Best Subset 为最优子集选择，
Ridge 为岭回归，Lasso 为最小绝对值收缩选择算子，
PCR 为主成份回归，PLS 为偏最小二乘回归。
这些方法都存在某种复杂度参数，并根据最小化从 10 次交叉验证估计的预测误差来确定这个参数，
更详细的说明见第 7.10 节。
简单来说，交叉验证将训练集随机分为 10 个相等（并互斥）的组，
对每个模型和不同的复杂度，在其中 9 组数据组成的集合上进行拟合，在剩下的 1 组数据上计算预测误差；
将这 10 组轮流作为那一组测试集，则最终得到 10 个预测误差，取平均后即为交叉验证所估计的预测误差。
这个流程后会得到不同模型的复杂度参数与估计的预测误差之间的曲线关系。

| Term       | LS     | Best Subset | Ridge  |Lasso | PCR    | PLS    |
|:-----------|--------|-------------|--------|------|--------|--------|
| Intercept  | 2.465  | 2.477       | 2.452  |2.468 | 2.497  | 2.452  |
| lcavol     | 0.680  | 0.740       | 0.420  |0.533 | 0.543  | 0.419  |
| lweight    | 0.263  | 0.316       | 0.238  |0.169 | 0.289  | 0.344  |
| age        | −0.141 |             | −0.046 |      | −0.152 | −0.026 |
| lbph       | 0.210  |             | 0.162  |0.002 | 0.214  | 0.220  |
| svi        | 0.305  |             | 0.227  |0.094 | 0.315  | 0.243  |
| lcp        | −0.288 |             | 0.000  |      | −0.051 | 0.079  |
| gleason    | −0.021 |             | 0.040  |      | 0.232  | 0.011  |
| pgg45      | 0.267  |             | 0.133  |      | −0.056 | 0.084  |
|____________|________|_____________|________|______|________|________|
| Test Error | 0.521  | 0.492       | 0.492  |0.479 | 0.449  | 0.528  |
| Std Error  | 0.179  | 0.143       | 0.165  |0.164 | 0.105  | 0.152  |
**表3.3**：各种子集选择和收缩方法在前列腺癌症数据集上的估计系数和测试集误差结果。
空白项意味这该变量被模型排除。

需要注意的是原数据集已经被分为了 67 个样本的训练集和 30 个样本的测试集。
交叉验证只使用训练集中的数据，选择模型复杂度或收缩参数仍属于模型训练的一部分。
测试集仍然可以用来比较不同模型之间的表现。

图 3.7 为估计的预测误差（与模型复杂度）曲线。
其中大多在曲线的最低值附近比较平缓。
图中的点为从 10 次交叉验证中得到的对应模型和复杂度下的估计误差率，以及其 10 个结果的标准误差。
这里我们使用了“一个标准误差”规则，即在最低错误率的一个标准误差内，选择最简洁的模型。
此规则考虑到了这个曲线是从交叉验证中估计所得，可能存在偏差，
也因此属于比较保守的方法。

{{< figure src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_07.png"
  title="**图3.7**：各种子集选择和收缩方法的估计预测误差的曲线和标准误差。每个曲线函数的输入参数为该方法的相应复杂度参数。从横坐标左边到右边对应着模型的复杂度由低到高。预测误差的估计值和标准误差来自于 10 次交叉验证，在第 7.10 节会更深入介绍。每个模型中，最终选择的为在曲线最低点一个标准差范围内最低的复杂度，在每个图中为紫色的虚线交叉点。"
>}}

最优子集选择除了截距外只使用了两个输入变量，lcvol 和 lweight。
表中最后两行为交叉验证产生的 10 个模型拟合在测试集上的平均预测误差和其标准误差。