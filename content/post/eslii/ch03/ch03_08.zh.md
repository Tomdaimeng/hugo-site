+++
title = "ESL-3.8 More on the Lasso and Related Path Algorithms"
summary = """
统计学习基础（译注）第三章第八节，第 86-93 页。
"""

date = 2018-09-24T23:32:07+08:00
lastmod = 2018-09-24T23:32:07+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Since the publication of the LAR algorithm (Efron et al., 2004) there has
been a lot of activity in developing algorithms for fitting regularization
paths for a variety of different problems. In addition, L 1 regularization has
taken on a life of its own, leading to the development of the field compressed
sensing in the signal-processing literature. (Donoho, 2006a; Candes, 2006).
In this section we discuss some related proposals and other path algorithms,
starting off with a precursor to the LAR algorithm.

### 3.8.1 增量前向分段回归
### 3.8.1 Incremental Forward Stagewise Regression

Here we present another LAR-like algorithm, this time focused on forward
stagewise regression. Interestingly, efforts to understand a flexible nonlinear
regression procedure (boosting) led to a new algorithm for linear models
(LAR). In reading the first edition of this book and the forward stagewise

> ### Algorithm 3.4 Incremental Forward Stagewise Regression—FS
> 1. Start with the residual r equal to y and β 1 , β 2 , . . . , β p = 0.
>   All the predictors are standardized to have mean zero and unit norm.
> 2. Find the predictor x j most correlated with r
> 3. Update β j ← β j + δ j , where δ j = ǫ · sign[hx j , ri] and ǫ > 0 is a small
>   step size, and set r ← r − δ j x j .
> 4. Repeat steps 2 and 3 many times, until the residuals are uncorrelated
>   with all the predictors.

Algorithm 16.1 of Chapter 16 4 , our colleague Brad Efron realized that with
linear models, one could explicitly construct the piecewise-linear lasso paths
of Figure 3.10. This led him to propose the LAR procedure of Section 3.4.4,
as well as the incremental version of forward-stagewise regression presented
here.

Consider the linear-regression version of the forward-stagewise boosting
algorithm 16.1 proposed in Section 16.1 (page 608). It generates a coefficient
profile by repeatedly updating (by a small amount ǫ) the coefficient of the
variable most correlated with the current residuals. Algorithm 3.4 gives
the details. Figure 3.19 (left panel) shows the progress of the algorithm on
the prostate data with step size ǫ = 0.01. If δ j = hx j , ri (the least-squares
coefficient of the residual on jth predictor), then this is exactly the usual
forward stagewise procedure (FS) outlined in Section 3.3.3.

{{< figure src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_19.png"
  title="**图3.19**："
>}}

Here we are mainly interested in small values of ǫ. Letting ǫ → 0 gives
the right panel of Figure 3.19, which in this case is identical to the lasso
path in Figure 3.10. We call this limiting procedure infinitesimal forward
stagewise regression or FS 0 . This procedure plays an important role in
non-linear, adaptive methods like boosting (Chapters 10 and 16) and is the
version of incremental forward stagewise regression that is most amenable
to theoretical analysis. Bühlmann and Hothorn (2007) refer to the same
procedure as “L2boost”, because of its connections to boosting.

Efron originally thought that the LAR Algorithm 3.2 was an implemen-
tation of FS 0 , allowing each tied predictor a chance to update their coeffi-
cients in a balanced way, while remaining tied in correlation. However, he
then realized that the LAR least-squares fit amongst the tied predictors
can result in coefficients moving in the opposite direction to their correla-
tion, which cannot happen in Algorithm 3.4. The following modification of
the LAR algorithm implements FS 0 :

> ### Algorithm 3.2b Least Angle Regression: FS 0 Modification.
> 4. Find the new direction by solving the constrained least squares problem
>   min ||r − X A b|| 22 subject to b j s j ≥ 0, j ∈ A,
>   where s j is the sign of hx j , ri.

The modification amounts to a non-negative least squares fit, keeping the
signs of the coefficients the same as those of the correlations. One can show
that this achieves the optimal balancing of infinitesimal “update turns”
for the variables tied for maximal correlation (Hastie et al., 2007). Like
lasso, the entire FS 0 path can be computed very efficiently via the LAR
algorithm.

As a consequence of these results, if the LAR profiles are monotone non-
increasing or non-decreasing, as they are in Figure 3.19, then all three
methods—LAR, lasso, and FS 0 —give identical profiles. If the profiles are
not monotone but do not cross the zero axis, then LAR and lasso are
identical.

Since FS 0 is different from the lasso, it is natural to ask if it optimizes
a criterion. The answer is more complex than for lasso; the FS 0 coefficient
profile is the solution to a differential equation. While the lasso makes op-
timal progress in terms of reducing the residual sum-of-squares per unit
increase in L 1 -norm of the coefficient vector β, FS 0 is optimal per unit
increase in L 1 arc-length traveled along the coefficient path. Hence its co-
efficient path is discouraged from changing directions too often.

FS 0 is more constrained than lasso, and in fact can be viewed as a mono-
tone version of the lasso; see Figure 16.3 on page 614 for a dramatic exam-
ple. FS 0 may be useful in p ≫ N situations, where its coefficient profiles
are much smoother and hence have less variance than those of lasso. More
details on FS 0 are given in Section 16.2.3 and Hastie et al. (2007). Fig-
ure 3.16 includes FS 0 where its performance is very similar to that of the
lasso.

### 3.8.2 Piecewise-Linear Path Algorithms

The least angle regression procedure exploits the piecewise linear nature of
the lasso solution paths. It has led to similar “path algorithms” for other
regularized problems. Suppose we solve

$$\hat{\beta}(\lambda) = \underset{\beta}{\text{argmin}}
[R(\beta) + \lambda J(\beta)] \tag{3.76}$$

with

$$R(\beta) = \sum\_{i=1}^N L(y\_i, \beta\_0 + \sum\_{j=1}^p x\_{ij}\beta\_j)
\tag{3.77}$$

where both the loss function L and the penalty function J are convex.
Then the following are sufficient conditions for the solution path β̂(λ) to
be piecewise linear (Rosset and Zhu, 2007):

1. R is quadratic or piecewise-quadratic as a function of β, and
2. J is piecewise linear in β.

This also implies (in principle) that the solution path can be efficiently
computed. Examples include squared- and absolute-error loss, “Huberized”
losses, and the L 1 , L ∞ penalties on β. Another example is the “hinge loss”
function used in the support vector machine. There the loss is piecewise
linear, and the penalty is quadratic. Interestingly, this leads to a piecewise-
linear path algorithm in the dual space; more details are given in Sec-
tion 12.3.5.

### 3.8.3 The Dantzig Selector

Candes and Tao (2007) proposed the following criterion:

$$\min\_{\beta}\\|\beta\\|\_1, \text{subject to }
\\|\mathbf{X}^T(\mathbf{y} - \mathbf{X}\beta)\\|\_\infty \leq s
\tag{3.78}$$

They call the solution the Dantzig selector (DS). It can be written equiva-
lently as

$$\min\_{\beta} \\|\mathbf{X}^T(\mathbf{y} - \mathbf{X}\beta)\\|\_\infty, \text{subject to }
\\|\beta\\|\_1 \leq t
\tag{3.79}$$

Here || · || ∞ denotes the L ∞ norm, the maximum absolute value of the
components of the vector. In this form it resembles the lasso, replacing
squared error loss by the maximum absolute value of its gradient. Note
that as t gets large, both procedures yield the least squares solution if
N < p. If p ≥ N , they both yield the least squares solution with minimum
L 1 norm. However for smaller values of t, the DS procedure produces a
different path of solutions than the lasso.

Candes and Tao (2007) show that the solution to DS is a linear pro-
gramming problem; hence the name Dantzig selector, in honor of the late
George Dantzig, the inventor of the simplex method for linear program-
ming. They also prove a number of interesting mathematical properties for
the method, related to its ability to recover an underlying sparse coeffi-
cient vector. These same properties also hold for the lasso, as shown later
by Bickel et al. (2008).

Unfortunately the operating properties of the DS method are somewhat
unsatisfactory. The method seems similar in spirit to the lasso, especially
when we look at the lasso’s stationary conditions (3.58). Like the LAR al-
gorithm, the lasso maintains the same inner product (and correlation) with
the current residual for all variables in the active set, and moves their co-
efficients to optimally decrease the residual sum of squares. In the process,
this common correlation is decreased monotonically (Exercise 3.23), and at
all times this correlation is larger than that for non-active variables. The
Dantzig selector instead tries to minimize the maximum inner product of
the current residual with all the predictors. Hence it can achieve a smaller
maximum than the lasso, but in the process a curious phenomenon can
occur. If the size of the active set is m, there will be m variables tied with
maximum correlation. However, these need not coincide with the active set!
Hence it can include a variable in the model that has smaller correlation
with the current residual than some of the excluded variables (Efron et
al., 2007). This seems unreasonable and may be responsible for its some-
times inferior prediction accuracy. Efron et al. (2007) also show that DS
can yield extremely erratic coefficient paths as the regularization parameter
s is varied.

### 3.8.4 The Grouped Lasso

In some problems, the predictors belong to pre-defined groups; for example
genes that belong to the same biological pathway, or collections of indicator
(dummy) variables for representing the levels of a categorical predictor. In
this situation it may be desirable to shrink and select the members of a
group together. The grouped lasso is one way to achieve this. Suppose that
the p predictors are divided into L groups, with p l the number in group
l. For ease of notation, we use a matrix X l to represent the predictors
corresponding to the lth group, with corresponding coefficient vector β l .
The grouped-lasso minimizes the convex criterion

$$\min\_{\beta\in\mathbb{R}^p} \left (
\\|\mathbf{y} - \beta\_0\mathbf{1} - \sum\_{l=1}^L\mathbf{X}\_l\beta\_l\\|\_2^2 +
\lambda \sum\_{l=1}^L \sqrt{p\_l}\\|\beta\\|\_2
\right )\tag{3.80}$$

where the p l terms accounts for the varying group sizes, and || · || 2 is
the Euclidean norm (not squared). Since the Euclidean norm of a vector
β l is zero only if all of its components are zero, this procedure encourages
sparsity at both the group and individual levels. That is, for some values of
λ, an entire group of predictors may drop out of the model. This procedure
was proposed by Bakin (1999) and Lin and Zhang (2006), and studied and
generalized by Yuan and Lin (2007). Generalizations include more general
L 2 norms ||η|| K = (η T Kη) 1/2 , as well as allowing overlapping groups of
predictors (Zhao et al., 2008). There are also connections to methods for
fitting sparse additive models (Lin and Zhang, 2006; Ravikumar et al.,
2008).

### 3.8.5 Further Properties of the Lasso

A number of authors have studied the ability of the lasso and related pro-
cedures to recover the correct model, as N and p grow. Examples of this
work include Knight and Fu (2000), Greenshtein and Ritov (2004), Tropp
(2004), Donoho (2006b), Meinshausen (2007), Meinshausen and Bühlmann
(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006), and Bunea
et al. (2007). For example Donoho (2006b) focuses on the p > N case and
considers the lasso solution as the bound t gets large. In the limit this gives
the solution with minimum L 1 norm among all models with zero training
error. He shows that under certain assumptions on the model matrix X, if
the true model is sparse, this solution identifies the correct predictors with
high probability.

Many of the results in this area assume a condition on the model matrix
of the form

$$\max\_{j\in\mathcal{S}^c}
\\|\mathbf{x}\_j^T\mathbf{X}\_\mathcal{S}(\mathbf{X}\_\mathcal{S}^T\mathbf{X}\_\mathcal{S})^{-1}\\|\_1 \leq
(1-\epsilon), \text{for some } \epsilon\in (0,1]
\tag{3.81}$$

Here S indexes the subset of features with non-zero coefficients in the true
underlying model, and X S are the columns of X corresponding to those
features. Similarly S c are the features with true coefficients equal to zero,
and X S c the corresponding columns. This says that the least squares coef-
ficients for the columns of X S c on X S are not too large, that is, the “good”
variables S are not too highly correlated with the nuisance variables S c .

Regarding the coefficients themselves, the lasso shrinkage causes the esti-
mates of the non-zero coefficients to be biased towards zero, and in general
they are not consistent 5 . One approach for reducing this bias is to run
the lasso to identify the set of non-zero coefficients, and then fit an un-
restricted linear model to the selected set of features. This is not always
feasible, if the selected set is large. Alternatively, one can use the lasso to
select the set of non-zero predictors, and then apply the lasso again, but
using only the selected predictors from the first step. This is known as the
relaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to
estimate the initial penalty parameter for the lasso, and then again for a
second penalty parameter applied to the selected set of predictors. Since
the variables in the second step have less “competition” from noise vari-
ables, cross-validation will tend to pick a smaller value for λ, and hence
their coefficients will be shrunken less than those in the initial estimate.

Alternatively, one can modify the lasso penalty function so that larger co-
efficients are shrunken less severely; the smoothly clipped absolute deviation
(SCAD) penalty of Fan and Li (2005) replaces λ|β| by J a (β, λ), where

$$\frac{d J\_a(\beta, \lambda)}{d \beta{}} = \lambda \cdot \text{sign}(\beta)
\left [ I(\|\beta\| \leq \lambda) +
\frac{(a\lambda - \|\beta\|)\_+}{(a-1)\lambda} I(\|\beta\| > \lambda)
 \right ] \tag{3.82}$$

for some a ≥ 2. The second term in square-braces reduces the amount of
shrinkage in the lasso for larger values of β, with ultimately no shrinkage
as a → ∞. Figure 3.20 shows the SCAD penalty, along with the lasso and
|β| 1−ν . However this criterion is non-convex, which is a drawback since it
makes the computation much more difficult. The adaptive lasso (Zou, 2006)
uses a weighted penalty of the form j=1 w j |β j | where w j = 1/| β̂ j | ν , β̂ j is
the ordinary least squares estimate and ν > 0. This is a practical approxi-
mation to the |β| q penalties (q = 1 − ν here) discussed in Section 3.4.3. The
adaptive lasso yields consistent estimates of the parameters while retaining
the attractive convexity property of the lasso.

{{< figure src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_20.png"
  title="**图3.20**："
>}}

### 3.8.6 Pathwise Coordinate Optimization

An alternate approach to the LARS algorithm for computing the lasso
solution is simple coordinate descent. This idea was proposed by Fu (1998)
and Daubechies et al. (2004), and later studied and generalized by Friedman
et al. (2007), Wu and Lange (2008) and others. The idea is to fix the penalty
parameter λ in the Lagrangian form (3.52) and optimize successively over
each parameter, holding the other parameters fixed at their current values.

Suppose the predictors are all standardized to have mean zero and unit
norm. Denote by β̃ k (λ) the current estimate for β k at penalty parameter
λ. We can rearrange (3.52) to isolate β j ,

$$\begin{align}
R(\tilde{\beta}(\lambda), \beta\_j) =&
\frac{1}{2} \sum\_{i=1}^N \left ( y\_i -
\sum\_{k \ne j}x\_{ik}\tilde{\beta}\_k(\lambda) -
x\_{ij}\beta\_j \right )^2 + \\\\ &\lambda
\sum\_{k \ne j} \|\tilde{\beta}\_k(\lambda) +
\lambda \|\beta\_j\|
\end{align}\tag{3.83}$$

where we have suppressed the intercept and introduced a factor 12 for con-
venience. This can be viewed as a univariate lasso problem with response
variable the partial residual y i − ỹ i = y i − k6 = j x ik β̃ k (λ). This has an
explicit solution, resulting in the update

$$\tilde{\beta}\_j(\lambda) \leftarrow S \left (
\sum\_{i=1}^N x\_{ij}(y\_i - \tilde{y}\_i^{(j)}), \lambda \right )
\tag{3.84}$$

Here S(t, λ) = sign(t)(|t|−λ) + is the soft-thresholding operator in Table 3.4
on page 71. The first argument to S(·) is the simple least-squares coefficient
of the partial residual on the standardized variable x ij . Repeated iteration
of (3.84)—cycling through each variable in turn until convergence—yields
the lasso estimate β̂(λ).

We can also use this simple algorithm to efficiently compute the lasso
solutions at a grid of values of λ. We start with the smallest value λ max
for which β̂(λ max ) = 0, decrease it a little and cycle through the variables
until convergence. Then λ is decreased again and the process is repeated,
using the previous solution as a “warm start” for the new value of λ. This
can be faster than the LARS algorithm, especially in large problems. A
key to its speed is the fact that the quantities in (3.84) can be updated
quickly as j varies, and often the update is to leave β̃ j = 0. On the other
hand, it delivers solutions over a grid of λ values, rather than the entire
solution path. The same kind of algorithm can be applied to the elastic
net, the grouped lasso and many other models in which the penalty is a
sum of functions of the individual parameters (Friedman et al., 2010). It
can also be applied, with some substantial modifications, to the fused lasso
(Section 18.4.2); details are in Friedman et al. (2007).