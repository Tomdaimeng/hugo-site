+++
title = "ESL-3.2 线性回归模型和最小二乘"
summary = """
统计学习基础（译注）第三章第二节，第 44-56 页。
系统介绍线性回归模型的最小二乘解及其在向量空间上的含义，
在附加的分布假设下，可以建立假设检验和置信区。
高斯-马尔可夫定理说明了最小二乘法在无偏估计中的最优性，
但也同时引入了估计量的偏差和方差权衡问题。
对输入变量空间的正交化，为最小二乘法的计算和直观理解提供了新的角度。
"""

date = 2018-09-14T10:13:07+08:00
lastmod = 2018-11-27T01:43:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文", "2018"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

如第二章所介绍，我们有一个输入变量向量
$X^T = (X\_1, X\_2, \dots, X\_p)$，
目的是预测一个取值为实数的输出变量 $Y$。
线性模型的表达式为：

$$f(X) = \beta\_0 + \sum\_{j=1}^p X\_j\beta\_j
\tag{3.1}$$

线性模型假设回归函数 $E(Y|X)$ 是线性的，或者至少足够近似于线性的。
式中 $\beta\_j$ 是未知的参数或系数，
输入变量 $X\_j$ 可以有多种类型：

- 量化的输入变量；
- 量化输入变量的某种变换，比如取对数、平方根、平方；
- 构建多项式表达式的基函数展开，
  比如 $X\_2 = X\_1^2$，$X\_3 = X\_1^3$。
- 分类输入变量的哑变量或数值编码。
  例如，对一个有五种类别的分类变量 $G$，
  可构建五个变量 $X\_j, j=1,\dots,5$，满足 $X\_j = I(G=j)$。
  其中只能有一个为 1，其他都为 0，
  因此与一组依赖于类别的常数一起，
  这组变量可完全重现 $G$ 变量的影响，
  即 $\sum\_{j=1}^5 X\_j\beta\_j$。
- 输入变量之间的交叉，例如 $X\_3 = X\_1 \cdot X\_2$。

不管 $X\_j$ 的形式如何，模型对参数是线性的。

我们通常有一组用来估计参数 $\beta$ 的训练数据
$(x\_1,y\_1),\dots,(x\_N,y\_N)$。
每个 $x\_i = (x\_{i1},x\_{i2},\dots,x\_{ip})^T$
是第 i 个样本的输入变量取值的向量。
最常见的估计方法是**最小二乘**（least squares），
通过对最小化残差平方和（residual sum of squares）
来确定系数 $\beta=(\beta\_0,\beta\_1,\dots,\beta\_p)^p$：

$$\begin{align}
\text{RSS}(\beta) &= \sum\_{i=1}^N (y\_i-f(x\_i))^2 \\\\ &=
\sum\_{i=1}^N (y\_i - \beta\_0 - \sum\_{j=1}^p x\_{ij}\beta\_j)^2
\tag{3.2}\end{align}$$

从统计学的角度看，
如果训练样本 $(x\_i,y\_i)$ 是从样本空间中独立随机采样的，
那么这个目标函数的选择是合理的。
既使 $x\_i$ 并不是完全随机采样的，
只要输出变量 $y\_i$ 在条件于输入变量 $x\_i$ 下相互独立，
这仍是个合理的目标函数。

{{< figure
  src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_01.png"
  title="**图3.1**：$X \in \mathbb{R}^2$ 空间上的最小二乘拟合，寻找最小化与 $Y$ 的残差的平方和的 $X$ 的线性函数。"
>}}

图 3.1 在训练样本点 $(X,Y)$ 所在的 $\mathbb{R}^{p+1}$ 空间上，
演示了最小二乘法拟合的几何含义。
注意等式 3.2 没有任何关于模型 3.1 正确与否的假设，
只是寻找训练样本的最佳线性拟合。
不管真实的数据是怎样的，最小二乘的拟合从直觉上也是合理的；
目标函数 RSS 衡量了（拟合线与样本的）平均拟合不足程度。

那么如何最小化等式 3.2？
记 $\mathbf{X}$ 为代表训练样本的 $N \times (p+1)$ 的矩阵，
其每行对应着一个输入变量向量（向量第一个元素为常数 1），
记 $\mathbf{y}$ 为代表训练样本输出变量的长度为 $N$ 的向量。
则残差平方和可写为：

$$\text{RSS}(\beta) =
(\mathbf{y} - \mathbf{X}\beta)^T(\mathbf{y} - \mathbf{X}\beta)
\tag{3.3}$$

这是 $p+1$ 个参数上的二次函数。
对 $\beta$ 求导：

$$\begin{align}
\frac{\partial \text{RSS}}{\partial\beta} &=
-2\mathbf{X}^T(\mathbf{y} - \mathbf{X}\beta)
\\\\ \frac{\partial^2 \text{RSS}}{\partial\beta\partial\beta^T} &=
2\mathbf{X}^T\mathbf{X}
\end{align}\tag{3.4}$$

（暂时）先假设 $\mathbf{X}$ 为列满秩矩阵，
因此 $\mathbf{X}^T\mathbf{X}$ 是正定矩阵[^1]，
将一阶导数设置为 0：

$$\mathbf{X}^T(\mathbf{y} - \mathbf{X}\beta) = 0 \tag{3.5}$$

可得到唯一解：

$$\hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\tag{3.6}$$

在输入变量向量 $x\_0$ 点的预测值为
$\hat{f}(x\_0) = (1, x\_0)^T \hat{\beta}$；
训练样本的拟合值为：

$$\hat{y} = \mathbf{X}\hat{\beta} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\tag{3.7}$$

或者 $\hat{y\_i} = \hat{f}(x\_i)$。
等式 3.7 中，
$\mathbf{y}$ 与矩阵
$\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$
相乘后就戴上了帽子 $\hat{\mathbf{y}}$，
所以这个矩阵也称为“帽子”矩阵。

{{< figure src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_02.png"
  title="**图3.2**：两个输入变量的最小二乘法在样本 $N$ 维空间上的几何含义。输出变量样本向量 $\mathbf{y}$ 在输出变量样本向量 $\mathbf{x\_1}$ 和 $\mathbf{x\_1}$ 生成的超平面上的投影 $\hat{\mathbf{y}}$ 即为最小二乘法的拟合值的样本向量。"
>}}

图 3.2 在 $\mathbb{R}^N$ 空间上用另一个方式展示最小二乘估计的几何表现。
将 $\mathbf{X}$ 的列向量记为
$\mathbf{x}\_0,\mathbf{x}\_1,\dots,\mathbf{x}\_p$，
其中 $\mathbf{x\_0} \equiv 1$。
在下面的推导中，第一个常数向量与其他变量的处理方式相同。
这些向量会生成一个 $\mathbb{R}^N$ 上的子空间，
也被称为 $\mathbf{X}$ 的列空间。
最小化
$\text{RSS}(\beta) = \\|\mathbf{y} - \mathbf{X}\beta\\|^2$
的过程，等价于寻找使残差向量
$\mathbf{y} - \hat{\mathbf{y}}$ 与这个列空间正交[^2]
的 $\hat{\beta}$。
等式 3.5 体现了这个正交的关系，
因此拟合的结果 $\hat{\mathbf{y}}$ 是 $\mathbf{y}$ 在这个列空间上的
**正交投影**（orthogonal projection）。
帽子矩阵 $\mathbf{H}$ 完成了正交投影的计算，因此也被称为投影矩阵。

$\mathbf{X}$ 的列向量有可能不是线性独立，
那么 $\mathbf{X}$ 便不是列满秩矩阵。
两个完全相关的输入变量（比如 $\mathbf{x}\_2 = 3\mathbf{x}\_1$）
就是一个例子。
这时 $\mathbf{X}^T\mathbf{X}$ 是个奇异矩阵，
最小二乘无法得到唯一解 $\hat{\beta}$。
然而拟合值 $\hat{\mathbf{y}} = \mathbf{X}\hat{\beta}$
仍然是 $\mathbf{y}$ 在 $\mathbf{X}$ 列空间上的正交投影。
只是这时有不止一种 $\mathbf{X}$ 列向量的组合方式可以表达这个投影。
出现非满秩输入变量矩阵的一个最常见原因是
一个或多个分类变量的编码方式出现了冗余。
因此对无唯一解的自然解决方案是重新编码或者去掉 $\mathbf{X}$ 中冗余的列。
大多数回归软件程序会检测 $\mathbf{X}$ 中的冗余列，
并自动地采用某些去除这些冗余的策略。
另一个可造成非满秩的原因是输入变量个数 $p$ 大于训练样本个数 $N$，
这在信号和图像识别领域常出现。
通常解决方法是过滤掉一些输入变量，
或者用正则化来控制拟合（见第 5.2.3 节和第十八章）。

上述讨论没有对数据的真实概率分布做太多假设。
然而为了进一推导 $\hat{\beta}$ 的性质，
现假设输出变量样本 $y\_i$ 互不相关，有固定的方差 $\sigma^2$，
并且将 $x\_i$ 视为非随机的常量。
从等式 3.6 可推导出最小二乘参数估计量的协方差矩阵为：

$$\text{Var}(\hat{\beta}) = (\mathbf{X}^T\mathbf{X})^{-1} \sigma^2 \tag{3.8}$$

通常用下式来估计方差 $\sigma^2$：

$$\hat{\sigma}^2 = \frac{1}{N-p-1} \sum\_{i=1}^N (y\_i-\hat{y\_i})^2$$

注意其中的除数为 $N-p-1$ 而不是 $N$，
这使得估计量 $\hat{\sigma}^2$ 是无偏估计量：
$E(\hat{\sigma}^2) = \sigma^2$。

为了对模型和参数做统计推断，
需要做进一步的假设。
首先，假设等式 3.1 为输出变量期望的正确模型，
即 $Y$ 的条件期望对 $X\_1, \dots, X\_p$ 呈线性关系。
其次，假设 $Y$ 与其期望的偏离是加性的且服从高斯分布。
因此：

$$\begin{align}
Y &= E(Y|X\_1, \dots, X\_p) + \varepsilon \\\\ &=
\beta\_0 + \sum\_{j=1}^p X\_j\beta\_j + \varepsilon
\tag{3.9}\end{align}$$

其中误差项 $\varepsilon$ 为高斯随机变量，期望为 0 方差为 $\sigma^2$，
即 $\varepsilon \sim \mathcal{N}(0, \sigma^2)$。

从等式 3.9 可推导出：

$$\hat{\beta} \sim \mathcal{N}(\beta,
(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2) \tag{3.10}$$

这是一个多元正态分布，期望向量和协方差矩阵如上式所示。并且：

$$(N-p-1)\hat{\sigma}^2 \sim \sigma^2\chi^2\_{N-p-1} \tag{3.11}$$

即自由度为 $N-p-1$ 的卡方分布。
此外，$\hat{\beta}$ 与 $\hat{\sigma}^2$ 统计意义上相互独立。
根据以上的概率分布，可对参数 $\beta\_j$ 进行假设检验和建立置信区间。

我们构造 **标准（回归）系数**（standardized (regression) coefficients）
或 **Z 分数**（Z-score）来检验某个系数的假设 $\beta\_j = 0$：

$$z\_j = \frac{\hat{\beta\_j}}{\hat{\sigma}\sqrt{v\_j}}\tag{3.12}$$

式中的 $v\_j$ 为矩阵 $(\mathbf{X}^T\mathbf{X})^{-1}$ 的第 j 个对角线元素。
在原假设下，即 $\beta\_j=0$，统计量 $z\_j$ 的概率分布为 $t\_{N-p-1}$
（自由度为 $N-p-1$ 的 t 分布）。
因此，如果计算得出的 $z\_j$ 绝对值过大，则会拒绝原假设。
若用已知真实的 $\sigma^2$ 替换等式中的估计量 $\hat{\sigma}^2$，
则 $z\_j$ 服从标准正态分布。
当样本量足够大时，t 分布与标准正态分布的尾部百分位非常接近，
所以在实践中一般会直接使用正态分布（见图 3.3）。

{{< figure
  src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_03.png"
  title="**图3.3**：三个概率分布的密度函数尾部 $\text{Pr}(|Z|>z)$，分别为 $t\_{30}$、$t\_{100}$、和标准正态。图中同时标记了三个分布在显著性水平 0.05 和 0.01 下相应的分位数。当 N 大于 100 后，t 分布与标准正态分布之间的区别可以忽略不计。"
>}}

通常也要检验一组系数是否同时显著。
例如，一个有 k 个类型的分类变量会在模型中引入多个哑变量，
为了验证这个分类变量是否可从模型中去除，
需要检验所有哑变量的系数同时为 0 的假设。
这可使用 F 统计量：

$$F = \frac{(\text{RSS}\_0 - \text{RSS}\_1) / (p\_1 - p\_0)}
{\text{RSS}\_1 / (N-p\_1-1)}\tag{3.13}$$

计算这个统计量需要进行两次最小二乘拟合。
首先使用所有的输入变量进行最小二乘拟合，称之为模型 1，有 $p\_1+1$ 个参数；
然后排除掉其中一部分输入变量，或也可视为将对应的参数值固定为 0，
再次进行最小二乘拟合，称之为模型 0，参数个数为 $p\_0+1$。
将第二个模型中的 $p\_1-p\_0$ 个输入变量的参数固定为 0，
即从模型 1 得到了 模型 0，
模型 0 是模型 1 的嵌套模型。
上式中的脚标 1 即为模型 1，脚标 0 即为模型 0，
RSS 为相应最小二乘拟合的残差平方和。
F 统计量衡量的是被 $\sigma^2$ 估计量标准化之后，
每多增加一个输入参数可减少的拟合残差平方和。
若延续之前的高斯分布假设，
并且且参数个数少的模型 0 为真实模型的原假设成立，
则 F 统计量服从概率分布 $F\_{p\_1-p\_0, N-p\_1-1}$。
可证明（练习 3.1）当原假设只涉及一个参数时，
等式 3.12 中的 $z\_j$ 与 F 统计量的检验结果是完全等价的。
当样本量 $N$ 足够大时，
概率分布 $F\_{p\_1-p\_0, N-p\_1-1}$ 的分位数逼近于概率分布
$\chi^2\_{p\_1-p\_0}$ 的分位数。

通过等式 3.10 可推导出 $\beta\_j$ 的 $1-2\alpha$ 置信区间：

$$(\hat{\beta\_j} - z^{(1-\alpha)} v\_j^{\frac{1}{2}} \hat{\sigma},
\hat{\beta\_j} + z^{(1-\alpha)} v\_j^{\frac{1}{2}} \hat{\sigma})
\tag{3.14}$$

其中 $z^{(1-\alpha)}$ 为标准正态分布的 $1-\alpha$ 分位数：

$$\begin{align}
z^{1-0.025} &= 1.96 \\\\ z^{1-0.05} &= 1.645, etc
\end{align}$$

因而实践中会默认输出 $\hat{\beta}\pm 2\cdot \text{se}(\hat{\beta})$ ,
即大约 95% 水平的置信区间。
即使高斯分布的假设不成立，在样本量足够大的情况下 $N\rightarrow\infty$，
这个区间的范围也大致是正确的，即 $1-2\alpha$ 的置信水平。

用类似的方法可得到一组输入变量参数向量 $\beta$ 在大样本下的近似置信集合：

$$C\_\beta = \\{\beta|
(\hat{\beta}-\beta)^T\mathbf{X}^T\mathbf{X}(\hat{\beta}-\beta) \leq
\hat{\sigma}^2 {\chi^2\_{p+1}}^{(1-\alpha)}\\}\tag{3.15}$$

其中 ${\chi^2\_l}^{(1-\alpha)}$
为 $l$ 个自由度的卡方分布的 $1-\alpha$ 分位数，
例如 ${\chi^2\_5}^{(1-0.05)} = 11.1$，${\chi^2\_5}^{(1-0.1)} = 9.2$。
而通过这个 $\beta$ 的置信集合可以得到真实函数 $f(x) = x^T\beta$ 相应的置信区间
$\\{x^T\beta | \beta \in C\_\beta\\}$。
（练习 3.2；函数的置信区域的例子另见第 5.2.2 节的图5.4。）

### 3.2.1 实例：前列腺癌症

本例子中的数据来自于 Stamey et al. (1989) 的研究。
其中考察了一组即将彻底切除前列腺的男性病人的
前列腺特异抗原（PSA）水平与一些临床指标之间的相关性。
特征变量有
癌肿瘤体积的对数（lcavol），
前列腺重量的对数（lweight），
年龄（age），
良性前列腺增生数量的对数（lbph），
是否存在精囊侵袭（svi）,
囊穿透的对数（lcp），
格里森评分（gleason），
以及格里森评分 4 和 5 所占百分比（pgg45）。
表 3.1 为这些自变量的相关系数矩阵，
可见它们之间存在着一些强相关性。
[第一章]({{< ref "/post/eslii/ch01/ch01_00.zh.md" >}})中的图 1.1（书中第 3 页）
为这些变量的两两散点图矩阵。
其中 svi 为二元分类变量，gleason 为有序分类变量。
从图中可看出变量 lcavol 和 lcp 都与输出变量 lpsa （PSA 水平的对数）有强相关性，
同时两者之间也存在强相关性。
通过利用这些自变量的最小二乘拟合，可以独立出每个自变量对因变量的影响。

|         | lcavol | lweight | age | lbph | svi | lcp | gleason |
|:--------|--------|---------|-----|------|-----|-----|---------|
| lcavol  | 0.300 |       |       |       |       |       |       |
| lweight | 0.286 | 0.317 |       |       |       |       |       |
| age     | 0.063 | 0.437 | 0.287 |       |       |       |       |
| lbph    | 0.593 | 0.181 | 0.129 |−0.139 |       |       |       |
| svi     | 0.692 | 0.157 | 0.173 |−0.089 | 0.671 |       |       |
| lcp     | 0.426 | 0.024 | 0.366 | 0.033 | 0.307 | 0.476 |       |
| gleason | 0.483 | 0.074 | 0.276 |−0.030 | 0.481 | 0.663 | 0.757 |
**表3.1**：前列腺癌症数据集中的自变量之间的相关性矩阵。

我们将自变量的方差标准化为 1 后，对输出变量 lpsa（PSA 水平的对数） 拟合线性模型。
数据集被随机分为两部分，训练集包含 67 个样本，测试集包含 30 个样本。
表 3.2 为最小二乘法拟合出的参数估计值、标准误差和 Z 分数。

| Term | Coefficient | Std. Error | Z Score |
|------|-------------|------------|---------|
| Intercept | 2.46  | 0.09 | 27.60 |
| lcavol    | 0.68  | 0.13 | 5.37  |
| lweight   | 0.26  | 0.10 | 2.75  |
| age       | −0.14 | 0.10 | −1.40 |
| lbph      | 0.21  | 0.10 | 2.06  |
| svi       | 0.31  | 0.12 | 2.47  |
| lcp       | −0.29 | 0.15 | −1.87 |
| gleason   | −0.02 | 0.15 | −0.15 |
| pgg45     | 0.27  | 0.15 | 1.74  |
**表3.1**：前列腺数据集的线性模型拟合结果。
Z 分数按照等式 3.12 计算，即参数的估计值除以其标准误差。
大致上绝对值大于 2 的 Z 分数代表着这个参数在 $p=0.05$ 的置信水平下显著不为 0。

Z 分数如等式 3.12 中所定义，代表着对应输入变量的显著性水平，
或可理解为去掉这个变量对拟合的影响大小。
大于 2 的 Z 分数即代表着大致 5% 置信水平下的显著性。
（在这个数据集中，拟合中有 9 个参数，
$t\_{67-9}$ 分布的 1-0.025 分位数为 2.002，非常接近于 2。）
自变量中 lcavol 的显著性最强，另外 lweight 和 svi 也有比较大的 Z 分数绝对值。
值得注意，当 lcavol 与 lcp 同时在模型中时，lcp 并不显著。
（但模型中若只有 lcp 而没有 lcavol，则 lcp 有强显著性。）


用 F 统计量（等式 3.13）来检验同时排除一组输入变量的假设。
假如要检验是否可以排除表 3.2 中所有不显著的变量，即 age，lcp，gleason，和 pgg45，
相应的 F 统计量为：

$$F = \frac{(32.81 - 29.43) / (9 - 5)}{29.43 / (67 - 9)} \tag{3.16}$$

其对应的 p 值为 0.17（$\text{Pr}(F\_{4,58}>1.67) = 0.17$），因此不显著。
结论是不能拒绝这几个变量的参数同时为 0 的假设。

在测试集上的平均预测误差为 0.521。
在用训练集的输出变量 lpse 的均值作为测试集的预测时，相应的平均预测误差为 1.057，
这也被称为“基础误差率（base error rate）”。
因此，线性模型的误差率比基础误差率小了大于 50%。
稍后章节我们会回到这个实例来对比不同的特征选择和 *收缩（shrinkage）* 方法。

### 3.2.2 高斯－马尔可夫定理

在参数 $\beta$ 的所有线性无偏估计中，最小二乘估计的方差是最小的，
这是统计学中最著名的结论之一。
接下来将明确地表述这个结果，
同时也指出对估计量无偏的限制可能是不明智的。
这也引出了在本章后续会介绍的有偏估计，比如岭回归。

考虑参数的某种线性组合 $\theta = a^T\beta$；
例如线性模型的预测 $f(x\_0) = x\_0^T\beta$ 即为一个线性组合。
那么 $a^T\beta$ 的最小二乘估计为：

$$\hat{\theta} = a^T\hat{\beta} =
a^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \tag{3.17}$$

假设 $\mathbf{X}$ 可视为常数，
那么上式为输出变量向量 $\mathbf{y}$ 的一个线性函数
$\mathbf{c}^T\_0\mathbf{y}$。
若线性模型是正确的，那么 $a^T\hat{\beta}$ 为无偏估计：

$$\begin{align}
E[a^T\hat{\beta}] &=
E(a^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}) \\\\ &=
E(a^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\beta) \\\\ &=
a^T\beta \tag{3.18}
\end{align}$$

**高斯－马尔可夫定理**（Gauss-Markov theorem）的内容是，
对于其他任意一个无偏的 $a^T\beta$ 的线性估计量
$\tilde{\theta} = \mathbf{c}^T\mathbf{y}$，
即 $E(\mathbf{c}^T\mathbf{y}) = a^T\beta$，则：

$$\text{Var}(a^T\hat{\beta}) \leq \text{Var}(a^T\mathbf{y}) \tag{3.19}$$

上式的证明（练习 3.3）利用了三角不等式。
简单起见上述过程只考虑了对单一数值 $a^T\beta$ 的估计，
在一些附加的定义后，
同样的结论可拓展到针对参数向量 $\beta$（练习 3.3）。

下面考虑 $\theta$ 的估计 $\tilde{\theta}$ 的均方误差：

$$\begin{align}
\text{MSE}(\tilde{\theta}) &= E(\tilde{\theta} - \theta)^2 \\\\ &=
\text{Var}(\tilde{\theta}) + [E(\tilde{\theta} - \theta)]^2 \tag{3.20}
\end{align}$$

式中第一项为方差，第二项为平方偏差。
高斯-马尔可夫定理说明在所有的线性无偏估计量中，最小二乘估计量的均方误差是最小的。
然而根据上式，如果引入一些小的偏差可以很大程度地降低方差，
则有可能存在一个均方误差更小的有偏的估计。
有偏估计很常见。
所有将最小二乘法中的部分参数向零缩减（shrink）或直接设为零的方法都可能产生有偏估计。
本章稍后介绍的变量选择和岭回归方法都是有偏估计的例子。
在实际应用场景中，模型都是对现实的简化，因此得到的都是有偏估计；
模型选择就是在偏差和方差之间寻找合适的平衡点。
第七章会更详细讨论相关内容。

在第二章提到过，均方误差是与预测的准确程度紧密关联的指标。
假设要对在输入变量 $x\_0$ 处的值做预测：

$$Y\_0 = f(x\_0) + \varepsilon\_0 \tag{3.21}$$

则某个估计量 $\tilde{f}(x\_0) = x\_0^T\beta$ 的期望预测误差为

$$\begin{align}
E(Y\_0 - \tilde{f}(x\_0))^2 &=
\sigma^2 + E(x\_0^T\tilde{\beta} - f(x\_0))^2 \\\\ &=
\sigma^2 + \text{MSE}(\tilde{f}(x\_0)) \tag{3.22}
\end{align}$$

因此期望预测误差与均方差仅相差一个代表观测本身的随机性的方差常数 $\sigma^2$。

### 3.2.3 从简单一元回归到多元回归

当 $p>1$ 时，等式 3.1 中的线性模型称为多元线性回归模型。本
本节从单变量线性模型（$p=1$）入手，
以此能够更好地理解多元的最小二乘解（等式 3.6）。

先假设一个无常数项的单变量模型：

$$Y = X\beta + \varepsilon \tag{3.23}$$

则最小二乘的估计和残差为：

$$\begin{align}
\hat{\beta} &= 
\frac{\sum\_{i=1}^N x\_i y\_i}{\sum\_{i=1}^N x\_i^2 } \\\\ r\_i &=
y\_i - x\_i\hat{\beta}
\end{align}\tag{3.24}$$

定义 $\mathbf{y} = (y\_1, \dots, y\_N)^T$，
$\mathbf{x} = (x\_1, \dots, x\_N)^T$，
以及两个向量的内积[^3]为： 

$$\begin{align}
\langle \mathbf{x}, \mathbf{y} \rangle &= \sum\_{i=1}^N x\_i, y\_i \\\\ &=
\mathbf{x}^T \mathbf{y} \tag{3.25}
\end{align}$$

则等式 3.24 可用向量和内积来表达：

$$\begin{align}
\hat{\beta} &= 
\frac{\langle \mathbf{x}, \mathbf{y} \rangle}
{\langle \mathbf{x}, \mathbf{y} \rangle} \\\\ \mathbf{r} &=
\mathbf{y} - \mathbf{x}\hat{\beta}
\end{align}\tag{3.24}$$

这个简单的单变量回归是多元线性回归的基础。
下面假设多元线性回归的多个输入变量向量
$\mathbf{x}\_1$，$\mathbf{x}\_2$，……，$\mathbf{x}\_p$
（矩阵 $\mathbf{X}$ 的列向量）彼此正交；
即对于任意的 $j \neq k$，
有 $\langle \mathbf{x}\_j, \mathbf{x}\_k \rangle = 0$。
则不难推导出多元最小二乘的估计 $\hat{\beta}\_j$ 为
$\langle \mathbf{x}\_j, \mathbf{y} \rangle /
\langle \mathbf{x}\_j, \mathbf{x}\_j \rangle$，
即这个变量的单变量回归估计。
也就是说当输入变量彼此正交时，
它们在模型参数的估计中彼此没有影响[^4]。

通常在精心设计的实验中可以得到正交的输入变量，
然而在观测数据集中几乎不可能存在。
因此我们需要通过对输入变量的正交化来推广上述的结果。
接下来考虑一个包含常数项的单变量 $\mathbf{x}$ 模型。
则 $\mathbf{x}$ 的最小二乘系数估计为：

$$\hat{\beta}\_1 = \frac{\langle \mathbf{x} - \bar{x}\mathbf{1}, \mathbf{y}\rangle}
{\langle \mathbf{x} - \bar{x}\mathbf{1}, \mathbf{x} - \bar{x}\mathbf{1}\rangle}\tag{3.27}$$

其中的 $\bar{x} = \sum\_i x\_i/N$，
$\mathbf{1} = \mathbf{x}\_0$ 为一个长度为 N 每个元素为 1 的向量。
等式 3.27 中的估计可被分解为应用了两个简单回归（等式 3.26）：

1. $\mathbf{x}$ 对常数向量 $\mathbf{1}$ 做回归，
   生成残差 $\mathbf{z} = \mathbf{x} - \bar{x}\mathbf{1}$。
2. $\mathbf{y}$ 对残差向量 $\mathbf{z}$ 做回归，
   得到系数的估计 $\hat{\beta\_1}$。

其中的“**b** 对 **a** 做回归（regress **b** on **a**）”
指的是以 **b** 为因变量 **a** 为自变量的无截距项的单变量回归，
其斜率的估计为
$\hat{\gamma} = \langle \mathbf{a}, \mathbf{b} \rangle / 
\langle \mathbf{a}, \mathbf{a} \rangle$，
残差向量为 $\mathbf{b} - \hat{\gamma}\mathbf{a}$。
这个过程称之为用 **a** 调整或正交化（orthogonalize） **b**，

第一步中用常数向量 $\mathbf{x}\_0 = \mathbf{1}$ 正交化 $\mathbf{x}$。
第二步中用第一步的残差 $\mathbf{z}$ 为输入变量进行简单单变量回归[^5]。
图 3.4 以两个输入变量为例演示了这个过程，
正交化不会改变输入向量 $\mathbf{x}\_1$ 和 $\mathbf{x}\_2$ 生成的子空间，
而是生成了这个子空间的一组正交基。

{{< figure src="http://public.guansong.wang/eslii/ch03/eslii_fig_03_04.png"
  title="**图3.4**：通过输入变量的正交化求解最小二乘回归。$\mathbf{z}$ 为向量 $\mathbf{x}\_2$ 对 $\mathbf{x}\_1$ 回归的残差，$\mathbf{y}$ 对 $\mathbf{z}$ 的回归得到多元回归中 $\mathbf{x}\_2$ 的系数估计。最小二乘拟合 $\hat{\mathbf{y}}$ 可以分解为 $\mathbf{y}$ 在 $\mathbf{x}\_1$ 和 $\mathbf{z}$ 方向上的投影的和。"
>}}

算法 3.1 将上述流程推广至 $p$ 个输入变量的回归中。
注意其中第二步中生成的残差向量
$\mathbf{z}\_0$，……，$\mathbf{z}\_{j-1}$ 彼此正交，
所以其中计算出的单变量回归的系数（$\hat{\gamma\_{lj}}$）
实际上也是多元回归的系数。

> ### 算法 3.1：逐步正交化回归
> 1. 初始化常数向量 $\mathbf{z}\_0 = \mathbf{x}\_0 = \mathbf{1}$.
> 2. 对 $j = 1, 2, \dots, p$ 依次进行： <br/>
>   通过 $\mathbf{x}\_j$ 对
>   $\mathbf{z}\_0$，$\mathbf{z}\_1$，……，$\mathbf{z}\_{j-1}$ 的回归，
>   得到系数估计 
>   $$\hat{\gamma}\_{lj} =
>   \langle \mathbf{z}\_l, \mathbf{x}\_j \rangle /
>   \langle \mathbf{z}\_l, \mathbf{z}\_l \rangle,
>   l = 0, \dots, j−1$$
>   以及回归的残差
>   $$\mathbf{z}\_j = \mathbf{x}\_j -
>   \sum\_{k=0}^{j-1}\hat{\gamma}\_{kj}\mathbf{z}\_k$$
> 3. 通过 $\mathbf{y}$ 对残差 $\mathbf{z}\_p$ 的回归
>   得到参数估计 $\hat{\beta\_p}$。

算法最终的结果为多元回归中的第 p 个参数的估计：

$$\hat{\beta}\_p  = \frac{\langle \mathbf{z}\_p, \mathbf{y} \rangle}
{\langle \mathbf{z}\_p, \mathbf{z}_p \rangle}\tag{3.28}$$

算法第二步中的逐步正交过程，使得每个向量 $\mathbf{x}\_j$
均为一组向量 $\mathbf{z}\_k$（$k\leq j$）的线性组合。
所有的 $\mathbf{z}\_j$ 彼此正交，
因此它们构成了 $\mathbf{X}$ 的列空间的一组基（basis）。
最小二乘拟合 $\hat{\mathbf{y}}$ 是输出变量向量在这个空间上的投影，
故它也是这组基的线性组合。
由于基中只有 $\mathbf{z}\_p$ 中包含 $\mathbf{x}\_p$（并且系数为 1），
其在投影的正交基线性组合中系数为等式 3.28，
故这个系数也是原多元回归中 $\mathbf{x}\_p$ 的最小二乘系数。
通过改变输入变量的脚标顺序，将不同的输入变量放在最后一位，
则重复算法 3.1 可以得到所有参数估计。

算法 3.1 不仅提供了另一个计算最小二乘的方法，
同时也揭示了多元回归中存在相关性的输入变量之间的影响。
这个过程说明了多元回归中的第 j 个回归系数，
等价于 $\mathbf{y}$ 对
$\mathbf{x}\_{j\cdot 012\dots (j-1)(j+1)\dots p}$
（$\mathbf{x}\_j$ 对所有其他输入变量
$\mathbf{x}\_0$，$\mathbf{x}\_1$，……，$\mathbf{x}\_{j-1}$，
$\mathbf{x}\_{j+1}$，……，$\mathbf{x}\_p$ 回归的残差）
的单变量回归系数。

直观的来说：
多元回归系数 $\hat{\beta\_j}$ 体现的是排除了其他变量的影响后，
$\mathbf{x}\_j$ 对 $\mathbf{y}$ 的影响。

若 $\mathbf{x}\_p$ 与其他一个或几个输入变量有强相关性，
那么最终的残差向量 $\mathbf{z}\_p$ 会十分接近于 0，
通过等式 3.28 可知其系数估计 $\hat{\beta\_p}$ 不稳定。
与之强相关的其他输入变量也同样如此。
此时可能出现的是，这组变量的 Z 评分都不大（如图 3.2），
其中每个变量都可以被排除，然而我们却无法将这组变量全部排除。
从等式 3.28 也可得到方差估计（等式 3.8）的另一个表达式：

$$\text{Var}(\hat{\beta}\_p) =
\frac{\sigma^2}{\langle \mathbf{z}\_p, \mathbf{z}\_p \rangle} =
\frac{\sigma^2}{\\|\mathbf{z}\_p\\|^2}\tag{3.29}$$

也就是说，对 $\hat{\beta\_p}$ 估计的准确度取决于残差向量
$\mathbf{z}\_p$ 的长度，
即在 $\mathbf{x}\_p$ 的信息中有多少是无法被其他输入变量解释的。

算法 3.1 也被称为多元回归的
**格拉姆-施密特正交化**（Gram–Schmidt procedure），
它也是一个实践中最小二乘估计的数值计算方法。
算法最终可获得参数估计 $\hat{\beta\_p}$，
以及最小二乘的拟合结果（练习 3.4）。

算法 3.1 中的第二步可以用矩阵的形式表达：

$$\mathbf{X} = \mathbf{Z}\mathbf{\Gamma} \tag{3.30}$$

其中 $\mathbf{Z}$ 的列（依次）为 $\mathbf{z}\_j$，
$\mathbf{\Gamma}$ 为上三角矩阵，矩阵中的元素为 $\hat{\gamma\_{kj}}$。
定义对角矩阵 $\mathbf{D}$，其第 j 个对角线元素为
$\mathbf{D}\_{jj} = \\| \mathbf{z}\_j \\| $，
则：

$$\begin{align}
\mathbf{X} &= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\\\ &=
\mathbf{Q}\mathbf{R} \tag{3.31}
\end{align}$$

便是所谓的矩阵 $\mathbf{X}$ 的**QR 分解**（QR decomposition）。
其中的 $\mathbf{Q}$ 为 $N \times (p+1)$ 的正交矩阵，
且 $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$，
$\mathbf{R}$ 为 $(p+1) \times (p+1)$ 的上三角矩阵。
QR 分解为矩阵 $\mathbf{X}$ 的列向量空间提供了一个方便的正交基。
例如，最小二乘的解可以很方便的写为：

$$\begin{align}
\hat{\beta} &= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}
\tag{3.32} \\\\ \hat{\mathbf{y}} &=
\mathbf{Q}\mathbf{Q}^T\mathbf{y}\tag{3.33}
\end{align}$$

由于 $\mathbf{R}$ 为上三角矩阵，其逆矩阵很容易获得。
（练习 3.4）。

### 3.2.4 多输出变量回归

假设有多个输出变量 $Y\_1$，$Y\_2$，……，$Y\_K$，
都用 $p$ 个输入变量 $X\_0$，$X\_1$，$X\_2$，……，$X\_p$ 来预测。
则对每一个输出变量，都有线性模型：

$$\begin{align}
Y\_k &= \beta\_{0k} + \sum\_{j=1}^p X\_j\beta\_{jk} + \varepsilon\_k
\tag{3.34} \\\\ &=
f\_k(X) + \varepsilon\_k \tag{3.35}
\end{align}$$

假设样本大小为 $N$，可以将模型写成矩阵形式：

$$\mathbf{Y} = \mathbf{X}\mathbf{B} + \mathbf{E} \tag{3.36}$$

其中 $\mathbf{Y}$ 为一个 $N \times K$ 的因变量矩阵，
其中第 ik 位置的元素为 $y\_{ik}$；
$\mathbf{X}$ 为 $N \times (p+1)$ 的输入变量矩阵；
$\mathbf{B}$ 为 $(p+1) \times K$ 的参数矩阵；
$\mathbf{E}$ 为 $N \times K$ 的误差项矩阵。
对单输出变量的损失函数（等式 3.2）的简单推广可得到：

$$\begin{align}
\text{RSS}(\mathbf{B}) &=
\sum\_{k=1}^K\sum\_{i=1}^N (y\_{ik} - f\_k(x\_i))^2 \tag{3.37} \\\\ &=
tr[(\mathbf{Y}-\mathbf{X}\mathbf{B})^T(\mathbf{Y}-\mathbf{X}\mathbf{B})] \tag{3.38}
\end{align}$$

最小二乘法的估计量和单输出变量时的解一致，但其中的矩阵维度不同：

$$\hat{\mathbf{B}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}\mathbf{Y} \tag{3.39}$$

因此第 k 个输出变量的系数估计等同于
$\mathbf{x}\_0$，$\mathbf{x}\_0$，……，$\mathbf{x}\_p$ 对 $\mathbf{y}\_k$ 的最小二乘回归。
多个输出变量不会影响彼此的最小二乘估计。

但如果等式 3.34 中的误差项 $\varepsilon = (\varepsilon\_1,\dots,\varepsilon\_K)$ 彼此存在相关，
则可能需要将等式 3.37 的定义针对多输出变量做调整。
假设 $\text{Cov}(\varepsilon) = \mathbf{\Sigma}$，则多元加权判别标准为：

$$\text{RSS}(\mathbf{B}; \mathbf{\Sigma}) =
\sum\_{i=1}^N (y\_i-f(x\_i))^T \mathbf{\Sigma}^{-1} (y\_i-f(x\_i)) \tag{3.40}$$

其与多元高斯分布的等式比较相像。
$f(x)$ 的取值为一个向量 $(f\_1(x),\dots,f\_K(x))^T$，
$y\_i$ 为观测样本 i 的长度为 K 的向量。
然而推导得出的解与等式 3.39 一致，即误差无相关性时的分别进行的 K 个回归解（练习 3.11）。
在样本间有不同的协方差矩阵 $\Sigma\_i$ 时，以上结论不再成立，输出变量之间会有相互影响。

第 3.7 节会继续讨论多个输出变量的问题，
以及一些联合回归可改善预测结果的场景。

### 练习

**练习 3.1** 证明在检验是否可从模型中排除单个系数时，
F 统计量（等式 3.13）等于相应的 Z 分数（等式 3.12）的平方。

**解 3.1**

单个系数的 Z 分数，或 t 统计量，的定义是这个系数的估计值与
这个系数标准差的估计值的比值：

$$ t\_j = \frac
{\hat{\beta\}_j}
{\sqrt{\widehat{\text{Var}(\hat{\beta})}}}
= \frac{\hat{\beta\}_j}{\hat{\sigma}\sqrt{v\_j}}$$

其中 $\hat{\beta}$ 的方差见等式 3.8，
上式中的 $v\_j$ 也来自于等式 3.8 中矩阵
$(\mathbf{X}^T\mathbf{X})^{-1}$ 对角线上的第 $j$ 个元素。

假设 $\mathbf{z}\_j$ 为 $x\_j$ 对所有其他输入变量回归后的残差，
则根据对算法 3.1 的讨论可知，

$$\begin{align}
\hat{\beta}\_j &= \frac
{\langle \mathbf{z}\_j, \mathbf{y} \rangle}
{\langle \mathbf{z}\_j, \mathbf{z}_j \rangle}
\tag{参考等式 3.28} \\\\ \widehat{\text{Var}(\hat{\beta}\_j)} &=
\frac{\hat{\sigma}^2}{\langle \mathbf{z}\_j, \mathbf{z}\_j \rangle} =
\frac{\hat{\sigma}^2}{\\|\mathbf{z}\_j\\|^2}
\tag{参考等式 3.29}
\end{align}$$

借鉴计量经济学中常用的定义：

* 总（离差）平方和（total sum of squares, TSS）：
  $\text{TSS} = \sum\_{i=1}^N (y\_i - \bar{y})^2$
* 解释平方和（explained sum of squares, ESS）：
  $\text{ESS} = \sum\_{i=1}^N (\hat{y}\_i - \bar{y})^2$
* 残差平方和（residual sum of squares, RSS）：
  $\text{RSS} = \sum\_{i=1}^N (y\_i - \hat{y}\_i)^2$

当回归中存在常数项时，$\text{TSS} = \text{ESS} + \text{RSS}$。
因此变量 $X\_j$ 带来的 RSS 降低从数值上等于其带来的 ESS 增加。
而由于 $z\_j$ 与所有其他变量的空间正交：

$$\begin{align}
\text{ESS} &= \sum\_{i=1}^N (\hat{y}\_i - \bar{y})^2 \\\\ &=
\sum\_{i=1}^N
(x\_{-j}^T\hat{\beta}\_{-j} + z\_{ji}\hat{\beta}\_j - \bar{y})^2 \\\\ &=
\sum\_{i=1}^N (x\_{-j}^T\hat{\beta}\_{-j} - \bar{y})^2 +
\sum\_{i=1}^N (z\_{ji}\hat{\beta}\_j)^2 \\\\ &=
\text{ESS}\_{-j} + \hat{\beta}\_j^2 \\|\mathbf{z}\_j\\|^2
\end{align}$$

其中的下标 $\cdot\_{-j}$ 表示去掉变量 $X\_j$ 后相应的值：
$x\_{-j}$ 为去掉 $x\_{j}$ 后的输入变量向量，长度为 $p+1-1=p$；
$\hat{\beta}\_{-j}$ 为去掉 $X\_j$ 后的最小二乘估计系数，长度为 $p$；
$\text{ESS}\_{-j}$ 为去掉 $X\_j$ 后的 $p$ 个变量的模型的 ESS。
第三步拆分平方项时，利用了 $\mathbf{z}\_j$ 与其他变量的正交，
以及 $\mathbf{z}\_j$ 本身是一个回归的残差。

由此可见，模型中加入变量 $X\_j$ 后使 RSS 降低了
$\hat{\beta}\_j^2 \\|z\_j\\|$。
代入到 F 统计量的定义等式 3.13 中：

$$\begin{align}
F\_j &=
\frac{(\text{RSS}\_{-j} - \text{RSS}) / 1}
{\text{RSS} / (N - p - 1)} \\\\ &=
\frac{\text{ESS}\_j}{\hat{\sigma}^2} =
\frac{\hat{\beta}\_j^2 \\|z\_j\\|^2}{\hat{\sigma}^2} =
\frac{\hat{\beta}\_j^2}{\hat{\sigma}^2 /  \\|z\_j\\|^2} =
\frac{\hat{\beta}\_j^2}{\widehat{\text{Var}(\hat{\beta})}} \\\\ &=
t\_j^2
\end{align}$$

证毕。


**练习 3.2** 给定两个变量 $X$ 和 $Y$ 的数据样本，拟合一个三次多项式回归模型
$f(X) = \sum\_{j=0}^3 \beta\_j X^j$。
除拟合曲线外，你需要围绕曲线的 95% 水平置信区间带。
考虑以下两种方法：

1. 在每个点 $x\_0$，为线性函数 $a^T\beta = \sum\_{j=0}^3 \beta\_j X^j$
  构建 95% 置信区间。
2. 先如等式 3.15 构建 $\beta$ 的 95% 置信区间，
  再从中生成函数 $f(x\_0)$ 的置信区间。

这两种方法有何区别？
哪个更可能得出更宽的置信区间带？
通过一个小模拟试验来比较两种方法。

**练习 3.3** 高斯－马尔可夫定理（Gauss-Markov theorem）：

1. 证明高斯－马尔可夫定理：一个参数 $a^T\beta$ 的最小二乘估计的方差
  不大于任意其他 $a^T\beta$ 的无偏估计的方差。
2. 矩阵不等式 $\mathbf{B} \preceq \mathbf{A}$
  的定义为 $\mathbf{A} - \mathbf{B}$ 是半正定矩阵。
  若 $\hat{\mathbf{V}}$ 为 $\beta$ 最小二乘估计的协方差矩阵，
  $\tilde{\mathbf{V}}$ 为任意其他线性无偏估计的协方差矩阵，
  则 $\hat{\mathbf{V}} \preceq \hat{\mathbf{V}}$。

**练习 3.4** Show how the vector of least squares coefficients can be obtained
from a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Represent
your solution in terms of the QR decomposition of X.

[^1]: 二阶导数矩阵为正定矩阵，故在一阶导数为 0 的点，为函数的极小值点。
[^2]: 注意图 3.2 与图 3.1 的想要表达的维度是不同的，前者是 $N$ 维，后者是 $p+1$ 维，只是为了理解都画成了三维的图形。图 3.2 中黄色的平面即代表着列空间，上面的点均为某个线性模型的实现，拟合的过程即寻找这个平面上与训练样本的输出实现向量最近的点。用三维图像中的直觉来说，最近的点即为向量 $\mathbf{y}$ 在这个平面上的投影点。
[^3]: 原文脚注 1：使用向量和内积的写法，把线性回归与多维空间和概率空间联系了起来。
[^4]: 比如 $\hat{\beta}\_j$ 的表达式中只涉及这个输入变量 $\mathbf{x}\_j$ 本身，不包含其他输入变量。
[^5]: 此处原文为“第二步为以 $\mathbf{1}$ 和 $\mathbf{z}$ 为输入变量的简单的单变量回归。”译者觉得原文可能有混淆，但正交化后 $\mathbf{z}$ 的参数估计并不会被是否存在常数项影响，只是加入常数项后可以得到最小二乘的拟合值 $\hat{y}$。
