+++
title = "ESL-3.6 Methods Using Derived Input Directions"
summary = """
统计学习基础（译注）第三章第五节，第 79-82 页。
"""

date = 2018-09-22T10:02:07+08:00
lastmod = 2018-09-22T10:02:07+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In many situations we have a large number of inputs, often very correlated.
The methods in this section produce a small number of linear combinations
Z m , m = 1, . . . , M of the original inputs X j , and the Z m are then used in
place of the X j as inputs in the regression. The methods differ in how the
linear combinations are constructed.

### 3.5.1 Principal Components Regression

In this approach the linear combinations Z m used are the principal com-
ponents as defined in Section 3.4.1 above.

Principal component regression forms the derived input columns z m =
Xv m , and then regresses y on z 1 , z 2 , . . . , z M for some M ≤ p. Since the z m
are orthogonal, this regression is just a sum of univariate regressions:

$$\hat{\mathbf{y}}^\text{pcr}\_{(M)} =
\bar{y}\mathbf{1} + \sum\_{m=1}^M \hat{\theta}\_m \mathbf{z}\_m \tag{3.61}$$

where θ̂ m = hz m , yi/hz m , z m i. Since the z m are each linear combinations
of the original x j , we can express the solution (3.61) in terms of coefficients
of the x j (Exercise 3.13)

$$\hat{\beta}^\text{pcr}(M) = \sum\_{m=1}^M \hat{\theta}\_m v\_m \tag{3.62}$$

As with ridge regression, principal components depend on the scaling of
the inputs, so typically we first standardize them. Note that if M = p, we
would just get back the usual least squares estimates, since the columns of
Z = UD span the column space of X. For M < p we get a reduced regres-
sion. We see that principal components regression is very similar to ridge
regression: both operate via the principal components of the input ma-
trix. Ridge regression shrinks the coefficients of the principal components
(Figure 3.17), shrinking more depending on the size of the corresponding
eigenvalue; principal components regression discards the p − M smallest
eigenvalue components. Figure 3.17 illustrates this.

In Figure 3.7 we see that cross-validation suggests seven terms; the re-
sulting model has the lowest test error in Table 3.3.

### 3.5.2 Partial Least Squares

This technique also constructs a set of linear combinations of the inputs
for regression, but unlike principal components regression it uses y (in ad-
dition to X) for this construction. Like principal component regression,
partial least squares (PLS) is not scale invariant, so we assume that each
x j is standardized to have mean 0 and variance 1. PLS begins by com-
puting φ̂ 1j = hx j , yi for each j. From this we construct the derived input
z 1 =j φ̂ 1j x j , which is the first partial least squares direction. Hence
in the construction of each z m , the inputs are weighted by the strength
of their univariate effect on y 3 . The outcome y is regressed on z 1 giving
coefficient θ̂ 1 , and then we orthogonalize x 1 , . . . , x p with respect to z 1 . We
continue this process, until M ≤ p directions have been obtained. In this
manner, partial least squares produces a sequence of derived, orthogonal
inputs or directions z 1 , z 2 , . . . , z M . As with principal-component regres-
sion, if we were to construct all M = p directions, we would get back a
solution equivalent to the usual least squares estimates; using M < p di-
rections produces a reduced regression. The procedure is described fully in
Algorithm 3.3.

> ### Algorithm 3.3 Partial Least Squares.
> 1. Standardize each x j to have mean zero and variance one. Set ŷ (0) =
> (0)
> ȳ1, and x j = x j , j = 1, . . . , p.
> 2. For m = 1, 2, . . . , p
> P p
> (m−1)
> (m−1)
> (a) z m = j=1 φ̂ mj x j
> , where φ̂ mj = hx j
> , yi.
> (b) θ̂ m = hz m , yi/hz m , z m i.
> (c) ŷ (m) = ŷ (m−1) + θ̂ m z m .
> (m−1)
> (d) Orthogonalize each x j
> (m−1)
> [hz m , x j
> i/hz m , z m i]z m ,
> (m)
> with respect to z m : x j
> j = 1, 2, . . . , p.
> (m−1)
> = x j
> −
> 3. Output the sequence of fitted vectors {ŷ (m) } p 1 . Since the {z l } m
> 1 are
> (m)
> pls
> linear in the original x j , so is ŷ
> = X β̂ (m). These linear coeffi-
> cients can be recovered from the sequence of PLS transformations.

In the prostate cancer example, cross-validation chose M = 2 PLS direc-
tions in Figure 3.7. This produced the model given in the rightmost column
of Table 3.3.

What optimization problem is partial least squares solving? Since it uses
the response y to construct its directions, its solution path is a nonlinear
function of y. It can be shown (Exercise 3.15) that partial least squares
seeks directions that have high variance and have high correlation with the
response, in contrast to principal components regression which keys only
on high variance (Stone and Brooks, 1990; Frank and Friedman, 1993). In
particular, the mth principal component direction v m solves:

$$\begin{gather}
\max\_\alpha \text{Var}(\mathbf{X}\alpha) \\\\ \text{subject to }
\\|\alpha\\| = 1, \alpha^T \mathbf{S} v\_l = 0, l = 1,\dots, m-1
\end{gather}\tag{3.63}$$

where S is the sample covariance matrix of the x j . The conditions α T Sv l =
0 ensures that z m = Xα is uncorrelated with all the previous linear com-
binations z l = Xv l . The mth PLS direction φ̂ m solves:

$$\begin{gather}
\max\_\alpha \text{Corr}^2(y, \mathbf{X}\alpha)
\text{Var}(\mathbf{X}\alpha) \\\\ \text{subject to }
\\|\alpha\\| = 1, \alpha^T \mathbf{S} v\_l = 0, l = 1,\dots, m-1
\end{gather}\tag{3.63}$$

Further analysis reveals that the variance aspect tends to dominate, and
so partial least squares behaves much like ridge regression and principal
components regression. We discuss this further in the next section.

If the input matrix X is orthogonal, then partial least squares finds the
least squares estimates after m = 1 steps. Subsequent steps have no effect
since the φ̂ mj are zero for m > 1 (Exercise 3.14). It can also be shown that
the sequence of PLS coefficients for m = 1, 2, . . . , p represents the conjugate
gradient sequence for computing the least squares solutions (Exercise 3.18).