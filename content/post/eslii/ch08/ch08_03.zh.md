+++
title = "ESL-8.3 贝叶斯方法"
summary = """
统计学习基础（译注）第八章第三节，第 267-270 页。
"""

date = 2018-12-14T11:40:00+08:00
lastmod = 2018-12-14T11:40:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In the Bayesian approach to inference, we specify a sampling model Pr(Z|θ)
(density or probability mass function) for our data given the parameters,
and a prior distribution for the parameters Pr(θ) reflecting our knowledge
about θ before we see the data. We then compute the posterior distribution

$$\text{Pr}(\theta|\mathbf{Z}) = \frac
{\text{Pr}(\mathbf{Z}|\theta) \cdot \text{Pr}(\theta)}
{\int \text{Pr}(\mathbf{Z}|\theta) \cdot \text{Pr}(\theta) d\theta}
\tag{8.23}$$

which represents our updated knowledge about θ after we see the data. To
understand this posterior distribution, one might draw samples from it or
summarize by computing its mean or mode. The Bayesian approach differs
from the standard (“frequentist”) method for inference in its use of a prior
distribution to express the uncertainty present before seeing the data, and
to allow the uncertainty remaining after seeing the data to be expressed in
the form of a posterior distribution.

The posterior distribution also provides the basis for predicting the values
of a future observation z new , via the predictive distribution:

$$\text{Pr}(z^\text{new}|\mathbf{Z}) = \int
\text{Pr}(z^\text{new}|\theta) \cdot \text{Pr}(\theta|\mathbf{Z}) d\theta
\tag{8.24}$$

In contrast, the maximum likelihood approach would use Pr(z new | θ̂),
the data density evaluated at the maximum likelihood estimate, to predict
future data. Unlike the predictive distribution (8.24), this does not account
for the uncertainty in estimating θ.

Let’s walk through the Bayesian approach in our smoothing example.
We start with the parametric model given by equation (8.5), and assume
for the moment that σ 2 is known. We assume that the observed feature
values x 1 , x 2 , . . . , x N are fixed, so that the randomness in the data comes
solely from y varying around its mean μ(x).

The second ingredient we need is a prior distribution. Distributions on
functions are fairly complex entities: one approach is to use a Gaussian
process prior in which we specify the prior covariance between any two
function values μ(x) and μ(x ′ ) (Wahba, 1990; Neal, 1996).

Here we take a simpler route: by considering a finite B-spline basis for
μ(x), we can instead provide a prior for the coefficients β, and this implicitly
defines a prior for μ(x). We choose a Gaussian prior centered at zero

$$\beta \sim \mathcal{N}(0, \tau\mathbf{\Sigma})\tag{8.25}$$

with the choices of the prior correlation matrix Σ and variance τ to be
discussed below. The implicit process prior for μ(x) is hence Gaussian,
with covariance kernel

$$\begin{align} K(x,x') &=
\text{cov}[\mu(x), \mu(x')] \\\\ &=
\tau \cdot h(x)^T \mathbf{\Sigma} h(x')
\tag{8.26}\end{align}$$

The posterior distribution for β is also Gaussian, with mean and covariance

$$\begin{align}
E(\beta|\mathbf{Z}) &= \left( \mathbf{H}^T\mathbf{H} +
\frac{\sigma^2}{\tau}\mathbf{\Sigma}^{-1} \right)^{-1}
\mathbf{H}^T \mathbf{y} \\\\ \text{cov}(\beta|\mathbf{Z}) &=
\left( \mathbf{H}^T\mathbf{H} +
\frac{\sigma^2}{\tau}\mathbf{\Sigma}^{-1} \right)^{-1} \sigma^2
\end{align}\tag{8.27}$$

with the corresponding posterior values for μ(x),

$$\begin{align}
E(\mu(x)|\mathbf{Z}) &= h(x)^T \left( \mathbf{H}^T\mathbf{H} +
\frac{\sigma^2}{\tau}\mathbf{\Sigma}^{-1} \right)^{-1}
\mathbf{H}^T \mathbf{y} \\\\ \text{cov}[\mu(x),\mu(x')|\mathbf{Z}] &=
h(x)^T \left( \mathbf{H}^T\mathbf{H} +
\frac{\sigma^2}{\tau}\mathbf{\Sigma}^{-1} \right)^{-1} h(x')\sigma^2
\end{align}\tag{8.28}$$

How do we choose the prior correlation matrix Σ? In some settings the
prior can be chosen from subject matter knowledge about the parameters.
Here we are willing to say the function μ(x) should be smooth, and have
guaranteed this by expressing μ in a smooth low-dimensional basis of B-
splines. Hence we can take the prior correlation matrix to be the identity
Σ = I. When the number of basis functions is large, this might not be suf-
ficient, and additional smoothness can be enforced by imposing restrictions
on Σ; this is exactly the case with smoothing splines (Section 5.8.1).

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_03.png"
  title="**图8.3**："
>}}
{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_04.png"
  title="**图8.4**："
>}}

Figure 8.3 shows ten draws from the corresponding prior for μ(x). To
generate posterior values of the function μ(x), we generate values β ′ from its
posterior (8.27), giving corresponding posterior value μ ′ (x) = 1 β j ′ h j (x).
Ten such posterior curves are shown in Figure 8.4. Two different values
were used for the prior variance τ , 1 and 1000. Notice how similar the
right panel looks to the bootstrap distribution in the bottom left panel
of Figure 8.2 on page 263. This similarity is no accident. As τ → ∞, the
posterior distribution (8.27) and the bootstrap distribution (8.7) coincide.
On the other hand, for τ = 1, the posterior curves μ(x) in the left panel
of Figure 8.4 are smoother than the bootstrap curves, because we have
imposed more prior weight on smoothness.

The distribution (8.25) with τ → ∞ is called a noninformative prior for
θ. In Gaussian models, maximum likelihood and parametric bootstrap anal-
yses tend to agree with Bayesian analyses that use a noninformative prior
for the free parameters. These tend to agree, because with a constant prior,
the posterior distribution is proportional to the likelihood. This correspon-
dence also extends to the nonparametric case, where the nonparametric
bootstrap approximates a noninformative Bayes analysis; Section 8.4 has
the details.

of Figure 8.2 on page 263. This similarity is no accident. As τ → ∞, the
posterior distribution (8.27) and the bootstrap distribution (8.7) coincide.
On the other hand, for τ = 1, the posterior curves μ(x) in the left panel
of Figure 8.4 are smoother than the bootstrap curves, because we have
imposed more prior weight on smoothness.
The distribution (8.25) with τ → ∞ is called a noninformative prior for
θ. In Gaussian models, maximum likelihood and parametric bootstrap anal-
yses tend to agree with Bayesian analyses that use a noninformative prior
for the free parameters. These tend to agree, because with a constant prior,
the posterior distribution is proportional to the likelihood. This correspon-
dence also extends to the nonparametric case, where the nonparametric
bootstrap approximates a noninformative Bayes analysis; Section 8.4 has
the details.

We have, however, done some things that are not proper from a Bayesian
point of view. We have used a noninformative (constant) prior for σ 2 and
replaced it with the maximum likelihood estimate σ̂ 2 in the posterior. A
more standard Bayesian analysis would also put a prior on σ (typically
g(σ) ∝ 1/σ), calculate a joint posterior for μ(x) and σ, and then integrate
out σ, rather than just extract the maximum of the posterior distribution
(“MAP” estimate).

We have, however, done some things that are not proper from a Bayesian
point of view. We have used a noninformative (constant) prior for σ 2 and
replaced it with the maximum likelihood estimate σ̂ 2 in the posterior. A
more standard Bayesian analysis would also put a prior on σ (typically
g(σ) ∝ 1/σ), calculate a joint posterior for μ(x) and σ, and then integrate
out σ, rather than just extract the maximum of the posterior distribution
(“MAP” estimate).