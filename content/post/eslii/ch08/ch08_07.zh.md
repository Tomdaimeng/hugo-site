+++
title = "ESL-8.7 Bagging"
summary = """
统计学习基础（译注）第八章第七节，第 282-288 页。
"""

date = 2018-12-18T13:45:00+08:00
lastmod = 2018-12-18T13:45:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Earlier we introduced the bootstrap as a way of assessing the accuracy of a
parameter estimate or a prediction. Here we show how to use the bootstrap
to improve the estimate or prediction itself. In Section 8.4 we investigated
the relationship between the bootstrap and Bayes approaches, and found
that the bootstrap mean is approximately a posterior average. Bagging
further exploits this connection.

Consider first the regression problem. Suppose we fit a model to our
training data Z = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x N , y N )}, obtaining the predic-
tion f ˆ (x) at input x. Bootstrap aggregation or bagging averages this predic-
tion over a collection of bootstrap samples, thereby reducing its variance.
For each bootstrap sample Z ∗b , b = 1, 2, . . . , B, we fit our model, giving
prediction f ˆ ∗b (x). The bagging estimate is defined by

$$\hat{f}\_\text{bag}(x) = \frac{1}{B} \sum\_{b=1}^B \hat{f}^{\*b}(x)
\tag{8.51}$$

Denote by P̂ the empirical distribution putting equal probability 1/N on
each of the data points (x i , y i ). In fact the “true” bagging estimate is
defined by E P̂ f ˆ ∗ (x), where Z ∗ = (x ∗ 1 , y 1 ∗ ), (x ∗ 2 , y 2 ∗ ), . . . , (x ∗ N , y N) and each
(x i , y i ) ∼ P̂. Expression (8.51) is a Monte Carlo estimate of the true
bagging estimate, approaching it as B → ∞.

The bagged estimate (8.51) will differ from the original estimate f ˆ (x)
only when the latter is a nonlinear or adaptive function of the data. For
example, to bag the B-spline smooth of Section 8.2.1, we average the curves
in the bottom left panel of Figure 8.2 at each value of x. The B-spline
smoother is linear in the data if we fix the inputs; hence if we sample using
the parametric bootstrap in equation (8.6), then f ˆ bag (x) → f ˆ (x) as B → ∞
(Exercise 8.4). Hence bagging just reproduces the original smooth in the
top left panel of Figure 8.2. The same is approximately true if we were to
bag using the nonparametric bootstrap.

A more interesting example is a regression tree, where f ˆ (x) denotes the
tree’s prediction at input vector x (regression trees are described in Chap-
ter 9). Each bootstrap tree will typically involve different features than the
original, and might have a different number of terminal nodes. The bagged
estimate is the average prediction at x from these B trees.

Now suppose our tree produces a classifier Ĝ(x) for a K-class response.
Here it is useful to consider an underlying indicator-vector function f ˆ (x),
with value a single one and K − 1 zeroes, such that Ĝ(x) = arg max k f ˆ (x).
Then the bagged estimate f ˆ bag (x) (8.51) is a K-vector [p 1 (x), p 2 (x), . . . ,
p K (x)], with p k (x) equal to the proportion of trees predicting class k at x.
The bagged classifier selects the class with the most “votes” from the B
trees, Ĝ bag (x) = arg max k f ˆ bag (x).

Often we require the class-probability estimates at x, rather than the
classifications themselves. It is tempting to treat the voting proportions
p k (x) as estimates of these probabilities. A simple two-class example shows
that they fail in this regard. Suppose the true probability of class 1 at x is
0.75, and each of the bagged classifiers accurately predict a 1. Then p 1 (x) =
1, which is incorrect. For many classifiers Ĝ(x), however, there is already
an underlying function f ˆ (x) that estimates the class probabilities at x (for
trees, the class proportions in the terminal node). An alternative bagging
strategy is to average these instead, rather than the vote indicator vectors.
Not only does this produce improved estimates of the class probabilities,
but it also tends to produce bagged classifiers with lower variance, especially
for small B (see Figure 8.10 in the next example).

### 8.7.1 Example: Trees with Simulated Data

We generated a sample of size N = 30, with two classes and p = 5 features,
each having a standard Gaussian distribution with pairwise correlation
0.95. The response Y was generated according to Pr(Y = 1|x 1 ≤ 0.5) = 0.2,
Pr(Y = 1|x 1 > 0.5) = 0.8. The Bayes error is 0.2. A test sample of size 2000
was also generated from the same population. We fit classification trees to
the training sample and to each of 200 bootstrap samples (classification
trees are described in Chapter 9). No pruning was used. Figure 8.9 shows
the original tree and eleven bootstrap trees. Notice how the trees are all
different, with different splitting features and cutpoints. The test error for
the original tree and the bagged tree is shown in Figure 8.10. In this ex-
ample the trees have high variance due to the correlation in the predictors.
Bagging succeeds in smoothing out this variance and hence reducing the
test error.

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_09.png"
  title="**图8.9**："
>}}
{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_10.png"
  title="**图8.10**："
>}}

Bagging can dramatically reduce the variance of unstable procedures
like trees, leading to improved prediction. A simple argument shows why
bagging helps under squared-error loss, in short because averaging reduces
variance and leaves bias unchanged.

Assume our training observations (x i , y i ), i = 1, . . . , N are indepen-
dently drawn from a distribution P, and consider the ideal aggregate es-
timator f ag (x) = E P f ˆ ∗ (x). Here x is fixed and the bootstrap dataset Z ∗
consists of observations x ∗ i , y i ∗ , i = 1, 2, . . . , N sampled from P. Note that
f ag (x) is a bagging estimate, drawing bootstrap samples from the actual
population P rather than the data. It is not an estimate that we can use
in practice, but is convenient for analysis. We can write

$$\begin{align}
E\_\mathcal{P}[Y-\hat{f}^\*(x)]^2 &=
E\_\mathcal{P}[Y - f\_{ag}(x) + f\_{ag}(x) - \hat{f}^\*(x)]^2 \\\\ &=
E\_\mathcal{P}[Y - f\_{ag}(x)]^2 +
E\_\mathcal{P}[\hat{f}^\*(x) - f\_{ag}(x)]^2 \\\\ & \geq
E\_\mathcal{P}[Y - f\_{ag}(x)]^2
\tag{8.52}\end{align}$$

The extra error on the right-hand side comes from the variance of f ˆ ∗ (x)
around its mean f ag (x). Therefore true population aggregation never in-
creases mean squared error. This suggests that bagging—drawing samples
from the training data— will often decrease mean-squared error.

The above argument does not hold for classification under 0-1 loss, be-
cause of the nonadditivity of bias and variance. In that setting, bagging a
good classifier can make it better, but bagging a bad classifier can make it
worse. Here is a simple example, using a randomized rule. Suppose Y = 1
for all x, and the classifier Ĝ(x) predicts Y = 1 (for all x) with proba-
bility 0.4 and predicts Y = 0 (for all x) with probability 0.6. Then the
misclassification error of Ĝ(x) is 0.6 but that of the bagged classifier is 1.0.

For classification we can understand the bagging effect in terms of a
consensus of independent weak learners (Dietterich, 2000a). Let the Bayes
optimal decision at x be G(x) = 1 in a two-class example. Suppose each
of the weak learners G ∗ b have an error-rate e b = e < 0.5, and let S 1 (x) =
b=1 I(G b (x) = 1) be the consensus vote for class 1. Since the weak learn-
ers are assumed to be independent, S 1 (x) ∼ Bin(B, 1 − e), and Pr(S 1 >
B/2) → 1 as B gets large. This concept has been popularized outside of
statistics as the “Wisdom of Crowds” (Surowiecki, 2004) — the collective
knowledge of a diverse and independent body of people typically exceeds
the knowledge of any single individual, and can be harnessed by voting.
Of course, the main caveat here is “independent,” and bagged trees are
not. Figure 8.11 illustrates the power of a consensus vote in a simulated
example, where only 30\% of the voters have some knowledge.

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_11.png"
  title="**图8.11**："
>}}

In Chapter 15 we see how random forests improve on bagging by reducing
the correlation between the sampled trees.

Note that when we bag a model, any simple structure in the model is
lost. As an example, a bagged tree is no longer a tree. For interpretation
of the model this is clearly a drawback. More stable procedures like near-
est neighbors are typically not affected much by bagging. Unfortunately,
the unstable models most helped by bagging are unstable because of the
emphasis on interpretability, and this is lost in the bagging process.
Figure 8.12 shows an example where bagging doesn’t help. The 100 data
points shown have two features and two classes, separated by the gray
linear boundary x 1 + x 2 = 1. We choose as our classifier Ĝ(x) a single
axis-oriented split, choosing the split along either x 1 or x 2 that produces
the largest decrease in training misclassification error.

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_12.png"
  title="**图8.12**："
>}}

The decision boundary obtained from bagging the 0-1 decision rule over
B = 50 bootstrap samples is shown by the blue curve in the left panel.
It does a poor job of capturing the true boundary. The single split rule,
derived from the training data, splits near 0 (the middle of the range of x 1
or x 2 ), and hence has little contribution away from the center. Averaging
the probabilities rather than the classifications does not help here. Bagging
estimates the expected class probabilities from the single split rule, that is,
averaged over many replications. Note that the expected class probabilities
computed by bagging cannot be realized on any single replication, in the
same way that a woman cannot have 2.4 children. In this sense, bagging
increases somewhat the space of models of the individual base classifier.
However, it doesn’t help in this and many other examples where a greater
enlargement of the model class is needed. “Boosting” is a way of doing this
and is described in Chapter 10. The decision boundary in the right panel is
the result of the boosting procedure, and it roughly captures the diagonal
boundary.