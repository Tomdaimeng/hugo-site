+++
title = "ESL-8.6 MCMC for Sampling from the Posterior"
summary = """
统计学习基础（译注）第八章第六节，第 279-282 页。
"""

date = 2018-12-18T10:45:00+08:00
lastmod = 2018-12-18T10:45:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Having defined a Bayesian model, one would like to draw samples from
the resulting posterior distribution, in order to make inferences about the
parameters. Except for simple models, this is often a difficult computa-
tional problem. In this section we discuss the Markov chain Monte Carlo
(MCMC) approach to posterior sampling. We will see that Gibbs sampling,
an MCMC procedure, is closely related to the EM algorithm: the main dif-
ference is that it samples from the conditional distributions rather than
maximizing over them.

Consider first the following abstract problem. We have random variables
U 1 , U 2 , . . . , U K and we wish to draw a sample from their joint distribution.
Suppose this is difficult to do, but it is easy to simulate from the conditional
distributions Pr(U j |U 1 , U 2 , . . . , U j−1 , U j+1 , . . . , U K ), j = 1, 2, . . . , K. The
Gibbs sampling procedure alternatively simulates from each of these distri-
butions and when the process stabilizes, provides a sample from the desired
joint distribution. The procedure is defined in Algorithm 8.3.

----------

#### Algorithm 8.3 Gibbs Sampler.
1. Take some initial values U k , k = 1, 2, . . . , K.
2. Repeat for t = 1, 2, . . . , . :
   For k = 1, 2, . . . , K generate U k from
   $$\text{Pr}(U\_k^{(t)}|U\_1^{(t)},\dots,U\_{k-1}^{(t)},
   U\_{k+1}^{(t)},\dots,U\_{K}^{(t)})$$
3. Continue step 2 until the joint distribution of (U 1 , U 2 , . . . , U K )
does not change.

----------

Under regularity conditions it can be shown that this procedure even-
tually stabilizes, and the resulting random variables are indeed a sample
from the joint distribution of U 1 , U 2 , . . . , U K . This occurs despite the fact
that the samples (U 1 , U 2 , . . . , U K ) are clearly not independent for dif-
ferent t. More formally, Gibbs sampling produces a Markov chain whose
stationary distribution is the true joint distribution, and hence the term
“Markov chain Monte Carlo.” It is not surprising that the true joint dis-
tribution is stationary under this process, as the successive steps leave the
marginal distributions of the U k ’s unchanged.

Note that we don’t need to know the explicit form of the conditional
densities, but just need to be able to sample from them. After the procedure
reaches stationarity, the marginal density of any subset of the variables
can be approximated by a density estimate applied to the sample values.
However if the explicit form of the conditional density Pr(U k , |U l , l 6 = k)
is available, a better estimate of say the marginal density of U k can be
obtained from (Exercise 8.3):

$$\widehat{\text{Pr}}\_{U\_k}(u) = \frac{1}{M-m+1}
\sum\_{t=m}^M \text{Pr}(u|U\_l^{(t)}, l\ne k) \tag{8.50}$$

Here we have averaged over the last M − m + 1 members of the sequence,
to allow for an initial “burn-in” period before stationarity is reached.

Now getting back to Bayesian inference, our goal is to draw a sample from
the joint posterior of the parameters given the data Z. Gibbs sampling will
be helpful if it is easy to sample from the conditional distribution of each
parameter given the other parameters and Z. An example—the Gaussian
mixture problem—is detailed next.

There is a close connection between Gibbs sampling from a posterior and
the EM algorithm in exponential family models. The key is to consider the
latent data Z m from the EM procedure to be another parameter for the
Gibbs sampler. To make this explicit for the Gaussian mixture problem,
we take our parameters to be (θ, Z m ). For simplicity we fix the variances
σ 1 2 , σ 2 2 and mixing proportion π at their maximum likelihood values so that
the only unknown parameters in θ are the means μ 1 and μ 2 . The Gibbs
sampler for the mixture problem is given in Algorithm 8.4. We see that
steps 2(a) and 2(b) are the same as the E and M steps of the EM pro-
cedure, except that we sample rather than maximize. In step 2(a), rather
than compute the maximum likelihood responsibilities γ i = E(∆ i |θ, Z),
the Gibbs sampling procedure simulates the latent data ∆ i from the distri-
butions Pr(∆ i |θ, Z). In step 2(b), rather than compute the maximizers of
the posterior Pr(μ 1 , μ 2 , ∆|Z) we simulate from the conditional distribution
Pr(μ 1 , μ 2 |∆, Z).

----------

#### Algorithm 8.4 Gibbs sampling for mixtures.
1. Take some initial values θ (0) = (μ 1 , μ 2 ).
2. Repeat for t = 1, 2, . . . , .
  1. For i = 1, 2, . . . , N generate ∆ i∈ {0, 1} with Pr(∆ i= 1) =
    γ̂ i (θ (t) ), from equation (8.42).
  2. Set
    $$\begin{align}
    \hat{\mu}\_1 &= \frac{\sum\_{i=1}^N (1-\Delta\_i^{(t)}) \cdot y\_i}
    {\sum\_{i=1}^N (1-\Delta\_i^{(t)})} \\\\ \hat{\mu}\_2 &=
    \frac{\sum\_{i=1}^N \Delta\_i^{(t)} \cdot y\_i}
    {\sum\_{i=1}^N \Delta\_i^{(t)}}
    \end{align}$$
    and generate μ 1 ∼ N (μ̂ 1 , σ̂ 1 2 ) and μ 2 ∼ N (μ̂ 2 , σ̂ 2 2 ).
3. Continue step 2 until the joint distribution of (∆ (t) , μ 1 , μ 2 ) doesn’t
  change

----------

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_08.png"
  title="**图8.8**："
>}}
Figure 8.8 shows 200 iterations of Gibbs sampling, with the mean param-
eters μ 1 (lower) and μ 2 P(upper) shown in the left panel, and the proportion
of class 2 observations i ∆ i /N on the right. Horizontal broken lines have
been drawn at the maximum likelihood estimate values μ̂ 1 , μ̂ 2 and i γ̂ i /N
in each case. The values seem to stabilize quite quickly, and are distributed
evenly around the maximum likelihood values.

The above mixture model was simplified, in order to make the clear
connection between Gibbs sampling and the EM algorithm. More realisti-
cally, one would put a prior distribution on the variances σ 1 2 , σ 2 2 and mixing
proportion π, and include separate Gibbs sampling steps in which we sam-
ple from their posterior distributions, conditional on the other parameters.
One can also incorporate proper (informative) priors for the mean param-
eters. These priors must not be improper as this will lead to a degenerate
posterior, with all the mixing weight on one component.

Gibbs sampling is just one of a number of recently developed procedures
for sampling from posterior distributions. It uses conditional sampling of
each parameter given the rest, and is useful when the structure of the prob-
lem makes this sampling easy to carry out. Other methods do not require
such structure, for example the Metropolis–Hastings algorithm. These and
other computational Bayesian methods have been applied to sophisticated
learning algorithms such as Gaussian process models and neural networks.
Details may be found in the references given in the Bibliographic Notes at
the end of this chapter.