+++
title = "ESL-8.5 The EM Algorithm"
summary = """
统计学习基础（译注）第八章第五节，第 272-279 页。
"""

date = 2018-12-14T23:50:00+08:00
lastmod = 2018-12-14T23:50:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++


The EM algorithm is a popular tool for simplifying difficult maximum
likelihood problems. We first describe it in the context of a simple mixture
model.

### 8.5.1 Two-Component Mixture Model

In this section we describe a simple mixture model for density estimation,
and the associated EM algorithm for carrying out maximum likelihood
estimation. This has a natural connection to Gibbs sampling methods for
Bayesian inference. Mixture models are discussed and demonstrated in several
other parts of the book, in particular Sections 6.8, 12.7 and 13.2.3.

The left panel of Figure 8.5 shows a histogram of the 20 fictitious data
points in Table 8.1

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_05.png"
  title="**图8.5**："
>}}

<table>
  <caption>
  TABLE 8.1. Twenty fictitious data points used in the two-component mixture
example in Figure 8.5.
  </caption>
  <tr>
    <td>-0.39</td>
    <td>0.12</td>
    <td>0.94</td>
    <td>1.67</td>
    <td>1.76</td>
    <td>2.44</td>
    <td>3.72</td>
    <td>4.28</td>
    <td>4.92</td>
    <td>5.53</td>
  </tr>
  <tr>
    <td>0.06</td>
    <td>0.48</td>
    <td>1.01</td>
    <td>1.68</td>
    <td>1.80</td>
    <td>3.25</td>
    <td>4.12</td>
    <td>4.60</td>
    <td>5.28</td>
    <td>6.22</td>
  </tr>
</table>

We would like to model the density of the data points, and due to the
apparent bi-modality, a Gaussian distribution would not be appropriate.
There seems to be two separate underlying regimes, so instead we model
Y as a mixture of two normal distributions:

$$\begin{align}
Y\_1 &\sim \mathcal{N}(\mu\_1, \sigma\_1^2) \\\\ Y\_2 & \sim
\mathcal{N}(\mu\_2, \sigma\_2^2) \\\\ Y &=
(1 - \Delta) \cdot Y\_1 + \Delta \cdot Y\_2
\end{align}\tag{8.36}$$

where ∆ ∈ {0, 1} with Pr(∆ = 1) = π. This generative representation is
explicit: generate a ∆ ∈ {0, 1} with probability π, and then depending on
the outcome, deliver either Y1 or Y2. Let φθ(x) denote the normal density
with parameters θ = (µ, σ2). Then the density of Y is

$$g\_Y(y) = (1-\pi)\phi\_{\theta\_1}(y) +
\pi\phi\_{\theta\_2}(y)\tag{8.37}$$

Now suppose we wish to fit this model to the data in Figure 8.5 by maximum
likelihood. The parameters are

$$\theta = (\pi, \theta\_1, \theta\_2) =
(\pi, \mu\_1, \sigma\_1^2, \mu\_2, \sigma\_2^2)\tag{8.38}$$

The log-likelihood based on the N training cases is

$$l(\theta;\mathbf{Z}) = \sum\_{i=1}^N
\log[(1-\pi)\phi\_{\theta\_1}(y\_i) + \pi\phi\_{\theta\_2}(y\_i)]
\tag{8.39}$$

Direct maximization of ℓ(θ; Z) is quite difficult numerically, because of
the sum of terms inside the logarithm. There is, however, a simpler approach.
We consider unobserved latent variables ∆i taking values 0 or 1 as
in (8.36): if ∆i = 1 then Yi comes from model 2, otherwise it comes from
model 1. Suppose we knew the values of the ∆i’s. Then the log-likelihood
would be

$$\begin{align}
l\_0(\theta;\mathbf{Z},\mathbf{\Delta}) =& \sum\_{i=1}^N
[(1-\Delta\_i)\log\phi\_{\theta\_1}(y\_i) +
  \Delta\_i\log\phi\_{\theta\_2}(y\_i)] \\\\ &+ \sum\_{i=1}^N
[(1-\Delta\_i)\log(1-\pi) + \Delta\_i\log\pi]
\end{align}\tag{8.40}$$

and the maximum likelihood estimates of µ1 and σ21 would be the sample
mean and variance for those data with ∆i = 0, and similarly those for µ2
and σ22 would be the sample mean and variance of the data with ∆i = 1.
The estimate of π would be the proportion of ∆i = 1.

Since the values of the ∆i’s are actually unknown, we proceed in an
iterative fashion, substituting for each ∆i in (8.40) its expected value

$$\gamma\_i(\theta) = E(\Delta\_i | \theta, \mathbf{Z}) =
\text{Pr}(\Delta\_i = 1 | \theta, \mathbf{Z}) \tag{8.41}$$

also called the responsibility of model 2 for observation i. We use a procedure
called the EM algorithm, given in Algorithm 8.1 for the special case of
Gaussian mixtures. In the expectation step, we do a soft assignment of each
observation to each model: the current estimates of the parameters are used
to assign responsibilities according to the relative density of the training
points under each model. In the maximization step, these responsibilities
are used in weighted maximum-likelihood fits to update the estimates of
the parameters.

A good way to construct initial guesses for ˆµ1 and ˆµ2 is simply to choose
two of the yi at random. Both ˆσ21 and ˆσ22 can be set equal to the overall
sample variance PN i=1(yi −y¯) 2/N. The mixing proportion ˆπ can be started
at the value 0.5.

Note that the actual maximizer of the likelihood occurs when we put a
spike of infinite height at any one data point, that is, ˆµ1 = yi for some
i and ˆσ2 1 = 0. This gives infinite likelihood, but is not a useful solution.
Hence we are actually looking for a good local maximum of the likelihood,
one for which ˆσ21, σˆ22 > 0. To further complicate matters, there can be
more than one local maximum having ˆσ21, σˆ22 > 0. In our example, we
ran the EM algorithm with a number of different initial guesses for the
parameters, all having ˆσ2 k > 0.5, and chose the run that gave us the highest
maximized likelihood. Figure 8.6 shows the progress of the EM algorithm in
maximizing the log-likelihood. Table 8.2 shows ˆπ =Piγˆi/N, the maximum
likelihood estimate of the proportion of observations in class 2, at selected
iterations of the EM procedure.

----------

#### Algorithm 8.1 EM Algorithm for Two-component Gaussian Mixture.
1. Take initial guesses for the parameters
  $\hat{\mu}\_1, \hat{\sigma}\_1^2,
  \hat{\mu}\_2, \hat{\sigma}\_2^2, \hat{\pi}$ (see text).
2. Expectation Step: compute the responsibilities
  $$\hat{\gamma}\_i = \frac{\hat{\pi}\phi\_{\hat{\theta}\_2}(y\_i)}
  {(1-\hat{\pi})\phi\_{\hat{\theta}\_1}(y\_i) +
  \hat{\pi}\phi\_{\hat{\theta}\_2}(y\_i)}, i=1,2,\dots,N \tag{8.42}$$
3. Maximization Step: compute the weighted means and variances:
  $$\begin{matrix}
  \hat{\mu}\_1 = \frac
    {\sum\_1^N (1-\hat{\gamma}\_i)y\_i}
    {\sum\_{i=1}^N (1-\hat{\gamma}\_i)} &
  \hat{\sigma}\_1^2 = \frac
    {\sum\_{i=1}^N (1-\hat{\gamma}\_i)(y\_i-\hat{\mu}\_1)^2}
    {\sum\_{i=1}^N (1-\hat{\gamma}\_i)} \\\\ \hat{\mu}\_2 =
  \frac{\sum\_{i=1}^N \hat{\gamma}\_i y\_i}
    {\sum\_{i=1}^N \hat{\gamma}\_i} &
  \hat{\sigma}\_2^2 = \frac
    {\sum\_{i=1}^N \hat{\gamma}\_i (y\_i-\hat{\mu}\_2)^2}
    {\sum\_{i=1}^N \hat{\gamma}\_i}
  \end{matrix}$$
  and the mixing probability
  $\hat{\pi} = \sum\_{i=1}^N \hat{\gamma}\_i / N$.
4. Iterate steps 2 and 3 until convergence.

----------

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_06.png"
  title="**图8.6**："
>}}


| Iteration | $\pi$ |
|-----------|-------|
| 1         | 0.485 |
| 5         | 0.493 |
| 10        | 0.523 |
| 15        | 0.544 |
| 20        | 0.546 |
TABLE 8.2. Selected iterations of the EM algorithm for mixture example.



The final maximum likelihood estimates are

$$\begin{matrix}
\hat{\mu}\_1 = 4.62 & & \hat{\sigma}\_1^2 = 0.87
\\\\ \hat{\mu}\_2 = 1.06 & & \hat{\sigma}\_2^2 = 0.77
\\\\ & \hat{\pi} = 0.546 &
\end{matrix}$$

The right panel of Figure 8.5 shows the estimated Gaussian mixture density
from this procedure (solid red curve), along with the responsibilities (dotted
green curve). Note that mixtures are also useful for supervised learning; in
Section 6.7 we show how the Gaussian mixture model leads to a version of
radial basis functions.

### 8.5.2 The EM Algorithm in General :scream:

The above procedure is an example of the EM (or Baum–Welch) algorithm
for maximizing likelihoods in certain classes of problems. These problems
are ones for which maximization of the likelihood is difficult, but made
easier by enlarging the sample with latent (unobserved) data. This is called
data augmentation. Here the latent data are the model memberships ∆i.
In other problems, the latent data are actual data that should have been
observed but are missing.

----------

#### Algorithm 8.2 EM Algorithm
1. Start with initial guesses for the parameters ˆθ(0).
2. Expectation Step: at the jth step, compute
  $$Q(\theta', \hat{\theta}^{(j)}) =
  E(l\_0(\theta';\mathbf{T})|\mathbf{Z}, \hat{\theta}^{(j)})
  \tag{8.43}$$
  as a function of the dummy argument θ′.
3. Maximization Step: determine the new estimate ˆθ(j+1) as the maximizer
  of Q(θ′, ˆθ(j)) over θ′.
4. Iterate steps 2 and 3 until convergence.

----------


Algorithm 8.2 gives the general formulation of the EM algorithm. Our
observed data is Z, having log-likelihood ℓ(θ; Z) depending on parameters
θ. The latent or missing data is Zm, so that the complete data is T =
(Z, Zm) with log-likelihood ℓ0(θ; T), ℓ0 based on the complete density. In
the mixture problem (Z, Zm) = (y, ∆), and ℓ0(θ; T) is given in (8.40).

In our mixture example, E(ℓ0(θ′; T)|Z,ˆθ(j)) is simply (8.40) with the ∆i
replaced by the responsibilities ˆγi(ˆθ), and the maximizers in step 3 are just
weighted means and variances.

We now give an explanation of why the EM algorithm works in general.

Since

$$\text{Pr}(\mathbf{Z}^m|\mathbf{Z}, \theta') = \frac
{\text{Pr}(\mathbf{Z}^m, \mathbf{Z} | \theta')}
{\text{Pr}(\mathbf{Z} | \theta')} \tag{8.44}$$

we can write

$$\text{Pr}(\mathbf{Z} | \theta') = \frac
{\text{Pr}(\mathbf{T} | \theta')}
{\text{Pr}(\mathbf{Z}^m | \mathbf{Z}, \theta')} \tag{8.45}$$

In terms of log-likelihoods, we have ℓ(θ′; Z) = ℓ0(θ′; T)−ℓ1(θ′; Zm|Z), where
ℓ1 is based on the conditional density Pr(Zm|Z, θ′). Taking conditional
expectations with respect to the distribution of T|Z governed by parameter
θ gives

$$\begin{align}
l(\theta'; \mathbf{Z}) &=
E[l\_0(\theta'; \mathbf{T})|\mathbf{Z}, \theta)] +
E[l\_0(\theta'; \mathbf{Z}^m | \mathbf{Z})|\mathbf{Z}, \theta)]
\\\\ &\equiv Q(\theta', \theta) − R(\theta', \theta).
\tag{8.46}\end{align}$$

In the M step, the EM algorithm maximizes Q(θ′, θ) over θ′, rather than
the actual objective function ℓ(θ′; Z). Why does it succeed in maximizing
ℓ(θ′; Z)? Note that R(θ∗, θ) is the expectation of a log-likelihood of a density
(indexed by θ∗), with respect to the same density indexed by θ, and hence
(by Jensen’s inequality) is maximized as a function of θ∗, when θ∗ = θ (see
Exercise 8.1). So if θ′ maximizes Q(θ′, θ), we see that

$$\begin{align}
l(\theta'; \mathbf{Z}) - l(\theta; \mathbf{Z}) &=
[Q(\theta', \theta) - Q(\theta, \theta)] −
[R(\theta', \theta) - R(\theta, \theta)] \\\\ & \geq 0
\tag{8.47}\end{align}$$

Hence the EM iteration never decreases the log-likelihood.

This argument also makes it clear that a full maximization in the M
step is not necessary: we need only to find a value ˆθ(j+1) so that Q(θ′,ˆθ(j))
increases as a function of the first argument, that is, Q(ˆθ(j+1),ˆθ(j)) >
Q(ˆθ(j),ˆθ(j)). Such procedures are called GEM (generalized EM) algorithms.
The EM algorithm can also be viewed as a minorization procedure: see
Exercise 8.7.

### 8.5.3 EM as a Maximization–Maximization Procedure :scream:

Here is a different view of the EM procedure, as a joint maximization
algorithm. Consider the function

$$F(\theta', \tilde{P}) =
E\_{\tilde{P}}[l\_0(\theta'; \mathbf{T})] −
E \_{\tilde{P}}[\log\tilde{P}(\mathbf{Z}^m)] \tag{8.48}$$

Here P˜(Zm) is any distribution over the latent data Zm. In the mixture
example, P˜(Zm) comprises the set of probabilities γi = Pr(∆i = 1|θ, Z).
Note that F evaluated at P˜(Zm) = Pr(Zm|Z, θ′), is the log-likelihood of
the observed data, from (8.46)1. The function F expands the domain of
the log-likelihood, to facilitate its maximization.

The EM algorithm can be viewed as a joint maximization method for F
over θ′ and P˜(Zm), by fixing one argument and maximizing over the other.
The maximizer over P˜(Zm) for fixed θ′ can be shown to be

$$\tilde{P}(\mathbf{Z}^m) =
\text{Pr}(\mathbf{Z}^m | \mathbf{Z}, \theta') \tag{8.49}$$

(Exercise 8.2). This is the distribution computed by the E step, for example,
(8.42) in the mixture example. In the M step, we maximize F(θ′, P˜) over θ′
with P˜ fixed: this is the same as maximizing the first term EP˜[ℓ0(θ′; T)|Z, θ]
since the second term does not involve θ′.

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_07.png"
  title="**图8.7**："
>}}

Finally, since F(θ′, P˜) and the observed data log-likelihood agree when
P˜(Zm) = Pr(Zm|Z, θ′), maximization of the former accomplishes maximization
of the latter. Figure 8.7 shows a schematic view of this process.
This view of the EM algorithm leads to alternative maximization proce
dures. For example, one does not need to maximize with respect to all of
the latent data parameters at once, but could instead maximize over one
of them at a time, alternating with the M step.