+++
title = "ESL-8.8 Model Averaging and Stacking"
summary = """
统计学习基础（译注）第八章第八节，第 288-290 页。
"""

date = 2018-12-18T13:55:00+08:00
lastmod = 2018-12-18T13:55:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In Section 8.4 we viewed bootstrap values of an estimator as approximate
posterior values of a corresponding parameter, from a kind of nonparamet-
ric Bayesian analysis. Viewed in this way, the bagged estimate (8.51) is
an approximate posterior Bayesian mean. In contrast, the training sample
estimate f ˆ (x) corresponds to the mode of the posterior. Since the posterior
mean (not mode) minimizes squared-error loss, it is not surprising that
bagging can often reduce mean squared-error.

Here we discuss Bayesian model averaging more generally. We have a
set of candidate models M m , m = 1, . . . , M for our training set Z. These
models may be of the same type with different parameter values (e.g.,
subsets in linear regression), or different models for the same task (e.g.,
neural networks and regression trees).

Suppose ζ is some quantity of interest, for example, a prediction f (x) at
some fixed feature value x. The posterior distribution of ζ is

$$\text{Pr}(\zeta|\mathbf{Z}) = \sum\_{m=1}^M
\text{Pr}(\zeta|\mathcal{M}\_m,\mathbf{Z})
\text{Pr}(\mathcal{M}\_m|\mathbf{Z}) \tag{8.53}$$

with posterior mean

$$E[\zeta|\mathbf{Z}] = \sum\_{m=1}^M
E(\zeta|\mathcal{M}\_m, \mathbf{Z})
\text{Pr}(\mathcal{M}\_m|\mathbf{Z}) \tag{8.54}$$

This Bayesian prediction is a weighted average of the individual predictions,
with weights proportional to the posterior probability of each model.

This formulation leads to a number of different model-averaging strate-
gies. Committee methods take a simple unweighted average of the predic-
tions from each model, essentially giving equal probability to each model.
More ambitiously, the development in Section 7.7 shows the BIC criterion
can be used to estimate posterior model probabilities. This is applicable
in cases where the different models arise from the same parametric model,
with different parameter values. The BIC gives weight to each model de-
pending on how well it fits and how many parameters it uses. One can also
carry out the Bayesian recipe in full. If each model M m has parameters
θ m , we write

$$\begin{align} & \text{Pr}(\mathcal{M}\_m|\mathbf{Z}) \propto 
\text{Pr}(\mathcal{M}\_m) \cdot
\text{Pr}(\mathbf{Z} | \mathcal{M}\_m) \\\\ & \propto
\text{Pr}(\mathcal{M}\_m) \cdot
\int \text{Pr}(\mathbf{Z} | \theta\_m, \mathcal{M}\_m)
\text{Pr}(\theta\_m | \mathcal{M}\_m) d\theta\_m
\tag{8.55}\end{align}$$

In principle one can specify priors Pr(θ m |M m ) and numerically com-
pute the posterior probabilities from (8.55), to be used as model-averaging
weights. However, we have seen no real evidence that this is worth all of
the effort, relative to the much simpler BIC approximation.

How can we approach model averaging from a frequentist viewpoint?
Given predictions f ˆ 1 (x), f ˆ 2 (x), . . . , f ˆ M (x), under squared-error loss, we can
seek the weights w = (w 1 , w 2 , . . . , w M ) such that

$$\hat{w} = \underset{w}{\text{argmin}} E\_\mathcal{P} \left[
Y - \sum\_{m=1}^M w\_m \hat{f}\_m(x)
\right]^2 \tag{8.56}$$

Here the input value x is fixed and the N observations in the dataset Z (and
the target Y ) are distributed according to P. The solution is the population
linear regression of Y on F̂ (x) T ≡ [ f ˆ 1 (x), f ˆ 2 (x), . . . , f ˆ M (x)]:

$$\hat{w} = E\_\mathcal{P}[\hat{F}(x)\hat{F}(x)^T]^{-1}
E\_\mathcal{P}[\hat{F}(x)Y] \tag{8.57}$$

Now the full regression has smaller error than any single model

$$E\_\mathcal{P}
\left[Y - \sum\_{m=1}^M \hat{w}\_m \hat{f}\_m(x)\right]^2 \leq 
E\_\mathcal{P} \left[Y - \hat{f}\_m(x)\right]^2 \forall m \tag{8.58}$$

so combining models never makes things worse, at the population level.

Of course the population linear regression (8.57) is not available, and it
is natural to replace it with the linear regression over the training set. But
there are simple examples where this does not work well. For example, if
f ˆ m (x), m = 1, 2, . . . , M represent the prediction from the best subset of
inputs of size m among M total inputs, then linear regression would put all
of the weight on the largest model, that is, ŵ M = 1, ŵ m = 0, m < M . The
problem is that we have not put each of the models on the same footing
by taking into account their complexity (the number of inputs m in this
example).

Stacked generalization, or stacking, is a way of doing this. Let f ˆ m
be the prediction at x, using model m, applied to the dataset with the
ith training observation removed. The stacking estimate of the weights is
obtained from the least squares linear regression of y i on f ˆ m(x i ), m =
1, 2, . . . , M . In detail the stacking weights are given by

$$\hat{w}^\text{st} = \underset{w}{\text{argmin}} \sum\_{i=1}^N \left[
y\_i - \sum\_{m=1}^M w\_m \hat{f}^{-i}\_m(x\_i) \right]^2
\tag{8.59}$$

The final prediction is m ŵ m f m (x). By using the cross-validated pre-
dictions f ˆ m(x), stacking avoids giving unfairly high weight to models with
higher complexity. Better results can be obtained by restricting the weights
to be nonnegative, and to sum to 1. This seems like a reasonable restriction
if we interpret the weights as posterior model probabilities as in equation
(8.54), and it leads to a tractable quadratic programming problem.

There is a close connection between stacking and model selection via
leave-one-out cross-validation (Section 7.10). If we restrict the minimization
in (8.59) to weight vectors w that have one unit weight and the rest zero,
this leads to a model choice m̂ with smallest leave-one-out cross-validation
error. Rather than choose a single model, stacking combines them with
estimated optimal weights. This will often lead to better prediction, but
less interpretability than the choice of only one of the M models.

The stacking idea is actually more general than described above. One
can use any learning method, not just linear regression, to combine the
models as in (8.59); the weights could also depend on the input location
x. In this way, learning methods are “stacked” on top of one another, to
improve prediction performance.