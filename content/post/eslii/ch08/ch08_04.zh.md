+++
title = "ESL-8.4 è‡ªåŠ©æ³•ä¸è´å¶æ–¯æ¨æ–­çš„å…³ç³» ğŸ˜±"
summary = """
ç»Ÿè®¡å­¦ä¹ åŸºç¡€ï¼ˆè¯‘æ³¨ï¼‰ç¬¬å…«ç« ç¬¬å››èŠ‚ï¼Œç¬¬ 271-272 é¡µã€‚
"""

date = 2018-12-14T23:10:00+08:00
lastmod = 2018-12-14T23:10:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["è¯‘æ–‡"]
categories = ["ç»Ÿè®¡å­¦ä¹ åŸºç¡€ï¼ˆè¯‘æ³¨ï¼‰"]

[header]
image = ""
caption = ""
preview = true
+++

Consider first a very simple example, in which we observe a single obser-
vation z from a normal distribution

$$z \sim \mathcal{N}(\theta, 1) \tag{8.29}$$

To carry out a Bayesian analysis for Î¸, we need to specify a prior. The
most convenient and common choice would be Î¸ âˆ¼ N (0, Ï„ ) giving posterior
distribution

$$\theta|z \sim \mathcal{N}\left(
\frac{z}{1+1/\tau}, \frac{1}{1+1/\tau}
\right)\tag{8.30}$$

Now the larger we take Ï„ , the more concentrated the posterior becomes
around the maximum likelihood estimate Î¸Ì‚ = z. In the limit as Ï„ â†’ âˆ we
obtain a noninformative (constant) prior, and the posterior distribution is

$$\theta|z \sim \mathcal{N}(z,1) \tag{8.31}$$

This is the same as a parametric bootstrap distribution in which we gen-
erate bootstrap values z âˆ— from the maximum likelihood estimate of the
sampling density N (z, 1).
There are three ingredients that make this correspondence work:

1. The choice of noninformative prior for Î¸.
2. The dependence of the log-likelihood l(Î¸; Z) on the data Z only
through the maximum likelihood estimate Î¸Ì‚. Hence we can write the
log-likelihood as l(Î¸; Î¸Ì‚).
3. The symmetry of the log-likelihood in Î¸ and Î¸Ì‚, that is, l(Î¸; Î¸Ì‚) =
l( Î¸Ì‚; Î¸) + constant.

Properties (2) and (3) essentially only hold for the Gaussian distribu-
tion. However, they also hold approximately for the multinomial distribu-
tion, leading to a correspondence between the nonparametric bootstrap
and Bayes inference, which we outline next.

Assume that we have a discrete sample space with L categories. Let w j be
the probability that a sample point falls in category j, and Åµ j the observed
proportion in category j. Let w = (w 1 , w 2 , . . . , w L ), Åµ = ( Åµ 1 , Åµ 2 , . . . , Åµ L ).
Denote our estimator by S( Åµ); take as a prior distribution for w a sym-
metric Dirichlet distribution with parameter a:

$$w \sim \text{Di}\_L(a1) \tag{8.32}$$

that is, the prior probability mass function is proportional to
Then the posterior density of w is

$$w \sim \text{Di}\_L(a1 + N\hat{w}) \tag{8.33}$$

where N is the sample size. Letting a â†’ 0 to obtain a noninformative prior
gives

$$w \sim \text{Di}\_L(N\hat{w}) \tag{8.34}$$

Now the bootstrap distribution, obtained by sampling with replacement
from the data, can be expressed as sampling the category proportions from
a multinomial distribution. Specifically,

$$N\hat{w} \sim \text{Mult}(N, \hat{w}) \tag{8.35}$$

where Mult(N, Åµ) denotes a multinomial distribution, having probability
mass function N Åµ âˆ— ,...,N    This distribution is similar to the pos-
terior distribution above, having the same support, same mean, and nearly
the same covariance matrix. Hence the bootstrap distribution of S( Åµ âˆ— ) will
closely approximate the posterior distribution of S(w).

In this sense, the bootstrap distribution represents an (approximate)
nonparametric, noninformative posterior distribution for our parameter.
But this bootstrap distribution is obtained painlesslyâ€”without having to
formally specify a prior and without having to sample from the posterior
distribution. Hence we might think of the bootstrap distribution as a â€œpoor
manâ€™sâ€ Bayes posterior. By perturbing the data, the bootstrap approxi-
mates the Bayesian effect of perturbing the parameters, and is typically
much simpler to carry out.