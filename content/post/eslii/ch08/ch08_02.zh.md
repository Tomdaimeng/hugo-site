+++
title = "ESL-8.2 The Bootstrap and Maximum Likelihood Methods"
summary = """
统计学习基础（译注）第八章第二节，第 261-267 页。
"""

date = 2018-12-13T16:05:00+08:00
lastmod = 2018-12-13T16:05:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

### 8.2.1 A Smoothing Example

The bootstrap method provides a direct computational way of assessing
uncertainty, by sampling from the training data. Here we illustrate the
bootstrap in a simple one-dimensional smoothing problem, and show its
connection to maximum likelihood.

Denote the training data by Z = {z 1 , z 2 , . . . , z N }, with z i = (x i , y i ),
i = 1, 2, . . . , N . Here x i is a one-dimensional input, and y i the outcome,
either continuous or categorical. As an example, consider the N = 50 data
points shown in the left panel of Figure 8.1.

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_01.png"
  title="**图8.1**："
>}}


Suppose we decide to fit a cubic spline to the data, with three knots
placed at the quartiles of the X values. This is a seven-dimensional lin-
ear space of functions, and can be represented, for example, by a linear
expansion of B-spline basis functions (see Section 5.9.2):

$$\mu(x) = \sum\_{j=1}^7 \beta\_j h\_j(x) \tag{8.1}$$

Here the h j (x), j = 1, 2, . . . , 7 are the seven functions shown in the right
panel of Figure 8.1. We can think of μ(x) as representing the conditional
mean E(Y |X = x).

Let H be the N × 7 matrix with ijth element h j (x i ). The usual estimate
of β, obtained by minimizing the squared error over the training set, is
given by

$$\hat{\beta} = (\mathbf{H}^T\mathbf{H})^{-1}\mathbf{H}^T\mathbf{y}
\tag{8.2}$$

The corresponding fit μ̂(x) = j=1 β̂ j h j (x) is shown in the top left panel
of Figure 8.2.

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_02.png"
  title="**图8.2**："
>}}


The estimated covariance matrix of β̂ is

$$\widehat{\text{Var}}(\hat{\beta}) =
(\mathbf{H}^T\mathbf{H})^{-1} \hat{\sigma}^2 \tag{8.3}$$

where we have estimated the noise variance by σ̂ 2 = i=1 (y i − μ̂(x i )) 2 /N .
Letting h(x) T = (h 1 (x), h 2 (x), . . . , h 7 (x)), the standard error of a predic-
tion μ̂(x) = h(x) T β̂ is

$$\widehat{\text{se}}[\hat{\mu}(x)] =
[h(x)^T(\mathbf{H}^T\mathbf{H})^{-1}h(x)]^{\frac{1}{2}}\hat{\sigma}
\tag{8.4}$$

In the top right panel of Figure 8.2 we have plotted μ̂(x) ± 1.96 · se[μ̂(x)].
Since 1.96 is the 97.5% point of the standard normal distribution, these
represent approximate 100 − 2 × 2.5% = 95% pointwise confidence bands
for μ(x).

Here is how we could apply the bootstrap in this example. We draw B
datasets each of size N = 50 with replacement from our training data, the
sampling unit being the pair z i = (x i , y i ). To each bootstrap dataset Z ∗
we fit a cubic spline μ̂ ∗ (x); the fits from ten such samples are shown in the
bottom left panel of Figure 8.2. Using B = 200 bootstrap samples, we can
form a 95% pointwise confidence band from the percentiles at each x: we
find the 2.5% × 200 = fifth largest and smallest values at each x. These are
plotted in the bottom right panel of Figure 8.2. The bands look similar to
those in the top right, being a little wider at the endpoints.

There is actually a close connection between the least squares estimates
(8.2) and (8.3), the bootstrap, and maximum likelihood. Suppose we further
assume that the model errors are Gaussian,

$$\begin{align}
Y &= \mu(X) + \varepsilon;
\varepsilon \sim \mathcal{N}(0, \sigma^2) \\\\ \mu(x) &=
\sum\_{j=1}^7 \beta\_j h\_j(x)
\tag{8.5}\end{align}$$

The bootstrap method described above, in which we sample with re-
placement from the training data, is called the nonparametric bootstrap.
This really means that the method is “model-free,” since it uses the raw
data, not a specific parametric model, to generate new datasets. Consider
a variation of the bootstrap, called the parametric bootstrap, in which we
simulate new responses by adding Gaussian noise to the predicted values:

$$y\_i^\* = \hat{\mu}(x\_i) + \varepsilon\_i^\*;
\varepsilon\_i^\* \sim \mathcal{N}(0, \hat{\sigma}^2);
i = 1,2,\dots,N \tag{8.6}$$

This process is repeated B times, where B = 200 say. The resulting boot-
strap datasets have the form (x 1 , y 1 ∗ ), . . . , (x N , y N) and we recompute the
B-spline smooth on each. The confidence bands from this method will ex-
actly equal the least squares bands in the top right panel, as the number of
bootstrap samples goes to infinity. A function estimated from a bootstrap
sample y ∗ is given by μ̂ ∗ (x) = h(x) T (H T H) −1 H T y ∗ , and has distribution

$$\hat{\mu}^\*(x) \sim \mathcal{N}(\hat{\mu}(x),
h(x)^T(\mathbf{H}^T\mathbf{H})^{-1}h(x)\hat{\sigma}^2) \tag{8.7}$$

Notice that the mean of this distribution is the least squares estimate, and
the standard deviation is the same as the approximate formula (8.4).

### 8.2.2 Maximum Likelihood Inference

It turns out that the parametric bootstrap agrees with least squares in the
previous example because the model (8.5) has additive Gaussian errors. In
general, the parametric bootstrap agrees not with least squares but with
maximum likelihood, which we now review.

We begin by specifying a probability density or probability mass function
for our observations

$$z\_i \sim g\_\theta(z) \tag{8.8}$$

In this expression θ represents one or more unknown parameters that gov-
ern the distribution of Z. This is called a parametric model for Z. As an
example, if Z has a normal distribution with mean μ and variance σ 2 , then

$$\theta = (\mu, \sigma^2) \tag{8.9}$$

and

$$g\_theta(z) = \frac{1}{\sqrt{2\pi}\sigma}
e^{-\frac{1}{2} (z-\mu)^2 / \sigma^2} \tag{8.10}$$

Maximum likelihood is based on the likelihood function, given by

$$L(\theta; \mathbf{Z}) = \prod\_{i=1}^N g\_\theta(z\_i) \tag{8.11}$$

the probability of the observed data under the model g θ . The likelihood is
defined only up to a positive multiplier, which we have taken to be one.
We think of L(θ; Z) as a function of θ, with our data Z fixed.
Denote the logarithm of L(θ; Z) by

$$\begin{align}
l(\theta; \mathbf{Z}) &=\sum\_{i=1}^N l(\theta; z\_i) \\\\ &=
\sum\_{i=1}^N \log g\_\theta(z\_i)
\tag{8.12}\end{align}$$

which we will sometimes abbreviate as l(θ). This expression is called the
log-likelihood, and each value l(θ; z i ) = log g θ (z i ) is called a log-likelihood
component. The method of maximum likelihood chooses the value θ = θ̂
to maximize l(θ; Z).

The likelihood function can be used to assess the precision of θ̂. We need
a few more definitions. The score function is defined by

$$\dot{l}(\theta; \mathbf{Z}) = \sum\_{i=1}^N \dot{l}(\theta; z\_i)
\tag{8.13}$$

where l̇(θ; z i ) = ∂l(θ; z i )/∂θ. Assuming that the likelihood takes its maxi-
mum in the interior of the parameter space, l̇( θ̂; Z) = 0. The information
matrix is

$$\mathbf{I}(\theta) = -\sum\_{i=1}^N
\frac{\partial^2 l(\theta; z\_i)}{\partial\theta\partial\theta^T}
\tag{8.14}$$

When I(θ) is evaluated at θ = θ̂, it is often called the observed information.
The Fisher information (or expected information) is

$$\mathbf{i}(\theta) = E\_\theta[\mathbf{I}(\theta)] \tag{8.15}$$

Finally, let θ 0 denote the true value of θ.

A standard result says that the sampling distribution of the maximum
likelihood estimator has a limiting normal distribution

$$\hat{\theta} \rightarrow \mathcal{N}
(\theta\_0, \mathbf{i}(\hat{\theta}\_0)^{-1}) \tag{8.16}$$

as N → ∞. Here we are independently sampling from g θ 0 (z). This suggests
that the sampling distribution of θ̂ may be approximated by

$$\mathcal{N}(\hat{\theta}, \mathbf{i}(\hat{\theta})^{-1}) \text{ 或 }
\mathcal{N}(\hat{\theta}, \mathbf{I}(\hat{\theta})^{-1}) \tag{8.17}$$

where θ̂ represents the maximum likelihood estimate from the observed
data.

The corresponding estimates for the standard errors of θ̂ j are obtained
from

$$\sqrt{\mathbf{i}(\hat{\theta})\_{jj}^{-1}} \text{ 和 }
\sqrt{\mathbf{I}(\hat{\theta})\_{jj}^{-1}} \tag{8.18}$$

Confidence points for θ j can be constructed from either approximation
in (8.17). Such a confidence point has the form

$$\theta\_j -
z^{(1-\alpha)} \cdot \sqrt{\mathbf{i}(\hat{\theta})\_{jj}^{-1}}
\text{ 或 }
\theta\_j -
z^{(1-\alpha)} \cdot \sqrt{\mathbf{I}(\hat{\theta})\_{jj}^{-1}}$$

respectively, where z (1−α) is the 1 − α percentile of the standard normal
distribution. More accurate confidence intervals can be derived from the
likelihood function, by using the chi-squared approximation

$$2[l(\hat{\theta}) − l(\theta\_0)] \sim \chi^2\_p \tag{8.19}$$

where p is the number of components in θ. The resulting 1 − 2α confi-
dence interval is the set of all θ 0 such that 2[l( θ̂) − l(θ 0 )] ≤ χ 2 p
where χ 2 p is the 1 − 2α percentile of the chi-squared distribution with
p degrees of freedom.

Let’s return to our smoothing example to see what maximum likelihood
yields. The parameters are θ = (β, σ 2 ). The log-likelihood is

$$l(\theta) = -\frac{N}{2} \log \sigma^2 2\pi
-\frac{1}{2\sigma^2} \sum\_{i=1}^N (y\_i - h(x\_i)^T\beta)^2
\tag{8.20}$$

The maximum likelihood estimate is obtained by setting ∂l/∂β = 0 and
∂l/∂σ 2 = 0, giving

$$\begin{align}
\hat{\beta} &=
(\mathbf{H}^T\mathbf{H})^{-1}\mathbf{H}^T\mathbf{y} \\\\ \hat{\sigma}^2
&= \frac{1}{N} \sum (y\_i - \hat{\mu}(x\_i))^2
\end{align}\tag{8.21}$$

which are the same as the usual estimates given in (8.2) and below (8.3).

The information matrix for θ = (β, σ 2 ) is block-diagonal, and the block
corresponding to β is

$$\mathbf{I}(\beta) = (\mathbf{H}^T\mathbf{H}) / \sigma^2 \tag{8.22}$$

so that the estimated variance (H T H) −1 σ̂ 2 agrees with the least squares
estimate (8.3).

### 8.2.3 Bootstrap versus Maximum Likelihood

In essence the bootstrap is a computer implementation of nonparametric or
parametric maximum likelihood. The advantage of the bootstrap over the
maximum likelihood formula is that it allows us to compute maximum like-
lihood estimates of standard errors and other quantities in settings where
no formulas are available.

In our example, suppose that we adaptively choose by cross-validation
the number and position of the knots that define the B-splines, rather
than fix them in advance. Denote by λ the collection of knots and their
positions. Then the standard errors and confidence bands should account
for the adaptive choice of λ, but there is no way to do this analytically.
With the bootstrap, we compute the B-spline smooth with an adaptive
choice of knots for each bootstrap sample. The percentiles of the resulting
curves capture the variability from both the noise in the targets as well as
that from λ̂. In this particular example the confidence bands (not shown)
don’t look much different than the fixed λ bands. But in other problems,
where more adaptation is used, this can be an important effect to capture.