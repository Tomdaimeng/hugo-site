+++
title = "ESL-8.9 Stochastic Search: Bumping"
summary = """
统计学习基础（译注）第八章第九节，第 290-292 页。
"""

date = 2018-12-19T10:00:00+08:00
lastmod = 2018-12-19T13:00:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The final method described in this chapter does not involve averaging or
combining models, but rather is a technique for finding a better single
model. Bumping uses bootstrap sampling to move randomly through model
space. For problems where fitting method finds many local minima, bump-
ing can help the method to avoid getting stuck in poor solutions.

As in bagging, we draw bootstrap samples and fit a model to each. But
rather than average the predictions, we choose the model estimated from a
bootstrap sample that best fits the training data. In detail, we draw boot-
strap samples Z ∗1 , . . . , Z ∗B and fit our model to each, giving predictions
f ˆ ∗b (x), b = 1, 2, . . . , B at input point x. We then choose the model that
produces the smallest prediction error, averaged over the original training
set. For squared error, for example, we choose the model obtained from
bootstrap sample b̂, where

$$\hat{b} = \underset{b}{\text{argmin}} \sum\_{i=1}^N
[y\_i - \hat{f}^{\*b}(x\_i)]^2 \tag{8.60}$$

The corresponding model predictions are f ˆ ∗ b̂ (x). By convention we also
include the original training sample in the set of bootstrap samples, so that
the method is free to pick the original model if it has the lowest training
error.

By perturbing the data, bumping tries to move the fitting procedure
around to good areas of model space. For example, if a few data points are
causing the procedure to find a poor solution, any bootstrap sample that
omits those data points should procedure a better solution.

{{< figure
  src="http://public.guansong.wang/eslii/ch08/eslii_fig_08_13.png"
  title="**图8.13**："
>}}

For another example, consider the classification data in Figure 8.13, the
notorious exclusive or (XOR) problem. There are two classes (blue and
orange) and two input features, with the features exhibiting a pure inter-
action. By splitting the data at x 1 = 0 and then splitting each resulting
strata at x 2 = 0, (or vice versa) a tree-based classifier could achieve per-
fect discrimination. However, the greedy, short-sighted CART algorithm
(Section 9.2) tries to find the best split on either feature, and then splits
the resulting strata. Because of the balanced nature of the data, all initial
splits on x 1 or x 2 appear to be useless, and the procedure essentially gener-
ates a random split at the top level. The actual split found for these data is
shown in the left panel of Figure 8.13. By bootstrap sampling from the data,
bumping breaks the balance in the classes, and with a reasonable number
of bootstrap samples (here 20), it will by chance produce at least one tree
with initial split near either x 1 = 0 or x 2 = 0. Using just 20 bootstrap
samples, bumping found the near optimal splits shown in the right panel
of Figure 8.13. This shortcoming of the greedy tree-growing algorithm is
exacerbated if we add a number of noise features that are independent of
the class label. Then the tree-growing algorithm cannot distinguish x 1 or
x 2 from the others, and gets seriously lost.

Since bumping compares different models on the training data, one must
ensure that the models have roughly the same complexity. In the case of
trees, this would mean growing trees with the same number of terminal
nodes on each bootstrap sample. Bumping can also help in problems where
it is difficult to optimize the fitting criterion, perhaps because of a lack of
smoothness. The trick is to optimize a different, more convenient criterion
over the bootstrap samples, and then choose the model producing the best
results for the desired criterion on the training sample.