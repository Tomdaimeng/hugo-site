+++
title = "ESL-11.7 Example: ZIP Code Data"
summary = """
统计学习基础（译注）第十一章第七节，第 404-408 页。
"""

date = 2019-02-17T20:59:00+08:00
lastmod = 2019-02-17T20:59:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

This example is a character recognition task: classification of handwritten
numerals. This problem captured the attention of the machine learning and
neural network community for many years, and has remained a benchmark
problem in the field. Figure 11.9 shows some examples of normalized hand-
written digits, automatically scanned from envelopes by the U.S. Postal
Service. The original scanned digits are binary and of different sizes and
orientations; the images shown here have been deslanted and size normal-
ized, resulting in 16 × 16 grayscale images (Le Cun et al., 1990). These 256
pixel values are used as inputs to the neural network classifier.

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_09.png"
  title="**图11.9**："
>}}
Examples of training cases from ZIP code data. Each image is
a 16 × 16 8-bit grayscale representation of a handwritten digit.

A black box neural network is not ideally suited to this pattern recogni-
tion task, partly because the pixel representation of the images lack certain
invariances (such as small rotations of the image). Consequently early at-
tempts with neural networks yielded misclassification rates around 4.5%
on various examples of the problem. In this section we show some of the
pioneering efforts to handcraft the neural network to overcome some these
deficiencies (Le Cun, 1989), which ultimately led to the state of the art in
neural network performance(Le Cun et al., 1998)[^1] .

Although current digit datasets have tens of thousands of training and
test examples, the sample size here is deliberately modest in order to em-
phasize the effects. The examples were obtained by scanning some actual
hand-drawn digits, and then generating additional images by random hor-
izontal shifts. Details may be found in Le Cun (1989). There are 320 digits
in the training set, and 160 in the test set.

Five different networks were fit to the data:
- Net-1: No hidden layer, equivalent to multinomial logistic regression.
- Net-2: One hidden layer, 12 hidden units fully connected.
- Net-3: Two hidden layers locally connected.
- Net-4: Two hidden layers, locally connected with weight sharing.
- Net-5: Two hidden layers, locally connected, two levels of weight sharing.

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_10.png"
  title="**图11.10**："
>}}
Architecture of the five networks used in the ZIP code example.

These are depicted in Figure 11.10. Net-1 for example has 256 inputs, one
each for the 16 × 16 input pixels, and ten output units for each of the digits
0–9. The predicted value f ˆ k (x) represents the estimated probability that
an image x has digit class k, for k = 0, 1, 2, . . . , 9.

The networks all have sigmoidal output units, and were all fit with the
sum-of-squares error function. The first network has no hidden layer, and
hence is nearly equivalent to a linear multinomial regression model (Exer-
cise 11.4). Net-2 is a single hidden layer network with 12 hidden units, of
the kind described above.

The training set error for all of the networks was 0%, since in all cases
there are more parameters than training observations. The evolution of the
test error during the training epochs is shown in Figure 11.11. The linear
network (Net-1) starts to overfit fairly quickly, while test performance of
the others level off at successively superior values.

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_11.png"
  title="**图11.11**："
>}}
Test performance curves, as a function of the number of train-
ing epochs, for the five networks of Table 11.1 applied to the ZIP code data.
(Le Cun, 1989)

The other three networks have additional features which demonstrate
the power and flexibility of the neural network paradigm. They introduce
constraints on the network, natural for the problem at hand, which allow
for more complex connectivity but fewer parameters.

Net-3 uses local connectivity: this means that each hidden unit is con-
nected to only a small patch of units in the layer below. In the first hidden
layer (an 8 × 8 array), each unit takes inputs from a 3× 3 patch of the input
layer; for units in the first hidden layer that are one unit apart, their recep-
tive fields overlap by one row or column, and hence are two pixels apart.
In the second hidden layer, inputs are from a 5 × 5 patch, and again units
that are one unit apart have receptive fields that are two units apart. The
weights for all other connections are set to zero. Local connectivity makes
each unit responsible for extracting local features from the layer below, and
reduces considerably the total number of weights. With many more hidden
units than Net-2, Net-3 has fewer links and hence weights (1226 vs. 3214),
and achieves similar performance.

Net-4 and Net-5 have local connectivity with shared weights. All units
in a local feature map perform the same operation on different parts of the
image, achieved by sharing the same weights. The first hidden layer of Net-
4 has two 8×8 arrays, and each unit takes input from a 3×3 patch just like
in Net-3. However, each of the units in a single 8 × 8 feature map share the
same set of nine weights (but have their own bias parameter). This forces
the extracted features in different parts of the image to be computed by
the same linear functional, and consequently these networks are sometimes
known as convolutional networks. The second hidden layer of Net-4 has
no weight sharing, and is the same as in Net-3. The gradient of the error
function R with respect to a shared weight is the sum of the gradients of
R with respect to each connection controlled by the weights in question.

|   | Network Architecture | Links | Weights | % Correct |
|---|----------------------|-------|---------|-----------|
| Net-1: | Single layer network  | 2570 | 2570 | 80.0% |
| Net-2: | Two layer network     | 3214 | 3214 | 87.0% |
| Net-3: | Locally connected     | 1226 | 1226 | 88.5% |
| Net-4: | Constrained network 1 | 2266 | 1132 | 94.0% |
| Net-5: | Constrained network 2 | 5194 | 1060 | 98.4% |
**Table 11.1:** Test set performance of five different neural networks on a hand-
written digit classification example (Le Cun, 1989).

Table 11.1 gives the number of links, the number of weights and the
optimal test performance for each of the networks. We see that Net-4 has
more links but fewer weights than Net-3, and superior test performance.
Net-5 has four 4 × 4 feature maps in the second hidden layer, each unit
connected to a 5 × 5 local patch in the layer below. Weights are shared
in each of these feature maps. We see that Net-5 does the best, having
errors of only 1.6%, compared to 13% for the “vanilla” network Net-2.
The clever design of network Net-5, motivated by the fact that features of
handwriting style should appear in more than one part of a digit, was the
result of many person years of experimentation. This and similar networks
gave better performance on ZIP code problems than any other learning
method at that time (early 1990s). This example also shows that neural
networks are not a fully automatic tool, as they are sometimes advertised.
As with all statistical models, subject matter knowledge can and should be
used to improve their performance.

This network was later outperformed by the tangent distance approach
(Simard et al., 1993) described in Section 13.3.3, which explicitly incorpo-
rates natural affine invariances. At this point the digit recognition datasets
become test beds for every new learning procedure, and researchers worked
hard to drive down the error rates. As of this writing, the best error rates on
a large database (60, 000 training, 10, 000 test observations), derived from
standard NIST[^2] databases, were reported to be the following: (Le Cun et
al., 1998):

- 1.1% for tangent distance with a 1-nearest neighbor classifier (Sec-
tion 13.3.3);
- 0.8% for a degree-9 polynomial SVM (Section 12.3);
- 0.8% for LeNet-5, a more complex version of the convolutional net-
work described here;
- 0.7% for boosted LeNet-4. Boosting is described in Chapter 8. LeNet-
4 is a predecessor of LeNet-5.

Le Cun et al. (1998) report a much larger table of performance results, and
it is evident that many groups have been working very hard to bring these
test error rates down. They report a standard error of 0.1% on the error
estimates, which is based on a binomial average with N = 10, 000 and
p ≈ 0.01. This implies that error rates within 0.1—0.2% of one another
are statistically equivalent. Realistically the standard error is even higher,
since the test data has been implicitly used in the tuning of the various
procedures.

[^1]: The figures and tables in this example were recreated from Le Cun (1989).
[^2]: The National Institute of Standards and Technology maintain large databases, including handwritten character databases; http://www.nist.gov/srd/.