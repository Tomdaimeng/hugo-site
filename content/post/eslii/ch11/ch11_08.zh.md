+++
title = "ESL-11.8 Discussion"
summary = """
统计学习基础（译注）第十一章第八节，第 408-409 页。
"""

date = 2019-02-17T21:00:00+08:00
lastmod = 2019-02-17T21:00:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Both projection pursuit regression and neural networks take nonlinear func-
tions of linear combinations (“derived features”) of the inputs. This is a
powerful and very general approach for regression and classification, and
has been shown to compete well with the best learning methods on many
problems.

These tools are especially effective in problems with a high signal-to-noise
ratio and settings where prediction without interpretation is the goal. They
are less effective for problems where the goal is to describe the physical pro-
cess that generated the data and the roles of individual inputs. Each input
enters into the model in many places, in a nonlinear fashion. Some authors
(Hinton, 1989) plot a diagram of the estimated weights into each hidden
unit, to try to understand the feature that each unit is extracting. This
is limited however by the lack of identifiability of the parameter vectors
α m , m = 1, . . . , M . Often there are solutions with α m spanning the same
linear space as the ones found during training, giving predicted values that
are roughly the same. Some authors suggest carrying out a principal com-
ponent analysis of these weights, to try to find an interpretable solution. In
general, the difficulty of interpreting these models has limited their use in
fields like medicine, where interpretation of the model is very important.

There has been a great deal of research on the training of neural net-
works. Unlike methods like CART and MARS, neural networks are smooth
functions of real-valued parameters. This facilitates the development of
Bayesian inference for these models. The next sections discusses a success-
ful Bayesian implementation of neural networks.