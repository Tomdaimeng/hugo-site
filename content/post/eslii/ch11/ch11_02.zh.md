+++
title = "ESL-11.2 Projection Pursuit Regression"
summary = """
统计学习基础（译注）第十一章第二节，第 389-392 页。
"""

date = 2019-02-15T19:10:00+08:00
lastmod = 2019-02-15T19:10:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

As in our generic supervised learning problem, assume we have an input
vector X with p components, and a target Y . Let ω m , m = 1, 2, . . . , M, be
unit p-vectors of unknown parameters. The projection pursuit regression
(PPR) model has the form

$$f(X) = \sum\_{m=1}^M g\_m(w\_m^T X) \tag{11.1}$$

This is an additive model, but in the derived features V m = ω m X rather
than the inputs themselves. The functions g m are unspecified and are esti-
mated along with the directions ω m using some flexible smoothing method
(see below).

The function g m (ω mX) is called a ridge function in IR p . It varies only
in the direction defined by the vector ω m . The scalar variable V m = ω m X
is the projection of X onto the unit vector ω m , and we seek ω m so that
the model fits well, hence the name “projection pursuit.” Figure 11.1 shows
some examples of ridge functions. In the example on the left ω = (1/ 2)(1, 1) T ,
so that the function only varies in the direction X 1 + X 2 . In the example
on the right, ω = (1, 0).

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_01.png"
  title="**图11.1**："
>}}
Perspective plots of two ridge functions.
(Left:) g(V ) = 1/[1 + exp(−5(V − 0.5))], where V = (X 1 + X 2 )/ 2.
(Right:) g(V ) = (V + 0.1) sin(1/(V /3 + 0.1)), where V = X 1 .

The PPR model (11.1) is very general, since the operation of forming
nonlinear functions of linear combinations generates a surprisingly large
class of models. For example, the product X 1 · X 2 can be written as [(X 1 +
X 2 ) 2 − (X 1 − X 2 ) 2 ]/4, and higher-order products can be represented simi-
larly.

In fact, if M is taken arbitrarily large, for appropriate choice of g m the
PPR model can approximate any continuous function in IR p arbitrarily
well. Such a class of models is called a universal approximator. However
this generality comes at a price. Interpretation of the fitted model is usually
difficult, because each input enters into the model in a complex and multi-
faceted way. As a result, the PPR model is most useful for prediction, and
not very useful for producing an understandable model for the data. The
M = 1 model, known as the single index model in econometrics, is an
exception. It is slightly more general than the linear regression model, and
offers a similar interpretation.

How do we fit a PPR model, given training data (x i , y i ), i = 1, 2, . . . , N ?
We seek the approximate minimizers of the error function

$$\sum\_{i=1}^N \left[ y\_i - \sum\_{m=1}^M g\_m(w\_m^T x\_i)
\right]^2 \tag{11.2}$$

over functions g m and direction vectors ω m , m = 1, 2, . . . , M . As in other
smoothing problems, we need either explicitly or implicitly to impose com-
plexity constraints on the g m , to avoid overfit solutions.

Consider just one term (M = 1, and drop the subscript). Given the
direction vector ω, we form the derived variables v i = ω T x i . Then we have
a one-dimensional smoothing problem, and we can apply any scatterplot
smoother, such as a smoothing spline, to obtain an estimate of g.

On the other hand, given g, we want to minimize (11.2) over ω. A Gauss–
Newton search is convenient for this task. This is a quasi-Newton method,
in which the part of the Hessian involving the second derivative of g is
discarded. It can be simply derived as follows. Let ω old be the current
estimate for ω. We write

$$g(w^T x\_i) \approx g(w\_\text{old}^T x\_i) +
g^\prime(w\_\text{old}^T x\_i)(w - w\_\text{old})^T x\_i
\tag{11.3}$$

to give

$$\begin{align}
& \sum\_{i=1}^N [y\_i - g(w^T x\_i)]^2 \\\\ \approx &
\sum\_{i=1}^N g^\prime(w\_\text{old}^T x\_i)^2 \left[
\left(w\_\text{old}^T x\_i +
\frac{y\_i - g(w\_\text{old}^T x\_i)}
{g^\prime(w\_\text{old}^T x\_i)}\right) -w^T x\_i
\right]^2 \tag{11.4}\end{align}$$

To minimize the right-hand side, we carry out a least squares regression
with target ω oldx i +(y i −g(ω oldx i ))/g ′ (ω oldx i ) on the input x i , with weights
g (ω old x i ) and no intercept (bias) term. This produces the updated coef-
ficient vector ω new .

These two steps, estimation of g and ω, are iterated until convergence.
With more than one term in the PPR model, the model is built in a forward
stage-wise manner, adding a pair (ω m , g m ) at each stage.

There are a number of implementation details.

- Although any smoothing method can in principle be used, it is conve-
nient if the method provides derivatives. Local regression and smooth-
ing splines are convenient.
- After each step the g m ’s from previous steps can be readjusted using
the backfitting procedure described in Chapter 9. While this may
lead ultimately to fewer terms, it is not clear whether it improves
prediction performance.
- Usually the ω m are not readjusted (partly to avoid excessive compu-
tation), although in principle they could be as well.
- The number of terms M is usually estimated as part of the forward
stage-wise strategy. The model building stops when the next term
does not appreciably improve the fit of the model. Cross-validation
can also be used to determine M .

There are many other applications, such as density estimation (Friedman
et al., 1984; Friedman, 1987), where the projection pursuit idea can be used.
In particular, see the discussion of ICA in Section 14.7 and its relationship
with exploratory projection pursuit. However the projection pursuit re-
gression model has not been widely used in the field of statistics, perhaps
because at the time of its introduction (1981), its computational demands
exceeded the capabilities of most readily available computers. But it does
represent an important intellectual advance, one that has blossomed in its
reincarnation in the field of neural networks, the topic of the rest of this
chapter.