+++
title = "ESL-11.9 Bayesian Neural Nets and the NIPS 2003 Challenge"
summary = """
统计学习基础（译注）第十一章第九节，第 409-414 页。
"""

date = 2019-02-17T21:34:00+08:00
lastmod = 2019-02-17T21:34:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

A classification competition was held in 2003, in which five labeled train-
ing datasets were provided to participants. It was organized for a Neural
Information Processing Systems (NIPS) workshop. Each of the data sets
constituted a two-class classification problems, with different sizes and from
a variety of domains (see Table 11.2). Feature measurements for a valida-
tion dataset were also available.

Participants developed and applied statistical learning procedures to
make predictions on the datasets, and could submit predictions to a web-
site on the validation set for a period of 12 weeks. With this feedback,
participants were then asked to submit predictions for a separate test set
and they received their results. Finally, the class labels for the validation
set were released and participants had one week to train their algorithms
on the combined training and validation sets, and submit their final pre-
dictions to the competition website. A total of 75 groups participated, with
20 and 16 eventually making submissions on the validation and test sets,
respectively.

There was an emphasis on feature extraction in the competition. Arti-
ficial “probes” were added to the data: these are noise features with dis-
tributions resembling the real features but independent of the class labels.
The percentage of probes that were added to each dataset, relative to the
total set of features, is shown on Table 11.2. Thus each learning algorithm
had to figure out a way of identifying the probes and downweighting or
eliminating them.

| Dataset | Domain | Feature Type | p | Percent Probes | $N\_\text{tr}$ | $N\_\text{val}$ | $N\_\text{te}$ |
|---------|--------|--------------|---|----------------|---------------|-----------------|----------------|
| Arcene  | Mass spectrometry   | Dense  | 10,000  | 30 | 100  | 100  | 700  |
| Dexter  | Text classification | Sparse | 20,000  | 50 | 300  | 300  | 2000 |
| Dorothea| Drug discovery      | Sparse | 100,000 | 50 | 800  | 350  | 800  |
| Gisette | Digit recognition   | Dense  | 5000    | 30 | 6000 | 1000 | 6500 |
| Madelon | Artificial          | Dense  | 500     | 96 | 2000 | 600  | 1800 |
TABLE 11.2. NIPS 2003 challenge data sets. The column labeled p is the number
of features. For the Dorothea dataset the features are binary. N tr , N val and N te
are the number of training, validation and test cases, respectively


A number of metrics were used to evaluate the entries, including the
percentage correct on the test set, the area under the ROC curve, and a
combined score that compared each pair of classifiers head-to-head. The
results of the competition are very interesting and are detailed in Guyon et
al. (2006). The most notable result: the entries of Neal and Zhang (2006)
were the clear overall winners. In the final competition they finished first
in three of the five datasets, and were 5th and 7th on the remaining two
datasets.

In their winning entries, Neal and Zhang (2006) used a series of pre-
processing feature-selection steps, followed by Bayesian neural networks,
Dirichlet diffusion trees, and combinations of these methods. Here we focus
only on the Bayesian neural network approach, and try to discern which
aspects of their approach were important for its success. We rerun their
programs and compare the results to boosted neural networks and boosted
trees, and other related methods.

### 11.9.1 Bayes, Boosting and Bagging

Let us first review briefly the Bayesian approach to inference and its appli-
cation to neural networks. Given training data X tr , y tr , we assume a sam-
pling model with parameters θ; Neal and Zhang (2006) use a two-hidden-
layer neural network, with output nodes the class probabilities Pr(Y |X, θ)
for the binary outcomes. Given a prior distribution Pr(θ), the posterior
distribution for the parameters is

$$\text{Pr}(\theta|\mathbf{X}\_\text{tr}, \mathbf{y}\_\text{tr}) = \frac
{\text{Pr}(\theta)
\text{Pr}(\mathbf{y}\_\text{tr}|\mathbf{X}\_\text{tr}, \theta)}
{\int \text{Pr}(\theta)
\text{Pr}(\mathbf{y}\_\text{tr}|\mathbf{X}\_\text{tr}, \theta) d\theta}
\tag{11.19}$$

For a test case with features X new , the predictive distribution for the
label Y new is

$$\begin{align}
& \text{Pr}(Y\_\text{new} | X\_\text{new},
          \mathbf{X}\_\text{tr}, \mathbf{y}\_{tr})
\\\\ = &\int
\text{Pr}(Y\_\text{new}|X\_\text{new}, \theta)
\text{Pr}(\theta | \mathbf{X}\_\text{tr}, \mathbf{y}\_{tr}) d\theta
\end{align}\tag{11.20}$$

(c.f. equation 8.24). Since the integral in (11.20) is intractable, sophisticated
Markov Chain Monte Carlo (MCMC) methods are used to sample from the
posterior distribution Pr(Y new |X new , X tr , y tr ). A few hundred values θ are
generated and then a simple average of these values estimates the integral.
Neal and Zhang (2006) use diffuse Gaussian priors for all of the parame-
ters. The particular MCMC approach that was used is called hybrid Monte
Carlo, and may be important for the success of the method. It includes
an auxiliary momentum vector and implements Hamiltonian dynamics in
which the potential function is the target density. This is done to avoid
random walk behavior; the successive candidates move across the sample
space in larger steps. They tend to be less correlated and hence converge
to the target distribution more rapidly.

Neal and Zhang (2006) also tried different forms of pre-processing of the
features:

1. univariate screening using t-tests, and
2. automatic relevance determination.

In the latter method (ARD), the weights (coefficients) for the jth feature
to each of the first hidden layer units all share a common prior variance
σ j 2 , and prior mean zero. The posterior distributions for each variance σ j 2
are computed, and the features whose posterior variance concentrates on
small values are discarded.

There are thus three main features of this approach that could be im-
portant for its success:

1. the feature selection and pre-processing,
2. the neural network model, and
3. the Bayesian inference for the model using MCMC.

According to Neal and Zhang (2006), feature screening in (a) is carried
out purely for computational efficiency; the MCMC procedure is slow with
a large number of features. There is no need to use feature selection to avoid
overfitting. The posterior average (11.20) takes care of this automatically.

We would like to understand the reasons for the success of the Bayesian
method. In our view, power of modern Bayesian methods does not lie in
their use as a formal inference procedure; most people would not believe
that the priors in a high-dimensional, complex neural network model are
actually correct. Rather the Bayesian/MCMC approach gives an efficient
way of sampling the relevant parts of model space, and then averaging the
predictions for the high-probability models.

Bagging and boosting are non-Bayesian procedures that have some simi-
larity to MCMC in a Bayesian model. The Bayesian approach fixes the data
and perturbs the parameters, according to current estimate of the poste-
rior distribution. Bagging perturbs the data in an i.i.d fashion and then
re-estimates the model to give a new set of model parameters. At the end,
a simple average of the model predictions from different bagged samples is
computed. Boosting is similar to bagging, but fits a model that is additive
in the models of each individual base learner, which are learned using non
i.i.d. samples. We can write all of these models in the form

$$\hat{f}(\mathbf{x}\_\text{new}) = \sum\_{\ell=1}^L w\_\ell
E(Y\_\text{new} | \mathbf{x}\_\text{new}, \hat{\theta}\_\ell)
\tag{11.21}$$

In all cases the θ̂ l are a large collection of model parameters. For the
Bayesian model the w l = 1/L, and the average estimates the posterior
mean (11.21) by sampling θ l from the posterior distribution. For bagging,
w l = 1/L as well, and the θ̂ l are the parameters refit to bootstrap re-
samples of the training data. For boosting, the weights are all equal to
1, but the θ̂ l are typically chosen in a nonrandom sequential fashion to
constantly improve the fit.

### 11.9.2 Performance Comparisons

Based on the similarities above, we decided to compare Bayesian neural
networks to boosted trees, boosted neural networks, random forests and
bagged neural networks on the five datasets in Table 11.2. Bagging and
boosting of neural networks are not methods that we have previously used
in our work. We decided to try them here, because of the success of Bayesian
neural networks in this competition, and the good performance of bagging
and boosting with trees. We also felt that by bagging and boosting neural
nets, we could assess both the choice of model as well as the model search
strategy.

Here are the details of the learning methods that were compared:

- Bayesian neural nets. The results here are taken from Neal and Zhang
(2006), using their Bayesian approach to fitting neural networks. The
models had two hidden layers of 20 and 8 units. We re-ran some
networks for timing purposes only.

- Boosted trees. We used the gbm package (version 1.5-7) in the R language.
Tree depth and shrinkage factors varied from dataset to dataset. We
consistently bagged 80% of the data at each boosting iteration (the
default is 50%). Shrinkage was between 0.001 and 0.1. Tree depth was
between 2 and 9.

- Boosted neural networks. Since boosting is typically most effective with
“weak” learners, we boosted a single hidden layer neural network with
two or four units, fit with the nnet package (version 7.2-36) in R.

- Random forests. We used the R package randomForest (version 4.5-16)
with default settings for the parameters.

- Bagged neural networks. We used the same architecture as in the Bayesian
neural network above (two hidden layers of 20 and 8 units), fit using
both Neal’s C language package “Flexible Bayesian Modeling” (2004-
11-10 release), and Matlab neural-net toolbox (version 5.1).

This analysis was carried out by Nicholas Johnson, and full details may
be found in Johnson (2008)[^3] . The results are shown in Figure 11.12 and
Table 11.3.

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_12.png"
  title="**图11.12**："
>}}
Performance of different learning methods on five problems,
using both univariate screening of features (top panel) and a reduced feature set
from automatic relevance determination. The error bars at the top of each plot
have width equal to one standard error of the difference between two error rates.
On most of the problems several competitors are within this error bound.

| Method | Screened Features | Screened Features | ARD Reduced Features | ARD Reduced Features |
|-----------------|--------------|--------------|------------|------------|
|  | **Average Rank** | **Average Time** | **Average Rank** | **Average Time** | 
| Bayesian neural networks | 1.5 | 384(138)  | 1.6 | 600(186)   |
| Boosted trees            | 3.4 | 3.03(2.5) | 4.0 | 34.1(32.4) |
| Boosted neural networks  | 3.8 | 9.4(8.6)  | 2.2 | 35.6(33.5) |
| Random forests           | 2.7 | 1.9(1.7)  | 3.2 | 11.2(9.3)  |
| Bagged neural networks   | 3.6 | 3.5(1.1)  | 4.0 | 6.4(4.4)   |
TABLE 11.3. Performance of different methods. Values are average rank of test
error across the five problems (low is good), and mean computation time and
standard error of the mean, in minutes.

The figure and table show Bayesian, boosted and bagged neural networks,
boosted trees, and random forests, using both the screened and reduced
features sets. The error bars at the top of each plot indicate one standard
error of the difference between two error rates. Bayesian neural networks
again emerge as the winner, although for some datasets the differences
between the test error rates is not statistically significant. Random forests
performs the best among the competitors using the selected feature set,
while the boosted neural networks perform best with the reduced feature
set, and nearly match the Bayesian neural net.

The superiority of boosted neural networks over boosted trees suggest
that the neural network model is better suited to these particular prob-
lems. Specifically, individual features might not be good predictors here
and linear combinations of features work better. However the impressive
performance of random forests is at odds with this explanation, and came
as a surprise to us.

Since the reduced feature sets come from the Bayesian neural network
approach, only the methods that use the screened features are legitimate,
self-contained procedures. However, this does suggest that better methods
for internal feature selection might help the overall performance of boosted
neural networks.

The table also shows the approximate training time required for each
method. Here the non-Bayesian methods show a clear advantage.
Overall, the superior performance of Bayesian neural networks here may
be due to the fact that

1. the neural network model is well suited to these five problems, and
2. the MCMC approach provides an efficient way of exploring the im-
portant part of the parameter space, and then averaging the resulting
models according to their quality.

The Bayesian approach works well for smoothly parametrized models like
neural nets; it is not yet clear that it works as well for non-smooth models
like trees.

[^3]: We also thank Isabelle Guyon for help in preparing the results of this section.