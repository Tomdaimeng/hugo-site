+++
title = "ESL-11.5 Some Issues in Training Neural Networks"
summary = """
统计学习基础（译注）第十一章第五节，第 397-401 页。
"""

date = 2019-02-17T13:28:00+08:00
lastmod = 2019-02-17T13:28:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

There is quite an art in training neural networks. The model is generally
overparametrized, and the optimization problem is nonconvex and unstable
unless certain guidelines are followed. In this section we summarize some
of the important issues.

### 11.5.1 Starting Values

Note that if the weights are near zero, then the operative part of the sigmoid
(Figure 11.3) is roughly linear, and hence the neural network collapses into
an approximately linear model (Exercise 11.2). Usually starting values for
weights are chosen to be random values near zero. Hence the model starts
out nearly linear, and becomes nonlinear as the weights increase. Individual
units localize to directions and introduce nonlinearities where needed. Use
of exact zero weights leads to zero derivatives and perfect symmetry, and
the algorithm never moves. Starting instead with large weights often leads
to poor solutions.

### 11.5.2 Overfitting

Often neural networks have too many weights and will overfit the data at
the global minimum of R. In early developments of neural networks, either
by design or by accident, an early stopping rule was used to avoid over-
fitting. Here we train the model only for a while, and stop well before we
approach the global minimum. Since the weights start at a highly regular-
ized (linear) solution, this has the effect of shrinking the final model toward
a linear model. A validation dataset is useful for determining when to stop,
since we expect the validation error to start increasing.

A more explicit method for regularization is weight decay, which is anal-
ogous to ridge regression used for linear models (Section 3.4.1). We add a
penalty to the error function R(θ) + λJ(θ), where

$$J(\theta) = \sum\_{km}\beta\_{km}^2 + \sum\_{m\ell}\alpha\_{m\ell}^2
\tag{11.16}$$

and λ ≥ 0 is a tuning parameter. Larger values of λ will tend to shrink
the weights toward zero: typically cross-validation is used to estimate λ.
The effect of the penalty is to simply add terms 2β km and 2α ml to the
respective gradient expressions (11.13). Other forms for the penalty have
been proposed, for example,

$$J(\theta) = \sum\_{km} \frac{\beta\_{km}^2}{1+\beta\_{km}^2} +
\sum\_{m\ell} \frac{\alpha\_{m\ell}^2}{1+\alpha\_{m\ell}^2}
\tag{11.17}$$

known as the weight elimination penalty. This has the effect of shrinking
smaller weights more than (11.16) does.

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_04.png"
  title="**图11.4**："
>}}
A neural network on the mixture example of Chapter 2. The
upper panel uses no weight decay, and overfits the training data. The lower panel
uses weight decay, and achieves close to the Bayes error rate (broken purple
boundary). Both use the softmax activation function and cross-entropy error.

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_05.png"
  title="**图11.5**："
>}}
Heat maps of the estimated weights from the training of neural
networks from Figure 11.4. The display ranges from bright green (negative) to
bright red (positive).

Figure 11.4 shows the result of training a neural network with ten hidden
units, without weight decay (upper panel) and with weight decay (lower
panel), to the mixture example of Chapter 2. Weight decay has clearly
improved the prediction. Figure 11.5 shows heat maps of the estimated
weights from the training (grayscale versions of these are called Hinton
diagrams.) We see that weight decay has dampened the weights in both
layers: the resulting weights are spread fairly evenly over the ten hidden
units.

### 11.5.3 Scaling of the Inputs

Since the scaling of the inputs determines the effective scaling of the weights
in the bottom layer, it can have a large effect on the quality of the final
solution. At the outset it is best to standardize all inputs to have mean zero
and standard deviation one. This ensures all inputs are treated equally in
the regularization process, and allows one to choose a meaningful range for
the random starting weights. With standardized inputs, it is typical to take
random uniform weights over the range [−0.7, +0.7].

### 11.5.4 Number of Hidden Units and Layers

Generally speaking it is better to have too many hidden units than too few.
With too few hidden units, the model might not have enough flexibility to
capture the nonlinearities in the data; with too many hidden units, the
extra weights can be shrunk toward zero if appropriate regularization is
used. Typically the number of hidden units is somewhere in the range of
5 to 100, with the number increasing with the number of inputs and num-
ber of training cases. It is most common to put down a reasonably large
number of units and train them with regularization. Some researchers use
cross-validation to estimate the optimal number, but this seems unneces-
sary if cross-validation is used to estimate the regularization parameter.
Choice of the number of hidden layers is guided by background knowledge
and experimentation. Each layer extracts features of the input for regres-
sion or classification. Use of multiple hidden layers allows construction of
hierarchical features at different levels of resolution. An example of the
effective use of multiple layers is given in Section 11.6.

### 11.5.5 Multiple Minima

The error function R(θ) is nonconvex, possessing many local minima. As a
result, the final solution obtained is quite dependent on the choice of start-
ing weights. One must at least try a number of random starting configura-
tions, and choose the solution giving lowest (penalized) error. Probably a
better approach is to use the average predictions over the collection of net-
works as the final prediction (Ripley, 1996). This is preferable to averaging
the weights, since the nonlinearity of the model implies that this averaged
solution could be quite poor. Another approach is via bagging, which aver-
ages the predictions of networks training from randomly perturbed versions
of the training data. This is described in Section 8.7.