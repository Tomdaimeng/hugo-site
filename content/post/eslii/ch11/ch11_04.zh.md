+++
title = "ESL-11.4 Fitting Neural Networks"
summary = """
统计学习基础（译注）第十一章第四节，第 395-397 页。
"""

date = 2019-02-15T19:11:00+08:00
lastmod = 2019-02-15T19:11:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The neural network model has unknown parameters, often called weights,
and we seek values for them that make the model fit the training data well.
We denote the complete set of weights by θ, which consists of

$$\begin{align}
\\{\alpha\_{0m}, \alpha\_m; m=1,2,\dots,M\\} && M(p+1)\text{ weights}
\\\\ \\{\beta\_{0k}, \beta\_k; k=1,2,\dots,K\\} && K(M+1)\text{ weights}
\end{align}\tag{11.8}$$

For regression, we use sum-of-squared errors as our measure of fit (error
function)

$$R(\theta) = \sum\_{k=1}^K \sum\_{i=1}^N
(y\_{ik} - f\_k(x\_i))^2 \tag{11.9}$$

For classification we use either squared error or cross-entropy (deviance):

$$R(\theta) = -\sum\_{i=1}^N \sum\_{k=1}^K
y\_{ik} \log f\_k(x\_i) \tag{11.10}$$

and the corresponding classifier is G(x) = argmax k f k (x). With the softmax
activation function and the cross-entropy error function, the neural network
model is exactly a linear logistic regression model in the hidden units, and
all the parameters are estimated by maximum likelihood.

Typically we don’t want the global minimizer of R(θ), as this is likely
to be an overfit solution. Instead some regularization is needed: this is
achieved directly through a penalty term, or indirectly by early stopping.
Details are given in the next section.

The generic approach to minimizing R(θ) is by gradient descent, called
back-propagation in this setting. Because of the compositional form of the
model, the gradient can be easily derived using the chain rule for differen-
tiation. This can be computed by a forward and backward sweep over the
network, keeping track only of quantities local to each unit.

Here is back-propagation in detail for squared error loss. Let z mi =
x i ), from (11.5) and let z i = (z 1i , z 2i , . . . , z M i ). Then we have

$$\begin{align}
R(\theta) &\equiv \sum\_{i=1}^N R\_i \\\\ &=
\sum\_{i=1}^N \sum\_{k=1}^K (y\_{ik} - f\_k(x\_i))^2
\tag{11.11}\end{align}$$

with derivatives

$$\begin{align}
\frac{\partial R\_i}{\partial \beta\_{km}} &=
-2 (y\_{ik} - f\_k(x\_i)) g\_k^\prime(\beta\_k^T z\_i) z\_{mi}
\\\\ \frac{\partial R\_i}{\partial \alpha\_{m\ell}} &=
- \sum\_{k=1}^K 2 (y\_{ik} - f\_k(x\_i)) g\_k^\prime(\beta\_k^T z\_i)
\beta\_{km} \sigma^\prime(\alpha\_m^T x\_i) x\_{i\ell}
\end{align}\tag{11.12}$$

Given these derivatives, a gradient descent update at the (r + 1)st iter-
ation has the form

$$\begin{align}
\beta\_{km}^{(r+1)} &= \beta\_{km}^{( r )} - \gamma\_r \sum\_{i=1}^N 
\frac{\partial R\_i}{\partial \beta\_{km}^{( r )}}
\\\\ \alpha\_{m\ell}^{(r+1)} &= \alpha\_{m\ell}^{( r )} -
\gamma\_r \sum\_{i=1}^N 
\frac{\partial R\_i}{\partial \alpha\_{m\ell}^{( r )}}
\end{align}\tag{11.13}$$

where γ r is the learning rate, discussed below.

Now write (11.12) as

$$\begin{align}
\frac{\partial R\_i}{\beta\_{km}} &= \delta\_{ki} z\_{mi}
\\\\ \frac{\partial R\_i}{\alpha\_{m\ell}} &= s\_{mi} x\_{i\ell}
\end{align}\tag{11.14}$$

The quantities δ ki and s mi are “errors” from the current model at the
output and hidden layer units, respectively. From their definitions, these
errors satisfy

$$s\_{mi} = \sigma^\prime(\alpha\_m^T x\_i)
\sum\_{k=1}^K \beta\_{km} \delta\_{ki} \tag{11.15}$$

known as the back-propagation equations. Using this, the updates in (11.13)
can be implemented with a two-pass algorithm. In the forward pass, the
current weights are fixed and the predicted values f ˆ k (x i ) are computed
from formula (11.5). In the backward pass, the errors δ ki are computed,
and then back-propagated via (11.15) to give the errors s mi . Both sets of
errors are then used to compute the gradients for the updates in (11.13),
via (11.14).

This two-pass procedure is what is known as back-propagation. It has
also been called the delta rule (Widrow and Hoff, 1960). The computational
components for cross-entropy have the same form as those for the sum of
squares error function, and are derived in Exercise 11.3.

The advantages of back-propagation are its simple, local nature. In the
back propagation algorithm, each hidden unit passes and receives infor-
mation only to and from units that share a connection. Hence it can be
implemented efficiently on a parallel architecture computer.

The updates in (11.13) are a kind of batch learning, with the parame-
ter updates being a sum over all of the training cases. Learning can also
be carried out online—processing each observation one at a time, updat-
ing the gradient after each training case, and cycling through the training
cases many times. In this case, the sums in equations (11.13) are replaced
by a single summand. A training epoch refers to one sweep through the
entire training set. Online training allows the network to handle very large
training sets, and also to update the weights as new observations come in.

The learning rate γ r for batch learning is usually taken to be a con-
stant, and can also be optimized by a line search that minimizes the error
function at each update. With online learning γ r should decrease to zero
as the iteration r → ∞. This learning is a form of stochastic approxima-
tion (Robbins and Munro, P 1951); results in this field ensure convergence if
γ r → 0, r γ r = ∞, and r γ r 2 < ∞ (satisfied, for example, by γ r = 1/r).

Back-propagation can be very slow, and for that reason is usually not
the method of choice. Second-order techniques such as Newton’s method
are not attractive here, because the second derivative matrix of R (the
Hessian) can be very large. Better approaches to fitting include conjugate
gradients and variable metric methods. These avoid explicit computation
of the second derivative matrix while still providing faster convergence.