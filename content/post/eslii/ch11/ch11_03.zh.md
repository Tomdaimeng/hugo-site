+++
title = "ESL-11.3 Neural Networks"
summary = """
统计学习基础（译注）第十一章第三节，第 392-395 页。
"""

date = 2019-02-16T11:25:00+08:00
lastmod = 2019-02-16T11:25:00+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The term neural network has evolved to encompass a large class of models
and learning methods. Here we describe the most widely used “vanilla” neu-
ral net, sometimes called the single hidden layer back-propagation network,
or single layer perceptron. There has been a great deal of hype surrounding
neural networks, making them seem magical and mysterious. As we make
clear in this section, they are just nonlinear statistical models, much like
the projection pursuit regression model discussed above.

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_02.png"
  title="**图11.2**："
>}}
Schematic of a single hidden layer, feed-forward neural network.

A neural network is a two-stage regression or classification model, typ-
ically represented by a network diagram as in Figure 11.2. This network
applies both to regression or classification. For regression, typically K = 1
and there is only one output unit Y 1 at the top. However, these networks
can handle multiple quantitative responses in a seamless fashion, so we will
deal with the general case.

For K-class classification, there are K units at the top, with the kth
unit modeling the probability of class k. There are K target measurements
Y k , k = 1, . . . , K, each being coded as a 0 − 1 variable for the kth class.

Derived features Z m are created from linear combinations of the inputs,
and then the target Y k is modeled as a function of linear combinations of
the Z m ,

$$\begin{align}
Z\_m &= \sigma(\alpha\_{0m} + \alpha\_m^T X), m = 1,\dots,M
\\\\ T\_k &= \beta\_{0k} + \beta\_k^T Z, k = 1,\dots,K
\\\\  f\_k(X) &= g\_k(T), k = 1,\dots,K
\end{align}\tag{11.5}$$

where Z = (Z 1 , Z 2 , . . . , Z M ), and T = (T 1 , T 2 , . . . , T K ).

{{< figure
  src="http://public.guansong.wang/eslii/ch11/eslii_fig_11_03.png"
  title="**图11.3**："
>}}
Plot of the sigmoid function σ(v) = 1/(1+exp(−v)) (red curve),
commonly used in the hidden layer of a neural network. Included are σ(sv) for
s = 2 1 (blue curve) and s = 10 (purple curve). The scale parameter s controls
the activation rate, and we can see that large s amounts to a hard activation at
v = 0. Note that σ(s(v − v 0 )) shifts the activation threshold from 0 to v 0 .

The activation function σ(v) is usually chosen to be the sigmoid σ(v) =
1/(1 + e −v ); see Figure 11.3 for a plot of 1/(1 + e −v ). Sometimes Gaussian
radial basis functions (Chapter 6) are used for the σ(v), producing what is
known as a radial basis function network.

Neural network diagrams like Figure 11.2 are sometimes drawn with an
additional bias unit feeding into every unit in the hidden and output layers.
Thinking of the constant “1” as an additional input feature, this bias unit
captures the intercepts α 0m and β 0k in model (11.5).

The output function g k (T ) allows a final transformation of the vector of
outputs T . For regression we typically choose the identity function g k (T ) =
T k . Early work in K-class classification also used the identity function, but
this was later abandoned in favor of the softmax function

$$g\_k(T) = \frac{e^{T\_k}}{\sum\_{\ell=1}^K e^{T\_\ell}} \tag{11.6}$$

This is of course exactly the transformation used in the multilogit model
(Section 4.4), and produces positive estimates that sum to one. In Sec-
tion 4.2 we discuss other problems with linear activation functions, in par-
ticular potentially severe masking effects.

The units in the middle of the network, computing the derived features
Z m , are called hidden units because the values Z m are not directly ob-
served. In general there can be more than one hidden layer, as illustrated
in the example at the end of this chapter. We can think of the Z m as a
basis expansion of the original inputs X; the neural network is then a stan-
dard linear model, or linear multilogit model, using these transformations
as inputs. There is, however, an important enhancement over the basis-
expansion techniques discussed in Chapter 5; here the parameters of the
basis functions are learned from the data.

Notice that if σ is the identity function, then the entire model collapses
to a linear model in the inputs. Hence a neural network can be thought of
as a nonlinear generalization of the linear model, both for regression and
classification. By introducing the nonlinear transformation σ, it greatly
enlarges the class of linear models. In Figure 11.3 we see that the rate of
activation of the sigmoid depends on the norm of α m , and if kα m k is very
small, the unit will indeed be operating in the linear part of its activation
function.

Notice also that the neural network model with one hidden layer has
exactly the same form as the projection pursuit model described above.
The difference is that the PPR model uses nonparametric functions g m (v),
while the neural network uses a far simpler function based on σ(v), with
three free parameters in its argument. In detail, viewing the neural network
model as a PPR model, we identify

$$\begin{align}
g\_m(\_m^T X) &= \beta\_m \sigma(\alpha\_{0m} + \alpha\_m^T X)
\\\\ &= \beta\_m \sigma(\alpha\_{0m} + \\|\alpha\_m^T\\|(w\_m^T X))
\tag{11.7}\end{align}$$

where ω m = α m /kα m k is the mth unit-vector. Since σ β,α 0 ,s (v) = βσ(α 0 +
sv) has lower complexity than a more general nonparametric g(v), it is not
surprising that a neural network might use 20 or 100 such functions, while
the PPR model typically uses fewer terms (M = 5 or 10, for example).

Finally, we note that the name “neural networks” derives from the fact
that they were first developed as models for the human brain. Each unit
represents a neuron, and the connections (links in Figure 11.2) represent
synapses. In early models, the neurons fired when the total signal passed to
that unit exceeded a certain threshold. In the model above, this corresponds
to use of a step function for σ(Z) and g m (T ). Later the neural network was
recognized as a useful tool for nonlinear statistical modeling, and for this
purpose the step function is not smooth enough for optimization. Hence the
step function was replaced by a smoother threshold function, the sigmoid
in Figure 11.3.