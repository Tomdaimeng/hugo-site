+++
title = "ç»Ÿè®¡å­¦ä¹ åŸºç¡€ï¼šç›®å½•ç´¢å¼•"
summary = "â€œç»Ÿè®¡å­¦ä¹ åŸºç¡€â€ï¼ˆESLï¼‰ä¸€ä¹¦çš„ç« èŠ‚ç›®å½•ç´¢å¼•ï¼Œéšå®Œæˆè¿›åº¦æ›´æ–°ã€‚"

date = 2018-08-27T15:18:10+08:00
lastmod = 2018-10-04T16:21:07+08:00
draft = false
math = true

authors = ["Butters"]

tags = ["2018"]
categories = ["ç»Ÿè®¡å­¦ä¹ åŸºç¡€ï¼ˆè¯‘æ³¨ï¼‰"]

[header]
image = ""
caption = ""
preview = true
+++

1. [åºè¨€]({{< ref "/post/eslii/ch01/ch01_00.zh.md" >}})
2. ç›‘ç£å­¦ä¹ æ¦‚è¿°
  1. [å¼•è¨€]({{< ref "/post/eslii/ch02/ch02_01.zh.md" >}})
  2. [å˜é‡ç±»å‹å’Œæœ¯è¯­]({{< ref "/post/eslii/ch02/ch02_02.zh.md" >}})
  3. [ä¸¤ä¸ªç®€å•çš„é¢„æµ‹æ–¹æ³•ï¼šæœ€å°äºŒä¹˜å’Œæœ€è¿‘é‚»åŸŸ]({{< ref "/post/eslii/ch02/ch02_03.zh.md" >}})
  4. [ç»Ÿè®¡å†³ç­–ç†è®º]({{< ref "/post/eslii/ch02/ch02_04.zh.md" >}})
  5. [å±€éƒ¨æ–¹æ³•ä¸­çš„é«˜ç»´åº¦é—®é¢˜]({{< ref "/post/eslii/ch02/ch02_05.zh.md" >}})
  6. [ç»Ÿè®¡æ¨¡å‹ã€ç›‘ç£å­¦ä¹ å’Œå‡½æ•°é€¼è¿‘]({{< ref "/post/eslii/ch02/ch02_06.zh.md" >}})
  7. [æœ‰ç»“æ„çš„å›å½’æ¨¡å‹]({{< ref "/post/eslii/ch02/ch02_07.zh.md" >}})
  8. [æœ‰çº¦æŸçš„ä¼°è®¡æ¨¡å‹ç±»å‹]({{< ref "/post/eslii/ch02/ch02_08.zh.md" >}})
  9. [æ¨¡å‹é€‰æ‹©å’Œåå·®æ–¹å·®æƒè¡¡]({{< ref "/post/eslii/ch02/ch02_09.zh.md" >}})
3. å›å½’é—®é¢˜çš„çº¿æ€§æ–¹æ³•
  1. [å¼•è¨€]({{< ref "/post/eslii/ch03/ch03_01.zh.md" >}})
  2. [çº¿æ€§å›å½’æ¨¡å‹å’Œæœ€å°äºŒä¹˜æ³•]({{< ref "/post/eslii/ch03/ch03_02.zh.md" >}})
  3. [å˜é‡å­é›†é€‰æ‹©]({{< ref "/post/eslii/ch03/ch03_03.zh.md" >}})
  4. [æ”¶ç¼©æ–¹æ³•]({{< ref "/post/eslii/ch03/ch03_04.zh.md" >}})
  5. [è¡ç”Ÿè¾“å…¥å˜é‡çš„æ–¹æ³•]({{< ref "/post/eslii/ch03/ch03_05.zh.md" >}})
  6. [è®¨è®ºï¼šå­é›†é€‰æ‹©å’Œæ”¶ç¼©æ–¹æ³•çš„æ¯”è¾ƒ]({{< ref "/post/eslii/ch03/ch03_06.zh.md" >}})
  7. [å¤šè¾“å‡ºå˜é‡çš„æ”¶ç¼©å’Œå˜é‡é€‰æ‹© ğŸ˜±]({{< ref "/post/eslii/ch03/ch03_07.zh.md" >}})
  8. [æ›´å¤šå…³äºå¥—ç´¢å›å½’å’Œç±»ä¼¼çš„è·¯å¾„ç®—æ³•]({{< ref "/post/eslii/ch03/ch03_08.zh.md" >}})
  9. [è®¡ç®—é‡è€ƒé‡]({{< ref "/post/eslii/ch03/ch03_09.zh.md" >}})
4. åˆ†ç±»é—®é¢˜çš„çº¿æ€§æ–¹æ³•
  1. [å¼•è¨€]({{< ref "/post/eslii/ch04/ch04_01.zh.md" >}})
  2. [å¯¹æŒ‡ç¤ºå˜é‡çŸ©é˜µçš„çº¿æ€§å›å½’]({{< ref "/post/eslii/ch04/ch04_02.zh.md" >}})
  3. [çº¿æ€§åˆ¤åˆ«åˆ†æ]({{< ref "/post/eslii/ch04/ch04_03.zh.md" >}})
  4. [å¯¹æ•°å‡ ç‡å›å½’ï¼ˆé€»è¾‘å›å½’ï¼‰]({{< ref "/post/eslii/ch04/ch04_04.zh.md" >}})
  5. [Separating Hyperplanes]
5. Basis Expansions and Regularization
  1. [Introduction]
  2. [Piecewise Polynomials and Splines]
  3. [Filtering and Feature Extraction]
  4. [Smoothing Splines]
  5. [Automatic Selection of the Smoothing Parameters]
  6. [Nonparametric Logistic Regression]
  7. [Multidimensional Splines]
  8. [Regularization and Reproducing Kernel Hilbert Spaces]
  9. [Wavelet Smoothing]
6. Kernel Smoothing Methods
  1. [One-Dimensional Kernel Smoothers]
  2. [Selecting the Width of the Kernel]
  3. [Local Regression in $\mathbb{R}^p$]
  4. [Structured Local Regression Models in $\mathbb{R}^p$]
  5. [Local Likelihood and Other Models]
  6. [Kernel Density Estimation and Classification]
  7. [Radial Basis Functions and Kernels]
  8. [Mixture Models for Density Estimation and Classification]
  9. [Computational Considerations]
7. Model Assessment and Selection
  1. [Introduction]
  2. [Bias, Variance and Model Complexity]
  3. [The Biasâ€“Variance Decomposition]
  4. [Optimism of the Training Error Rate]
  5. [Estimates of In-Sample Prediction Error]
  6. [The Effective Number of Parameters]
  7. [The Bayesian Approach and BIC]
  8. [Minimum Description Length]
  9. [Vapnikâ€“Chervonenkis Dimension]
  10. [Cross-Validation]
  11. [Bootstrap Methods]
  12. [Conditional or Expected Test Error?]
8. Model Inference and Averaging
  1. [Introduction]
  2. [The Bootstrap and Maximum Likelihood Methods]
  3. [Bayesian Methods]
  4. [Relationship Between the Bootstrap and Bayesian Inference]
  5. [The EM Algorithm]
  6. [MCMC for Sampling from the Posterior]
  7. [Bagging]
  8. [Model Averaging and Stacking]
  9. [Stochastic Search: Bumping]
9. Additive Models, Trees, and Related Methods
  1. [Generalized Additive Models]
  2. [Tree-Based Methods]
  3. [PRIM: Bump Hunting]
  4. [MARS: Multivariate Adaptive Regression Splines]
  5. [Hierarchical Mixtures of Experts]
  6. [Missing Data]
  7. [Computational Considerations]
10. Boosting and Additive Trees
  1. [æå‡æ–¹æ³•]()
  2. [Boosting Fits an Additive Model]()
  3. [Forward Stagewise Additive Modeling]()
  4. [Exponential Loss and AdaBoost]()
  5. [Why Exponential Loss?]()
  6. [Loss Functions and Robustness]
  7. ["Off-the-Shelf" Procedures for Data Mining]
  8. [Example: Spam Data]
  9. [Boosting Trees]
  10. [Numerical Optimization via Gradient Boosting]
  11. [Right-Sized Trees for Boosting]
  12. [Regularization]
  13. [Interpretation]
  14. [Illustrations]
11. Neural Networks
12. Support Vector Machines and Flexible Discriminants
13. Prototype Methods and Nearest-Neighbors
14. Unsupervised Learning
15. Random Forests
16. Ensemble Learning
17. Undirected Graphical Models
18. High-Dimensional Problems: p â‰« N
