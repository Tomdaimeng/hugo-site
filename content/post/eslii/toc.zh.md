+++
title = "统计学习基础：目录索引"
summary = "“统计学习基础”（ESL）一书的章节目录索引，随完成进度更新。"

date = 2018-08-27T15:18:10+08:00
lastmod = 2018-08-30T17:01:10+08:00
draft = false
math = true

authors = ["Butters"]

tags = []
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

1. [序言]({{< ref "/post/eslii/ch01/ch01_00.zh.md" >}})
2. 监督学习概述
  1. [引言]({{< ref "/post/eslii/ch02/ch02_01.zh.md" >}})
  2. [变量类型和术语]({{< ref "/post/eslii/ch02/ch02_02.zh.md" >}})
  3. [Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors]()
  4. [Statistical Decision Theory]()
  5. [Local Methods in High Dimensions]()
  6. [Statistical Models, Supervised Learning and Function Approximation]()
  7. [Structured Regression Models]()
  8. [Classes of Restricted Estimators]()
  9. [Model Selection and the Bias-Variance Tradeoff]()
3. Linear Methods for Regression
  1. [Introduction]()
  2. [Linear Regression Models]()
  3. [Subset Selection]()
  4. [Shrinkage Methods]()
  5. [Methods Using Derived Inpute Directions]()
  6. [Discussion: A Comparison of the Selection
and Shrinkage Methods]()
  7. [Multiple Outcome Shrinkage and Selection]()
  8. [More on the Lasso and Related Path Algorithms]()
  9. [Computational Considerations]()
4. Linear Methods for Classification
  1. [Introduction]
  2. [Linear Regression of an Indicator Matrix]
  3. [Linear Discriminant Analysis]
  4. [Logistic Regression]
  5. [Separating Hyperplanes]
5. Basis Expansions and Regularization
  1. [Introduction]
  2. [Piecewise Polynomials and Splines]
  3. [Filtering and Feature Extraction]
  4. [Smoothing Splines]
  5. [Automatic Selection of the Smoothing Parameters]
  6. [Nonparametric Logistic Regression]
  7. [Multidimensional Splines]
  8. [Regularization and Reproducing Kernel Hilbert Spaces]
  9. [Wavelet Smoothing]
6. Kernel Smoothing Methods
  1. [One-Dimensional Kernel Smoothers]
  2. [Selecting the Width of the Kernel]
  3. [Local Regression in $\mathbb{R}^p$]
  4. [Structured Local Regression Models in $\mathbb{R}^p$]
  5. [Local Likelihood and Other Models]
  6. [Kernel Density Estimation and Classification]
  7. [Radial Basis Functions and Kernels]
  8. [Mixture Models for Density Estimation and Classification]
  9. [Computational Considerations]
7. Model Assessment and Selection
  1. [Introduction]
  2. [Bias, Variance and Model Complexity]
  3. [The Bias–Variance Decomposition]
  4. [Optimism of the Training Error Rate]
  5. [Estimates of In-Sample Prediction Error]
  6. [The Effective Number of Parameters]
  7. [The Bayesian Approach and BIC]
  8. [Minimum Description Length]
  9. [Vapnik–Chervonenkis Dimension]
  10. [Cross-Validation]
  11. [Bootstrap Methods]
  12. [Conditional or Expected Test Error?]
8. Model Inference and Averaging
  1. [Introduction]
  2. [The Bootstrap and Maximum Likelihood Methods]
  3. [Bayesian Methods]
  4. [Relationship Between the Bootstrap and Bayesian Inference]
  5. [The EM Algorithm]
  6. [MCMC for Sampling from the Posterior]
  7. [Bagging]
  8. [Model Averaging and Stacking]
  9. [Stochastic Search: Bumping]
9. Additive Models, Trees, and Related Methods
  1. [Generalized Additive Models]
  2. [Tree-Based Methods]
  3. [PRIM: Bump Hunting]
  4. [MARS: Multivariate Adaptive Regression Splines]
  5. [Hierarchical Mixtures of Experts]
  6. [Missing Data]
  7. [Computational Considerations]
10. Boosting and Additive Trees
  1. [提升方法]()
  2. [Boosting Fits an Additive Model]()
  3. [Forward Stagewise Additive Modeling]()
  4. [Exponential Loss and AdaBoost]()
  5. [Why Exponential Loss?]()
  6. [Loss Functions and Robustness]
  7. ["Off-the-Shelf" Procedures for Data Mining]
  8. [Example: Spam Data]
  9. [Boosting Trees]
  10. [Numerical Optimization via Gradient Boosting]
  11. [Right-Sized Trees for Boosting]
  12. [Regularization]
  13. [Interpretation]
  14. [Illustrations]
11. Neural Networks
12. Support Vector Machines and Flexible Discriminants
13. Prototype Methods and Nearest-Neighbors
14. Unsupervised Learning
15. Random Forests
16. Ensemble Learning
17. Undirected Graphical Models
18. High-Dimensional Problems: p ≫ N
