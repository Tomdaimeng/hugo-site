+++
title = "ESL-7.8 Minimum Description Length"
summary = """
统计学习基础（译注）第七章第八节，第 235-237 页。
"""

date = 2018-12-01T18:17:00+08:00
lastmod = 2018-12-01T18:17:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The minimum description length (MDL) approach gives a selection cri-
terion formally identical to the BIC approach, but is motivated from an
optimal coding viewpoint. We first review the theory of coding for data
compression, and then apply it to model selection.

We think of our datum z as a message that we want to encode and
send to someone else (the “receiver”). We think of our model as a way of
encoding the datum, and will choose the most parsimonious model, that is
the shortest code, for the transmission.

Suppose first that the possible messages we might want to transmit are
z 1 , z 2 , . . . , z m . Our code uses a finite alphabet of length A: for example, we
might use a binary code {0, 1} of length A = 2. Here is an example with
four possible messages and a binary coding:

$$\begin{array}{l\\|l|l|l|l}
\text{Message} & z\_1 & z\_2 & z\_3 & z\_4
\\\\ \hline \text{Code} & 0 & 10 & 110 & 111
\end{array}\tag{7.42}$$

This code is known as an instantaneous prefix code: no code is the pre-
fix of any other, and the receiver (who knows all of the possible codes),
knows exactly when the message has been completely sent. We restrict our
discussion to such instantaneous prefix codes.

One could use the coding in (7.42) or we could permute the codes, for
example use codes 110, 10, 111, 0 for z 1 , z 2 , z 3 , z 4 . How do we decide which
to use? It depends on how often we will be sending each of the messages.
If, for example, we will be sending z 1 most often, it makes sense to use the
shortest code 0 for z 1 . Using this kind of strategy—shorter codes for more
frequent messages—the average message length will be shorter.

In general, if messages are sent with probabilities Pr(z i ), i = 1, 2, . . . , 4,
a famous theorem due to Shannon says we should use code lengths l i =
− log 2 Pr(z i ) and the average message length satisfies

$$E(\text{length}) \geq - \sum \text{Pr}(z\_i) \log\_2(\text{Pr}(z\_i))
\tag{7.43}$$

The right-hand side above is also called the entropy of the distribution
Pr(z i ). The inequality is an equality when the probabilities satisfy p i =
A −l i . In our example, if Pr(z i ) = 1/2, 1/4, 1/8, 1/8, respectively, then the
coding shown in (7.42) is optimal and achieves the entropy lower bound.

In general the lower bound cannot be achieved, but procedures like the
Huffmann coding scheme can get close to the bound. Note that with an
infinite set of messages, the entropy is replaced by − Pr(z) log 2 Pr(z)dz.
From this result we glean the following:

> To transmit a random variable z having probability density func-
> tion Pr(z), we require about − log 2 Pr(z) bits of information.

We henceforth change notation from log 2 Pr(z) to log Pr(z) = log e Pr(z);
this is for convenience, and just introduces an unimportant multiplicative
constant.

Now we apply this result to the problem of model selection. We have
a model M with parameters θ, and data Z = (X, y) consisting of both
inputs and outputs. Let the (conditional) probability of the outputs under
the model be Pr(y|θ, M, X), assume the receiver knows all of the inputs,
and we wish to transmit the outputs. Then the message length required to
transmit the outputs is

$$\text{length} = -\log\text{Pr}(\mathbf{y} | \theta, M, \mathbf{X}) -
\log\text{Pr}(\theta|M) \tag{7.44}$$

the log-probability of the target values given the inputs. The second term
is the average code length for transmitting the model parameters θ, while
the first term is the average code length for transmitting the discrepancy
between the model and actual target values. For example suppose we have
a single target y with y ∼ N (θ, σ 2 ), parameter θ ∼ N (0, 1) and no input
(for simplicity). Then the message length is

$$\text{length} = \text{constant} + \log\sigma +
\frac{(y-\theta)^2}{\sigma^2} + \frac{\theta^2}{2} \tag{7.45}$$

Note that the smaller σ is, the shorter on average is the message length,
since y is more concentrated around θ.

The MDL principle says that we should choose the model that mini-
mizes (7.44). We recognize (7.44) as the (negative) log-posterior distribu-
tion, and hence minimizing description length is equivalent to maximizing
posterior probability. Hence the BIC criterion, derived as approximation to
log-posterior probability, can also be viewed as a device for (approximate)
model choice by minimum description length.

Note that we have ignored the precision with which a random variable
z is coded. With a finite code length we cannot code a continuous variable
exactly. However, if we code z within a tolerance δz, the message length
needed is the log of the probability in the interval [z, z+δz] which is well ap-
proximated by δzPr(z) if δz is small. Since log δzPr(z) = log δz + log Pr(z),
this means we can just ignore the constant log δz and use log Pr(z) as our
measure of message length, as we did above.

The preceding view of MDL for model selection says that we should
choose the model with highest posterior probability. However, many Bayes-
ians would instead do inference by sampling from the posterior distribution.