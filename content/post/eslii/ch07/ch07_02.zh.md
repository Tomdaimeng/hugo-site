+++
title = "ESL-7.2 偏差、方差、和模型复杂度"
summary = """
统计学习基础（译注）第七章第二节，第 219-223 页。
"""

date = 2018-11-23T12:46:07+08:00
lastmod = 2018-11-23T12:46:07+08:00
draft = true
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Figure 7.1 illustrates the important issue in assessing the ability of a learn-
ing method to generalize. Consider first the case of a quantitative or interval
scale response. We have a target variable Y , a vector of inputs X, and a
prediction model f ˆ (X) that has been estimated from a training set T .
The loss function for measuring errors between Y and f ˆ (X) is denoted by
L(Y, f ˆ (X)). Typical choices are

$$L(Y, \hat{f}(X)) =\begin{cases}
(Y - \hat{f}(X))^2 & \text{ squared error} \\\\  \|Y-\hat{f}(X)\| & \text{ absolute error}
\end{cases}\tag{7.1}$$

Test error, also referred to as generalization error, is the prediction error
over an independent test sample

$$\text{Err}\_{\mathcal{T}} =
E[L(Y, \hat{f}(X)) | \mathcal{T}]\tag{7.2}$$

Note that this expectation averages over everything that is random, includ-
ing the randomness in the training set that produced f ˆ .

Figure 7.1 shows the prediction error (light red curves) Err T for 100
simulated training sets each of size 50. The lasso (Section 3.4.2) was used
to produce the sequence of fits. The solid red curve is the average, and
hence an estimate of Err.

Estimation of Err T will be our goal, although we will see that Err is
more amenable to statistical analysis, and most methods effectively esti-
mate the expected error. It does not seem possible to estimate conditional
error effectively, given only the information in the same training set. Some
discussion of this point is given in Section 7.12.

Training error is the average loss over the training sample

$$\overline{\text{err}} =
\frac{1}{N} \sum\_{i=1}^N L(y\_i, \hat{f}(x\_i))\tag{7.4}$$

We would like to know the expected test error of our estimated model
f ˆ . As the model becomes more and more complex, it uses the training
data more and is able to adapt to more complicated underlying structures.
Hence there is a decrease in bias but an increase in variance. There is some
intermediate model complexity that gives minimum expected test error.

Unfortunately training error is not a good estimate of the test error,
as seen in Figure 7.1. Training error consistently decreases with model
complexity, typically dropping to zero if we increase the model complexity
enough. However, a model with zero training error is overfit to the training
data and will typically generalize poorly.

The story is similar for a qualitative or categorical response G taking
one of K values in a set G, labeled for convenience as 1, 2, . . . , K. Typically
we model the probabilities p k (X) = Pr(G = k|X) (or some monotone
transformations f k (X)), and then Ĝ(X) = arg max k p̂ k (X). In some cases,
such as 1-nearest neighbor classification (Chapters 2 and 13) we produce
Ĝ(X) directly. Typical loss functions are

$$\begin{align}
L(G, \hat{G}(X)) &=
\underbrace{I(G \ne \hat{G}(X))}\_\text{0-1 loss}
\tag{7.5} \\\\ L(G, \hat{p}(X)) &=
-2 \sum\_{k=1}^K I(G=k) \log \hat{p}\_k(X) \\\\ &=
\underbrace{-2 \log \hat{p}\_G(X)}\_{-2 \times \text{log-likelihood}}
\tag{7.6}\end{align}$$

The quantity −2 × the log-likelihood is sometimes referred to as the
deviance.

Again, test error here is Err T = E[L(G, Ĝ(X))|T ], the population mis-
classification error of the classifier trained on T , and Err is the expected
misclassification error.

Training error is the sample analogue, for example,

$$\overline{\text{err}} = -\frac{2}{N}
\sum\_{i=1}^N \log\hat{p}\_{g\_i}(x\_i) \tag{7.7}$$

the sample log-likelihood for the model.

The log-likelihood can be used as a loss-function for general response
densities, such as the Poisson, gamma, exponential, log-normal and others.
If Pr θ(X) (Y ) is the density of Y , indexed by a parameter θ(X) that depends
on the predictor X, then

$$L(Y, \theta(X)) = -2 \cdot \log \text{Pr}\_{\theta(X)}(Y) \tag{7.8}$$

The “−2” in the definition makes the log-likelihood loss for the Gaussian
distribution match squared-error loss.

For ease of exposition, for the remainder of this chapter we will use Y and
f (X) to represent all of the above situations, since we focus mainly on the
quantitative response (squared-error loss) setting. For the other situations,
the appropriate translations are obvious.

In this chapter we describe a number of methods for estimating the
expected test error for a model. Typically our model will have a tuning
parameter or parameters α and so we can write our predictions as f ˆ α (x).
The tuning parameter varies the complexity of our model, and we wish to
find the value of α that minimizes error, that is, produces the minimum of
the average test error curve in Figure 7.1. Having said this, for brevity we
will often suppress the dependence of f ˆ (x) on α.

It is important to note that there are in fact two separate goals that we
might have in mind:

* Model selection: estimating the performance of different models in order
to choose the best one.
* Model assessment: having chosen a final model, estimating its predic-
tion error (generalization error) on new data.

If we are in a data-rich situation, the best approach for both problems is
to randomly divide the dataset into three parts: a training set, a validation
set, and a test set. The training set is used to fit the models; the validation
set is used to estimate prediction error for model selection; the test set is
used for assessment of the generalization error of the final chosen model.
Ideally, the test set should be kept in a “vault,” and be brought out only
at the end of the data analysis. Suppose instead that we use the test-set
repeatedly, choosing the model with smallest test-set error. Then the test
set error of the final chosen model will underestimate the true test error,
sometimes substantially.

It is difficult to give a general rule on how to choose the number of
observations in each of the three parts, as this depends on the signal-to-
noise ratio in the data and the training sample size. A typical split might
be 50% for training, and 25% each for validation and testing:

{{< figure
  src="http://public.guansong.wang/eslii/ch07/eslii_fig_07_a.png"
>}}

The methods in this chapter are designed for situations where there is
insufficient data to split it into three parts. Again it is too difficult to give
a general rule on how much training data is enough; among other things,
this depends on the signal-to-noise ratio of the underlying function, and
the complexity of the models being fit to the data.

The methods of this chapter approximate the validation step either an-
alytically (AIC, BIC, MDL, SRM) or by efficient sample re-use (cross-
validation and the bootstrap). Besides their use in model selection, we also
examine to what extent each method provides a reliable estimate of test
error of the final chosen model.

Before jumping into these topics, we first explore in more detail the
nature of test error and the bias–variance tradeoff.