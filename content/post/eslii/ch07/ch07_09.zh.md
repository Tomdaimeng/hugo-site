+++
title = "ESL-7.9 Vapnik–Chervonenkis Dimension 😱"
summary = """
统计学习基础（译注）第七章第九节，第 237-241 页。
"""

date = 2018-12-01T18:43:00+08:00
lastmod = 2018-12-01T18:43:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

A difficulty in using estimates of in-sample error is the need to specify the
number of parameters (or the complexity) d used in the fit. Although the
effective number of parameters introduced in Section 7.6 is useful for some
nonlinear models, it is not fully general. The Vapnik–Chervonenkis (VC)
theory provides such a general measure of complexity, and gives associated
bounds on the optimism. Here we give a brief review of this theory.

Suppose we have a class of functions {f (x, α)} indexed by a parameter
vector α, with x ∈ IR p . Assume for now that f is an indicator function,
that is, takes the values 0 or 1. If α = (α 0 , α 1 ) and f is the linear indi-
cator function I(α 0 + α 1 T x > 0), then it seems reasonable to say that the
complexity of the class f is the number of parameters p + 1. But what
about f (x, α) = I(sin α · x) where α is any real number and x ∈ IR? The
function sin(50 · x) is shown in Figure 7.5. This is a very wiggly function
that gets even rougher as the frequency α increases, but it has only one
parameter: despite this, it doesn’t seem reasonable to conclude that it has
less complexity than the linear indicator function I(α 0 + α 1 x) in p = 1
dimension.

The Vapnik–Chervonenkis dimension is a way of measuring the com-
plexity of a class of functions by assessing how wiggly its members can
be.

> The VC dimension of the class {f (x, α)} is defined to be the
> largest number of points (in some configuration) that can be
> shattered by members of {f (x, α)}.

A set of points is said to be shattered by a class of functions if, no matter
how we assign a binary label to each point, a member of the class can
perfectly separate them.

Figure 7.6 shows that the VC dimension of linear indicator functions
in the plane is 3 but not 4, since no four points can be shattered by a
set of lines. In general, a linear indicator function in p dimensions has VC
dimension p + 1, which is also the number of free parameters. On the other
hand, it can be shown that the family sin(αx) has infinite VC dimension,
as Figure 7.5 suggests. By appropriate choice of α, any set of points can be
shattered by this class (Exercise 7.8).

So far we have discussed the VC dimension only of indicator functions,
but this can be extended to real-valued functions. The VC dimension of a
class of real-valued functions {g(x, α)} is defined to be the VC dimension
of the indicator class {I(g(x, α) − β > 0)}, where β takes values over the
range of g.

One can use the VC dimension in constructing an estimate of (extra-
sample) prediction error; different types of results are available. Using the
concept of VC dimension, one can prove results about the optimism of the
training error when using a class of functions. An example of such a result is
the following. If we fit N training points using a class of functions {f (x, α)}
having VC dimension h, then with probability at least 1 − η over training
sets:

$$\begin{align}
\text{Err}\_\mathcal{T} & \leq \overline{\text{err}} +
  \frac{\epsilon}{2} \left( 1 +
  \sqrt{1 + \frac{4\cdot\overline{\text{err}}}{\epsilon}}\right)
  \tag{二分类} \\\\ \text{Err}\_\mathcal{T} & \leq
  \frac{\overline{\text{err}}}{(1-c\sqrt{\epsilon})\_\epsilon}
  \tag{回归} \\\\ & \text{where }
  \epsilon = a\_1\frac{h[\log(a\_2N/h)+1]-\log(\eta/4)}{N}
  \tag{7.46} \\\\ & \text{and }
  0 < a\_1 \leq 4, 0 < a\_2 \leq 2
\end{align}$$
  

These bounds hold simultaneously for all members f (x, α), and are taken
from Cherkassky and Mulier (2007, pages 116–118). They recommend the
value c = 1. For regression they suggest a 1 = a 2 = 1, and for classification
they make no recommendation, with a 1 = 4 and a 2 = 2 corresponding
to worst-case scenarios. They also give an alternative practical bound for
regression

$$\text{Err}\_\mathcal{T} \leq \overline{\text{err}} \left(
1 - \sqrt{\rho - \rho\log\rho + \frac{\log N}{2N}}
\right)^{-1}\_+ \tag{7.47}$$

with ρ = p/N, which is free of tuning constants. The bounds suggest that the
optimism increases with h and decreases with N in qualitative agreement
with the AIC correction d/N given is (7.24). However, the results in (7.46)
are stronger: rather than giving the expected optimism for each fixed func-
tion f (x, α), they give probabilistic upper bounds for all functions f (x, α),
and hence allow for searching over the class.

Vapnik’s structural risk minimization (SRM) approach fits a nested se-
quence of models of increasing VC dimensions h 1 < h 2 < · · · , and then
chooses the model with the smallest value of the upper bound.

We note that upper bounds like the ones in (7.46) are often very loose,
but that doesn’t rule them out as good criteria for model selection, where
the relative (not absolute) size of the test error is important. The main
drawback of this approach is the difficulty in calculating the VC dimension
of a class of functions. Often only a crude upper bound for VC dimension
is obtainable, and this may not be adequate. An example in which the
structural risk minimization program can be successfully carried out is the
support vector classifier, discussed in Section 12.2.

### 7.9.1 Example (Continued)

Figure 7.7 shows the results when AIC, BIC and SRM are used to select
the model size for the examples of Figure 7.3. For the examples labeled KNN ,
the model index α refers to neighborhood size, while for those labeled REG α
refers to subset size. Using each selection method (e.g., AIC) we estimated
the best model α̂ and found its true prediction error Err T (α̂) on a test
set. For the same training set we computed the prediction error of the best
and worst possible model choices: min α Err T (α) and max α Err T (α). The
boxplots show the distribution of the quantity

$$100 \times \frac
{\text{Err}\_\mathcal{T}(\hat{\alpha}) -
\min\alpha \text{Err}\_\mathcal{T}(\alpha)}
{\max\_\alpha \text{Err}\_\mathcal{T}(\alpha) -
\min\_\alpha \text{Err}\_\mathcal{T}(\alpha)} $$

which represents the error in using the chosen model relative to the best
model. For linear regression the model complexity was measured by the
number of features; as mentioned in Section 7.5, this underestimates the
df, since it does not charge for the search for the best model of that size.
This was also used for the VC dimension of the linear classifier. For k-
nearest neighbors, we used the quantity N/k. Under an additive-error re-
gression model, this can be justified as the exact effective degrees of free-
dom (Exercise 7.6); we do not know if it corresponds to the VC dimen-
sion. We used a 1 = a 2 = 1 for the constants in (7.46); the results for SRM
changed with different constants, and this choice gave the most favorable re-
sults. We repeated the SRM selection using the alternative practical bound
(7.47), and got almost identical results. For misclassification error we used
σ ˆ ε 2 = [N/(N − d)] · err(α) for the least restrictive model (k = 5 for KNN,
since k = 1 results in zero training error). The AIC criterion seems to work
well in all four scenarios, despite the lack of theoretical support with 0–1
loss. BIC does nearly as well, while the performance of SRM is mixed.