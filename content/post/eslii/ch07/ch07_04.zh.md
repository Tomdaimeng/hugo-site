+++
title = "ESL-7.4 Optimism of the Training Error Rate"
summary = """
统计学习基础（译注）第七章第四节，第 228-230 页。
"""

date = 2018-11-23T10:10:00+08:00
lastmod = 2018-11-23T10:10:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Discussions of error rate estimation can be confusing, because we have
to make clear which quantities are fixed and which are random 1 . Before
we continue, we need a few definitions, elaborating on the material of Sec-
tion 7.2. Given a training set T = {(x 1 , y 1 ), (x 2 , y 2 ), . . . (x N , y N )} the gen-
eralization error of a model f ˆ is

$$\tag{7.15}

Note that the training set T is fixed in expression (7.15). The point (X 0 , Y 0 )
is a new test data point, drawn from F , the joint distribution of the data.
Averaging over training sets T yields the expected error

$$\tag{7.16}

which is more amenable to statistical analysis. As mentioned earlier, it
turns out that most methods effectively estimate the expected error rather
than E T ; see Section 7.12 for more on this point.

Now typically, the training error

$$\tag{7.17}

will be less than the true error Err T , because the same data is being used
to fit the method and assess its error (see Exercise 2.9). A fitting method
typically adapts to the training data, and hence the apparent or training
error err will be an overly optimistic estimate of the generalization error
Err T .

Part of the discrepancy is due to where the evaluation points occur. The
quantity Err T can be thought of as extra-sample error, since the test input
vectors don’t need to coincide with the training input vectors. The nature
of the optimism in err is easiest to understand when we focus instead on
the in-sample error

$$\tag{7.18}

The Y 0 notation indicates that we observe N new response values at
each of the training points x i , i = 1, 2, . . . , N . We define the optimism as
the difference between Err in and the training error err:

$$\tag{7.19}

This is typically positive since err is usually biased downward as an estimate
of prediction error. Finally, the average optimism is the expectation of the
optimism over training sets

$$\tag{7.20}

Here the predictors in the training set are fixed, and the expectation is
over the training set outcome values; hence we have used the notation E y
instead of E T . We can usually estimate only the expected error ω rather
than op, in the same way that we can estimate the expected error Err
rather than the conditional error Err T .

For squared error, 0–1, and other loss functions, one can show quite
generally tha

$$\tag{7.21}

where Cov indicates covariance. Thus the amount by which err underesti-
mates the true error depends on how strongly y i affects its own prediction.
The harder we fit the data, the greater Cov(ŷ i , y i ) will be, thereby increas-
ing the optimism. Exercise 7.4 proves this result for squared error loss where
ŷ i is the fitted value from the regression. For 0–1 loss, ŷ i ∈ {0, 1} is the
classification at x i , and for entropy loss, ŷ i ∈ [0, 1] is the fitted probability
of class 1 at x i .

In summary, we have the important relation

$$\tag{7.22}

This expression simplifies if ŷ i is obtained by a linear fit with d inputs
or basis functions. For example,

$$\tag{7.23}

for the additive error model Y = f (X) + ε, and so

$$\tag{7.24}

Expression (7.23) is the basis for the definition of the effective number of
parameters discussed in Section 7.6 The optimism increases linearly with
the number d of inputs or basis functions we use, but decreases as the
training sample size increases. Versions of (7.24) hold approximately for
other error models, such as binary data and entropy loss.

An obvious way to estimate prediction error is to estimate the optimism
and then add it to the training error err. The methods described in the
next section—C p , AIC, BIC and others—work in this way, for a special
class of estimates that are linear in their parameters.
In contrast, cross-validation and bootstrap methods, described later in
the chapter, are direct estimates of the extra-sample error Err. These gen-
eral tools can be used with any loss function, and with nonlinear, adaptive
fitting techniques.

In-sample error is not usually of direct interest since future values of the
features are not likely to coincide with their training set values. But for
comparison between models, in-sample error is convenient and often leads
to effective model selection. The reason is that the relative (rather than
absolute) size of the error is what matters.