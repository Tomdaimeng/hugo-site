+++
title = "ESL-7.12 Conditional or Expected Test Error? ğŸ˜±"
summary = """
ç»Ÿè®¡å­¦ä¹ åŸºç¡€ï¼ˆè¯‘æ³¨ï¼‰ç¬¬ä¸ƒç« ç¬¬åäºŒèŠ‚ï¼Œç¬¬ 254-257 é¡µã€‚
"""

date = 2018-11-23T10:18:00+08:00
lastmod = 2018-11-23T10:18:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["è¯‘æ–‡"]
categories = ["ç»Ÿè®¡å­¦ä¹ åŸºç¡€ï¼ˆè¯‘æ³¨ï¼‰"]

[header]
image = ""
caption = ""
preview = true
+++

Figures 7.14 and 7.15 examine the question of whether cross-validation does
a good job in estimating Err T , the error conditional on a given training set
T (expression (7.15) on page 228), as opposed to the expected test error.
For each of 100 training sets generated from the â€œreg/linearâ€ setting in
the top-right panel of Figure 7.3, Figure 7.14 shows the conditional error
curves Err T as a function of subset size (top left). The next two panels show
10-fold and N -fold cross-validation, the latter also known as leave-one-out
(LOO). The thick red curve in each plot is the expected error Err, while
the thick black curves are the expected cross-validation curves. The lower
right panel shows how well cross-validation approximates the conditional
and expected error.

One might have expected N -fold CV to approximate Err T well, since it
almost uses the full training sample to fit a new test point. 10-fold CV, on
the other hand, might be expected to estimate Err well, since it averages
over somewhat different training sets. From the figure it appears 10-fold
does a better job than N -fold in estimating Err T , and estimates Err even
better. Indeed, the similarity of the two black curves with the red curve
suggests both CV curves are approximately unbiased for Err, with 10-fold
having less variance. Similar trends were reported by Efron (1983).

Figure 7.15 shows scatterplots of both 10-fold and N -fold cross-validation
error estimates versus the true conditional error for the 100 simulations.
Although the scatterplots do not indicate much correlation, the lower right
panel shows that for the most part the correlations are negative, a curi-
ous phenomenon that has been observed before. This negative correlation
explains why neither form of CV estimates Err T well. The broken lines in
each plot are drawn at Err(p), the expected error for the best subset of
size p. We see again that both forms of CV are approximately unbiased for
expected error, but the variation in test error for different training sets is
quite substantial.

Among the four experimental conditions in 7.3, this â€œreg/linearâ€ scenario
showed the highest correlation between actual and predicted test error. This
phenomenon also occurs for bootstrap estimates of error, and we would
guess, for any other estimate of conditional prediction error.

We conclude that estimation of test error for a particular training set is
not easy in general, given just the data from that same training set. Instead,
cross-validation and related methods may provide reasonable estimates of
the expected error Err.