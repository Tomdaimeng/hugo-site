+++
title = "ESL-7.11 Bootstrap Methods"
summary = """
统计学习基础（译注）第七章第十一节，第 249-254 页。
"""

date = 2018-11-23T10:17:00+08:00
lastmod = 2018-11-23T10:17:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The bootstrap is a general tool for assessing statistical accuracy. First we
describe the bootstrap in general, and then show how it can be used to
estimate extra-sample prediction error. As with cross-validation, the boot-
strap seeks to estimate the conditional error Err T , but typically estimates
well only the expected prediction error Err.

Suppose we have a model fit to a set of training data. We denote the
training set by Z = (z 1 , z 2 , . . . , z N ) where z i = (x i , y i ). The basic idea is
to randomly draw datasets with replacement from the training data, each
sample the same size as the original training set. This is done B times
(B = 100 say), producing B bootstrap datasets, as shown in Figure 7.12.
Then we refit the model to each of the bootstrap datasets, and examine
the behavior of the fits over the B replications.

In the figure, S(Z) is any quantity computed from the data Z, for ex-
ample, the prediction at some input point. From the bootstrap sampling
we can estimate any aspect of the distribution of S(Z), for example, its
variance,

$$\tag{7.53}$$

where S̄ ∗ = b S(Z )/B. Note that Var[S(Z)] can be thought of as a
Monte-Carlo estimate of the variance of S(Z) under sampling from the
empirical distribution function F̂ for the data (z 1 , z 2 , . . . , z N ).

How can we apply the bootstrap to estimate prediction error? One ap-
proach would be to fit the model in question on a set of bootstrap samples,
and then keep track of how well it predicts the original training set. If
f ˆ ∗b (x i ) is the predicted value at x i , from the model fitted to the bth boot-
strap dataset, our estimate is

$$\tag{7.54}$$

However, it is easy to see that Errboot does not provide a good estimate in
general. The reason is that the bootstrap datasets are acting as the training
samples, while the original training set is acting as the test sample, and
these two samples have observations in common. This overlap can make
overfit predictions look unrealistically good, and is the reason that cross-
validation explicitly uses non-overlapping data for the training and test
samples. Consider for example a 1-nearest neighbor classifier applied to a
two-class classification problem with the same number of observations in
each class, in which the predictors and class labels are in fact independent.
Then the true error rate is 0.5. But the contributions to the bootstrap
estimate Err boot will be zero unless the observation i does not appear in the
bootstrap sample b. In this latter case it will have the correct expectation
0.5. Now

$$\tag{7.55}$$

Hence the expectation of Errboot is about 0.5 × 0.368 = 0.184, far below
the correct error rate 0.5.

By mimicking cross-validation, a better bootstrap estimate can be ob-
tained. For each observation, we only keep track of predictions from boot-
strap samples not containing that observation. The leave-one-out bootstrap
estimate of prediction error is defined by

$$\tag{7.56}$$

Here C −i is the set of indices of the bootstrap samples b that do not contain
observation i, and |C −i | is the number of such samples. In computing Err(1),
we either have to choose B large enough to ensure that all of the |C −i | are
greater than zero, or we can just leave out the terms in (7.56) corresponding
to |C −i |’s that are zero.

The leave-one out bootstrap solves the overfitting problem suffered by
Errboot but has the training-set-size bias mentioned in the discussion of
cross-validation. The average number of distinct observations in each boot-
strap sample is about 0.632 · N , so its bias will roughly behave like that of
twofold cross-validation. Thus if the learning curve has considerable slope
at sample size N/2, the leave-one out bootstrap will be biased upward as
an estimate of the true error.

The “.632 estimator” is designed to alleviate this bias. It is defined by

$$\tag{7.57}$$

The derivation of the .632 estimator is complex; intuitively it pulls the
leave-one out bootstrap estimate down toward the training error rate, and
hence reduces its upward bias. The use of the constant .632 relates to (7.55).

The .632 estimator works well in “light fitting” situations, but can break
down in overfit ones. Here is an example due to Breiman et al. (1984).
Suppose we have two equal-size classes, with the targets independent of
the class labels, and we apply a one-nearest neighbor rule. Then err = 0,
Err = 0.5 and so Err = .632 × 0.5 = .316. However, the true error
rate is 0.5.

One can improve the .632 estimator by taking into account the amount
of overfitting. First we define γ to be the no-information error rate: this
is the error rate of our prediction rule if the inputs and class labels were
independent. An estimate of γ is obtained by evaluating the prediction rule
on all possible combinations of targets y i and predictors x i ′

$$\tag{7.58}$$

For example, consider the dichotomous classification problem: let p̂ 1 be
the observed proportion of responses y i equaling 1, and let q̂ 1 be the ob-
served proportion of predictions f ˆ (x i ′ ) equaling 1. Then

$$\tag{7.59}$$

With a rule like 1-nearest neighbors for which q̂ 1 = p̂ 1 the value of γ̂ is
2p̂ 1 (1− p̂ 1 ). The multi-category generalization of (7.59) is γ̂ = l p̂ l (1− q̂ l ).

Using this, the relative overfitting rate is defined to be

$$\tag{7.60}$$

a quantity that ranges from 0 if there is no overfitting ( Err = err ) to 1
if the overfitting equals the no-information value γ̂ − err. Finally, we define
the “.632+” estimator by

$$\tag{7.61}$$

The weight w ranges from .632 if R̂ = 0 to 1 if R̂ = 1, so Err(.632)
ranges from Err(.632) to Err(1). Again, the derivation of (7.61) is compli-
cated: roughly speaking, it produces a compromise between the leave-one-
out bootstrap and the training error rate that depends on the amount of
overfitting. For the 1-nearest-neighbor problem with class labels indepen-
dent of the inputs, ŵ = R̂ = 1, so Err(.632+) = Err(1), which has the correct
expectation of 0.5. In other problems with less overfitting, Err(.632+) will
lie somewhere between err and Err(1).

7.11.1 Example (Continued)

Figure 7.13 shows the results of tenfold cross-validation and the .632+ boot-
strap estimate in the same four problems of Figures 7.7. As in that figure,
Figure 7.13 shows boxplots of 100 · [Err α̂ − min α Err(α)]/[max α Err(α) −
min α Err(α)], the error in using the chosen model relative to the best model.
There are 100 different training sets represented in each boxplot. Both mea-
sures perform well overall, perhaps the same or slightly worse that the AIC
in Figure 7.7.

Our conclusion is that for these particular problems and fitting methods,
minimization of either AIC, cross-validation or bootstrap yields a model
fairly close to the best available. Note that for the purpose of model selec-
tion, any of the measures could be biased and it wouldn’t affect things, as
long as the bias did not change the relative performance of the methods.
For example, the addition of a constant to any of the measures would not
change the resulting chosen model. However, for many adaptive, nonlinear
techniques (like trees), estimation of the effective number of parameters is
very difficult. This makes methods like AIC impractical and leaves us with
cross-validation or bootstrap as the methods of choice.

A different question is: how well does each method estimate test error?
On the average the AIC criterion overestimated prediction error of its cho-
sen model by 38%, 37%, 51%, and 30%, respectively, over the four scenarios,
with BIC performing similarly. In contrast, cross-validation overestimated
the error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the
same. Hence the extra work involved in computing a cross-validation or
bootstrap measure is worthwhile, if an accurate estimate of test error is
required. With other fitting methods like trees, cross-validation and boot-
strap can underestimate the true error by 10%, because the search for best
tree is strongly affected by the validation set. In these situations only a
separate test set will provide an unbiased estimate of test error.