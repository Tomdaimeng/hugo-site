+++
title = "ESL-7.7 The Bayesian Approach and BIC"
summary = """
统计学习基础（译注）第七章第七节，第 233-235 页。
"""

date = 2018-12-01T17:52:00+08:00
lastmod = 2018-12-01T17:52:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The Bayesian information criterion (BIC), like AIC, is applicable in settings
where the fitting is carried out by maximization of a log-likelihood. The
generic form of BIC is

$$\text{BIC} = -2 \cdot \text{loglik} + (\log N) \cdot d \tag{7.35}$$

The BIC statistic (times 1/2) is also known as the Schwarz criterion (Schwarz,
1978).

Under the Gaussian model, assuming the variance σ ε 2 is known, −2·loglik
equals (up to a constant) i (y i − f ˆ (x i )) 2 /σ ε 2 , which is N ·err/σ ε 2 for squared
error loss. Hence we can write

$$\text{BIC} = \frac{N}{\sigma^2\_\varepsilon} \left[
\overline{\text{err}} + (\log N) \cdot \frac{d}{N} \sigma^2\_\varepsilon
\right] \tag{7.36}$$

Therefore BIC is proportional to AIC (C p ), with the factor 2 replaced
by log N . Assuming N > e 2 ≈ 7.4, BIC tends to penalize complex models
more heavily, giving preference to simpler models in selection. As with AIC,
σ ε 2 is typically estimated by the mean squared error of a low-bias model.
For classification problems, use of the multinomial log-likelihood leads to a
similar relationship with the AIC, using cross-entropy as the error measure.
Note however that the misclassification error measure does not arise in the
BIC context, since it does not correspond to the log-likelihood of the data
under any probability model.

Despite its similarity with AIC, BIC is motivated in quite a different
way. It arises in the Bayesian approach to model selection, which we now
describe.

Suppose we have a set of candidate models M m , m = 1, . . . , M and
corresponding model parameters θ m , and we wish to choose a best model
from among them. Assuming we have a prior distribution Pr(θ m |M m ) for
the parameters of each model M m , the posterior probability of a given
model is

$$\begin{align} \text{Pr}(\mathcal{M}\_m|\mathbf{Z}) & \propto
\text{Pr}(\mathcal{M}\_m) \cdot \text{Pr}(\mathbf{Z}|\mathcal{M}\_m)
\tag{7.37}\\\\ & \propto
\text{Pr}(\mathcal{M}\_m) \cdot \int
\text{Pr}(\mathbf{Z}|\theta\_m, \mathcal{M}\_m)
\text{Pr}(\theta\_m|\mathcal{M}\_m) d\theta\_m
\end{align}$$

where Z represents the training data {x i , y i } 1 . To compare two models
M m and M l , we form the posterior odds

$$\frac{\text{Pr}(\mathcal{M}\_m | \mathbf{Z})}
{\text{Pr}(\mathcal{M}\_l | \mathbf{Z})} =
\frac{\text{Pr}(\mathcal{M}\_m)}{\text{Pr}(\mathcal{M}\_l)} \cdot
\frac{\text{Pr}(\mathbf{Z} | \mathcal{M}\_m)}
{\text{Pr}(\mathbf{Z} | \mathcal{M}\_l)}
\tag{7.38}$$

If the odds are greater than one we choose model m, otherwise we choose
model l. The rightmost quantity

$$\text{BF}(\mathbf{Z}) =
\frac{\text{Pr}(\mathbf{Z} | \mathcal{M}\_m)}
{\text{Pr}(\mathbf{Z} | \mathcal{M}\_l)}
\tag{7.39}$$

is called the Bayes factor, the contribution of the data toward the posterior
odds.

Typically we assume that the prior over models is uniform, so that
Pr(M m ) is constant. We need some way of approximating Pr(Z|M m ).
A so-called Laplace approximation to the integral followed by some other
simplifications (Ripley, 1996, page 64) to (7.37) gives

$$\log\text{Pr}(\mathbf{Z} | \mathcal{M}\_m) =
\log \text{Pr}(\mathbf{Z} | \hat{\theta}\_m, \mathcal{M}\_m) -
\frac{d\_m}{2} \cdot \log N + O(1) \tag{7.40}$$

Here θ̂ m is a maximum likelihood estimate and d m is the number of free
parameters in model M m . If we define our loss function to be

$$ -2 \log\text{Pr}(\mathbf{Z} | \hat{\theta}\_m, \mathcal{M}\_m)$$

this is equivalent to the BIC criterion of equation (7.35).

Therefore, choosing the model with minimum BIC is equivalent to choos-
ing the model with largest (approximate) posterior probability. But this
framework gives us more. If we compute the BIC criterion for a set of M ,
models, giving BIC m , m = 1, 2, . . . , M , then we can estimate the posterior
probability of each model M m as

$$\frac{e^{\frac{1}{2} \cdot \text{BIC}\_m}}
{\sum\_{l=1}^M e^{\frac{1}{2} \cdot \text{BIC}\_l}}
\tag{7.41}$$

Thus we can estimate not only the best model, but also assess the relative
merits of the models considered.

For model selection purposes, there is no clear choice between AIC and
BIC. BIC is asymptotically consistent as a selection criterion. What this
means is that given a family of models, including the true model, the prob-
ability that BIC will select the correct model approaches one as the sample
size N → ∞. This is not the case for AIC, which tends to choose models
which are too complex as N → ∞. On the other hand, for finite samples,
BIC often chooses models that are too simple, because of its heavy penalty
on complexity.