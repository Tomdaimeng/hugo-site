+++
title = "ESL-7.5 样本内预测误差的估计"
summary = """
统计学习基础（译注）第七章第五节，第 230-232 页。
介绍了用于模型选择的 AIC 统计量。
"""

date = 2018-12-02T13:41:00+08:00
lastmod = 2018-12-02T13:41:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

样本内估计的一般形式是：

$$\widehat{\text{Err}}\_\text{in} =
\overline{\text{err}} + \hat{w} \tag{7.25}$$

其中 $\hat{w}$ 是平均乐观值（optimism）的估计。

当用平方误差损失函数拟合 $d$ 个参数时，
根据表达式 7.24,
可得到 $C\_p$ 统计量的一个形式：

$$C\_p = \overline{\text{err}} +
2 \cdot \frac{d}{N} \hat{\sigma}^2\_\varepsilon \tag{7.26}$$

其中 $\hat{\sigma}^2\_\varepsilon$ 为噪声方差的估计，
在偏差低的模型中通常是均方误差。
这个准则对训练误差的调整与使用的参数或基函数个数成正比。

**赤池信息量准则**（Akaike information criterion，AIC）
是一个类似的，但可更广泛地适用于使用了对数似然损失函数的
$\text{Err}\_\text{in}$ 的估计。
它基于一个在渐进条件 $N\rightarrow\infty$ 下成立的类似于
等是 7.24 的表达式：

$$ -2 \cdot E[\log\text{Pr}\_{\hat{\theta}}(Y)] \approx
-\frac{2}{N} \cdot E[\text{loglik}] + 2\cdot\frac{d}{N}
\tag{7.27}$$

其中 $\text{Pr}\_{\theta}(Y)$ 是 $Y$ 的一族密度函数（其中包含了真实的密度函数），
$\hat{\theta}$ 是 $\theta$ 的最大似然估计，
“loglik” 是最大化的对数似然度：

$$\text{loglik} = \sum\_{i=1}^N \log\text{Pr}\_{\hat{\theta}}(y\_i)
\tag{7.28}$$

例如，在对数几率回归模型中使用的是二项分布的对数似然函数：

$$\text{AIC} = -\frac{2}{N}\cdot\text{loglik} + 2\cdot\frac{d}{N}
\tag{7.29}$$

在高斯模型中（假设已知方差 $\sigma^2\_\varepsilon = \hat{\sigma}^2\_\varepsilon$，
则 AIC 统计量与 $C\_p$ 等价，
因此将它们统称为 AIC。

AIC 可用于模型选择，即从备选模型集合中选择 AIC 最小的模型。
对非线性和其他复杂模型，需要将 $d$ 替换为模型复杂度的某种度量，
这在第 7.6 节会介绍。

给定一个由调节参数 $\alpha$ 索引的函数 $f\_\alpha(x)$ 的集合，
记 $\overline{err}(\alpha)$ 和 $d(\alpha)$ 分别为每个模型的
训练误差和参数个数。
那么在这个模型集合上，定义：

$$\text{AIC}(\alpha) = \overline{\text{err}}(\alpha) +
2\cdot\frac{d(\alpha)}{N}\hat{\sigma}^2\_\varepsilon
\tag{7.29}$$

函数 $\text{AIC}(\alpha)$ 是测试误差曲线的估计，
通过对其最小化可找到对应的调节参数 $\hat{\alpha}$。
最终选择的模型就是 $f\_\hat{\alpha}(x)$。
需要注意如果以自适应的方式选择基函数，
那么表达式 7.23 不再成立。
例如，如果共有 $p$ 个输入变量，
选择的模型是 $d$ 个输入变量的最优子集线性模型，
那么乐观值会高于 $(2d/N)\sigma^2\_\varepsilon$。
换句话说，
如果从输入变量中选取 $d$ 个变量的最优拟合模型，
拟合的有效参数个数大于 $d$。

{{< figure
  src="http://public.guansong.wang/eslii/ch07/eslii_fig_07_04.png"
  title="**图7.4**：用于第 5.2.3 节中元音识别例子中使用 AIC 选择模型。对数几率回归系数函数的模型为 $M$ 个样条基函数的展开 $\beta(f) = \sum\_{m=1}^M h\_m(f)\theta\_m$。左图为使用对数似然函数的估计 $\text{Err}\_\text{in}$ 的 AIC 统计量。图中包括了基于独立测试样本的 $\text{Err}$ 的估计。除了在极端过参数化的情况外（$M=256$ 个参数，$N=1000$ 个样本），AIC 的估计比较准确。右图使用 0-1 损失的结果。虽然严格来说 AIC 表达式并不成立，但它的估计仍然比较合理。"
>}}

图 7.4 以第 148 页的
[第 5.2.3 节]({{< ref "/post/eslii/ch05/ch05_02.zh.md" >}})
中的元音识别为例展示了 AIC 的实际应用。
输入向量为在均匀分布的 256 个频率上量化的元音发音的对数周期律。
用线性对数逻辑回归模型来预测元音分类，
系数函数是 $M$ 个样条基函数的展开 $\beta(f) = \sum\_{m=1}^M h\_m(f)\theta\_m$。
对任意给定的 $M$，用自然三次样条基函数作为 $h\_m$，
选择的节点均匀分布在频率的范围上（故$d(\alpha) = d(M) = M$）。
利用 AIC 选择基函数的个数，
可以近似地最小化熵损失和 0-1 损失下的 $\text{Err}(M)$。

$$\frac{2}{N}\sum\_{i=1}^N \text{Cov}(\hat{y}\_i, y\_i) =
\frac{2d}{N} \sigma^2\_\varepsilon$$

上式对加性误差项以及平方误差损失的线性模型严格成立，
对线性模型和对数似然函数近似成立。
尽管它在 0-1 损失中并不一定成立（Efron, 1986），
但很多作者仍会在那些场景中使用这个关系式（图 7.4 的右图）。