+++
title = "ESL-7.3 偏差-方差分解"
summary = """
统计学习基础（译注）第七章第三节，第 223-228 页。
用理论分析、图示和实例展示一个模型与真实函数之间差异的分解。
其中的偏差和方差通常是此消彼涨的关系。
由于 0-1 损失的离散性，其预测误差的表现与平方误差损失不同。
"""

date = 2018-11-28T17:00:00+08:00
lastmod = 2018-11-28T17:00:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

如同第二章，
假设 $Y = f(X) + \varepsilon$，
$E(\varepsilon) = 0$，
$\text{Var}(\varepsilon) = \sigma^2\_\varepsilon$，
则可推导回归拟合 $\hat{f}(X)$ 在输入向量 $X = x\_0$ 点处
使用平方误差损失函数的期望预测误差：

$$\begin{align} \text{Err}(x\_0) &=
E[(Y - \hat{f}(x\_0))^2 | X = x\_0] \\\\ &=
\sigma^2\_\varepsilon + [E\hat{f}(x\_0) - f(x\_0)]^2 +
  E[\hat{f}(x\_0) - E\hat{f}(x\_0)]^2 \\\\ &=
\sigma^2\_\varepsilon + \text{Bias}^2(\hat{f}(x\_0)) +
  \text{Variance}^2(\hat{f}(x\_0)) \\\\ &=
\text{不可约误差} + \text{偏差}^2 + \text{方差}
\tag{7.9}\end{align}$$

其中第一项为输出变量以真实均值 $f(x\_0)$ 为中心的方差，
除非 $\sigma^2\_\varepsilon = 0$，
无论对 $f(x\_0)$ 的估计有多好，都无法避免这一项的存在。
第二项是平方偏差，衡量了估计的平均与真实均值之间的差距。
最后一项是方差，衡量了 $\hat{f}(x\_0)$ 距离其均值的期望平方偏差。
通常模型 $\hat{f}$ 越复杂，
（平方）偏差越低而方差越高。

在 k-近邻回归拟合中，对应的表达式可以简化为：

$$\begin{align} \text{Err}(x\_0) &=
E[(Y - \hat{f}\_k(x\_0))^2 | X = x\_0] \\\\ &=
\sigma^2\_\varepsilon +
\left[ f(x\_0) - \frac{1}{k}\sum\_{l=1}^k f(x\_{(l)}) \right]^2 +
\frac{\sigma^2\_\varepsilon}{k}
\tag{7.10}\end{align}$$

这里简单地假设训练集的输入变量 $x\_i$ 为固定的，
随机性全来自 $y\_i$。
模型复杂度与近邻的个数 $k$ 负相关。
当 $k$ 较小，模型估计 $\hat{f}\_k(x)$ 可能更好地根据潜在的 $f(x)$ 进行调整。
随着 $k$ 增大，
模型的偏差，即 $f(x\_0)$ 与 k-近邻的 $f(x)$ 平均值之间差的平方，
通常会增加，同时方差会降低。

在线性模型拟合中，$\hat{f}\_p(x) = x^t\hat{\beta}$，
其中 $p$ 个参数的向量 $\beta$ 由最小二乘拟合，则有：

$$\begin{align} \text{Err}(x\_0) &=
E[(Y - \hat{f}\_p(x\_0))^2 | X = x\_0] \\\\ &=
\sigma^2\_\varepsilon + [f(x\_0) - E\hat{f}\_p(x\_0)]^2 +
  \\|\mathbf{h}(x\_0)\\|^2 \sigma^2\_\varepsilon
\tag{7.11}\end{align}$$

其中 $\mathbf{h}(x\_0) = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}x\_0$，
即拟合 $\hat{f}\_p(x\_0) = x\_0^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$
中的线性权重 $N \times 1$ 向量，
因此 $\text{Var}[\hat{f}\_p(x\_0)] = \\|h(x\_0)\\|^2 \sigma^2\_\varepsilon$。
虽然这个方差随 $x\_0$ 变化，但它（让 $x\_0$ 取值在样本输入 $x\_i$ 上）的平均值是
$(p / N)\sigma^2\_\varepsilon$。
所以**样本内**（in-sample）误差为：

$$\frac{1}{N}\sum\_{i=1}^N \text{Err}(x\_i) = \sigma^2\_\varepsilon +
\frac{1}{N} \sum\_{i=1}^N [f(x\_i) - E\hat{f}(x\_i)]^2 +
\frac{p}{N}\sigma^2\_\varepsilon \tag{7.12}$$

这里的模型复杂度直接由参数的个数 $p$ 决定。

岭回归 $\hat{f}\_\alpha(x\_0)$ 的测试误差 $\text{Err}(x\_0)$ 与等式 7.11 形式相同，
只需要改变其中方差项的线性权重的定义：
$\mathbf{h}(x\_0) = \mathbf{X}(\mathbf{X}^T\mathbf{X} + \alpha\mathbf{I})^{-1}x\_0$。
同时偏差项也有区别。

在比如岭回归的线性模型族中，可以对偏差进一步细分。
记 $\beta\_\*$ 为对 $f$ 的最优拟合线性近似的参数：

$$ \beta\_\* = \underset{\beta}{\text{argmin}}
E \left( f(X) − X^T\beta \right)^2 \tag{7.13}$$

其中的期望是对输入变量 $X$ 的分布。
则可将平均平方偏差写为：

$$\begin{align}
&E\_{x\_0} \left[ f(x\_0) - E\hat{f}\_\alpha(x\_0) \right]^2 \\\\ &=
E\_{x\_0} \left[ f(x\_0) - x\_0^T\beta\_\* \right]^2 +
E\_{x\_0} \left[ x\_0^T\beta\_\* - Ex\_0^T\hat{\beta}\_\alpha \right]^2 \\\\ &=
\text{Ave}[\text{模型偏差}]^2 + \text{Ave}[\text{估计偏差}]^2
\tag{7.14}\end{align}$$

等式右边第一项为平均平方**模型偏差**（model bias），
即最优拟合线性近似与真实函数之间的偏差。
第二项为平均平方**估计偏差**（estimation bias），
即估计的平均 $E(x\_0^T\hat{\beta})$与最优拟合线性近似之间的偏差。

在线性模型的最小二乘拟合中，估计偏差为零。
在有约束的拟合中，比如岭回归，估计偏差大于零，
这是为了得到更低的方差付出的代价。
要降低模型偏差只能通过扩大线性模型的范畴以囊括更多类型的模型，
例如在模型中添加变量的交叉项或函数变换。

{{< figure
  src="http://public.guansong.wang/eslii/ch07/eslii_fig_07_02.png"
  title="**图7.2**：偏差和方差变化的示意图。模型空间是现有模型形式中所有可能的预测的集合，其中标记了“closest fit”的黑点为“最近拟合”。图中展示了模型与真实之间的差距，同时用标记为“closest fit in population”（总体最近拟合）的黑点周围的黄色大圆圈标记了模型的方差。图中的收缩或正则化拟合会有额外的估计偏差，但较小的方差使其预测误差更小。"
>}}

> #### 译者对上图的说明
> * Truth：$f(x)$。真实的模型，无法观测。
> * Realization：$\\{x\_i, y\_i\\}\_{i=1}^N$。
>   对模型的观测样本，
>   由于存在干扰项，所以与真实的模型有偏离。
> * MODEL SPACE：$\\{f\_\beta(x)|\beta \in B \subset \mathbb{R}^p\\}$。
>   模型空间，现有模型框架所能囊括的所有模型选择。
>   图中的红色曲线为空间的边界，曲线右侧的区域为待选模型。
> * Closest fit：$\hat{f}(x)$。最近拟合，
>   即模型空间中距离观测样本最近的点。
> * Closest fit in population：$E[\hat{f}(x)]$。
>   总体最近拟合，最近拟合对观测样本随机性的期望。
>   如果有很多个样本，那么它们的最近拟合的平均会不断接近总体最近拟合点。
> * RESTRICTED MODEL SPACE：$\\{f\_\beta(x)|\beta \in B'\subset B\\}$。
    有限制的模型空间，只在模型空间满足某些条件的子集上选择模型，
    比如限制自变量的个数或对系数的范式有约束（正则化）。
> * Shrunken fit：$\hat{f}^{\text{ridge/lasso}}(x)$。
>   在有限制的模型空间中，距离观测样本最近的模型。
> * Model bias：$\|f(x) - E[\hat{f}(x)]\|$。
>   模型偏差，真实模型与模型空间（最近点）之间的差距。
> * Estimation bias：$\|\hat{f}\_\tilde{\beta}(x) - E[\hat{f}(x)]\|$。
    估计偏差，总体最近拟合与具体使用的估计方法所对应的（有限制）模型空间（最近点）之间的差距。
> * Estimation variance：$\text{Var}(\hat{f}\_\tilde{\beta})$。
>   估计方差。

图 7.2 为偏差-方差权衡的示意图。
以线性模型为例，模型空间为所有基于 $p$ 个输入变量的线性预测的集合，
标记着“closest fit”（最近拟合）的黑点为 $x^T\beta\_\*$。
蓝色阴影区域表示了噪声项的影响 $\sigma\_\varepsilon$，
训练样本的观测点即为真实模型加上了噪声的影响。

图中以标记为“closest fit in population”（总体最近拟合）为中心的黄色大圆圈，
表示着最小二乘拟合的方差。
如果现在要用更少的自变量拟合模型，
或通过向零收缩对系数正则化，
则得到的是图中的“shrunken fit”（收缩拟合）。
由于它不是模型空间上的最近拟合，这个拟合中会有额外的估计偏差。
但另一方面，它的方差更小。
若方差的降低大于（平方）偏差的增加，那这个取舍就是值得的。

### 7.3.1 实例：偏差-方差权衡

{{< figure
  src="http://public.guansong.wang/eslii/ch07/eslii_fig_07_03.png"
  title="**图7.3**：一个模拟例子的期望预测误差（橙色）、平方偏差（绿色）和方差（蓝色）。上面一行是使用平方误差损失函数的回归问题；下面一行是使用 0-1 损失的分类问题。左侧图使用 k-近邻，右侧图使用 $p$ 个变量的最优子集回归。在回归和分类场景中欧给你的方差和偏差曲线都相同，但预测误差曲线不同。"
>}}

图 7.3 展示了两个模拟例子中的偏差-方差权衡。
其中有 80 个观测点和 20 个在超立方体 $[0,1]^{20}$ 上均匀分布的自变量，
真实输出变量的生成方式如下：

* 左边：若 $X\_1 \leq 1 / 2$，则 $Y=0$；若 $X\_1 > 1 / 2$，则 $Y=1$。
  并且使用 k-近邻方法。
* 右边：若 $\sum\_{j=1}^{10} X\_j > 5$，则 $Y=1$；否则，$Y=0$。
  并且使用 $p$ 个自变量的最优子集线性回归。

顶部一行是回归的平方误差损失；
底部一行是分类的 0-1 损失。
图中展示了在一个足够大的测试集上计算的预测误差（红色）、平方偏差（绿色）和方差（蓝色）。

在回归问题中，预测误差曲线为偏差和方差的和，
在 k-近邻中其最低点为 $k=5$，在线性模型中最低点为 $ p \geq 10$。
在分类问题中（下面二图）出现了有趣的现象。
偏差和方差曲线与上面二图完全相同，
而这里的预测误差为错误分类比率，
其曲线不再是平方偏差和方差曲线之和。
在 k-近邻分类器中，
近邻数量增加到 20 之后，
尽管平方偏差仍在上升，预测误差却略有下降。
在线性模型分类器中，
最低点和回归问题一样出现在 $p \geq 10$，
但模型从 $p=1$ 开始的提升幅度非常巨大。
偏差和方差在决定预测误差上看似有相互的牵制。

为什么会如此？对预测误差曲线的改变有一个简单的解释。
假设给定一个输入变量点，
其分类为 1 的真实概率为 0.9 而模型估计的期望值为 0.6。
则此时存在合理的平方偏差 $(0.6 - 0.9)^2$，
但由于两者都给会给出正确的分类，预测误差为灵。
或者说，在判别边界同一边的统计误差不会影响预测误差。
练习 7.2 从理论分析演示了这个现象，
并展示了偏差和方差之间的相互影响。

总的来说，偏差-方差权衡在 0-1 损害和平方误差损失中的表现不同。
这意味着在两个不同的场景中调节参数的最优选择可能截然不同。
如下面章节的讨论，条件参数的选择要基于预测误差的估计。