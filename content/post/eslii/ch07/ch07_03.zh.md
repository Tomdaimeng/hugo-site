+++
title = "ESL-7.3 偏差-方差分解"
summary = """
统计学习基础（译注）第七章第三节，第 223-228 页。
"""

date = 2018-11-27T17:40:00+08:00
lastmod = 2018-11-27T17:40:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

As in Chapter 2, if we assume that Y = f (X) + ε where E(ε) = 0 and
Var(ε) = σ ε 2 , we can derive an expression for the expected prediction error
of a regression fit f ˆ (X) at an input point X = x 0 , using squared-error loss:

$$\begin{align} \text{Err}(x\_0) &=
E[(Y - \hat{f}(x\_0))^2 | X = x\_0] \\\\ &=
\sigma^2\_\varepsilon + [E\hat{f}(x\_0) - f(x\_0)]^2 +
  E[\hat{f}(x\_0) - Ef(x\_0)]^2 \\\\ &=
\sigma^2\_\varepsilon + \text{Bias}^2(\hat{f}(x\_0)) +
  \text{Variance}^2(\hat{f}(x\_0)) \\\\ &=
\text{Irreducible Error} + \text{Bias}^2 + \text{Variance}
\tag{7.9}\end{align}$$

The first term is the variance of the target around its true mean f (x 0 ), and
cannot be avoided no matter how well we estimate f (x 0 ), unless σ ε 2 = 0.
The second term is the squared bias, the amount by which the average of
our estimate differs from the true mean; the last term is the variance; the
expected squared deviation of f ˆ (x 0 ) around its mean. Typically the more
complex we make the model f ˆ , the lower the (squared) bias but the higher
the variance.

For the k-nearest-neighbor regression fit, these expressions have the sim-
ple form

$$\begin{align} \text{Err}(x\_0) &=
E[(Y - \hat{f}\_k(x\_0))^2 | X = x\_0] \\\\ &=
\sigma^2\_\varepsilon +
\left[ f(x\_0) - \frac{1}{k}\sum\_{l=1}^k f(x\_{(l)}) \right]^2 +
\frac{\sigma^2\_\varepsilon}{k}
\tag{7.10}\end{align}$$

Here we assume for simplicity that training inputs x i are fixed, and the ran-
domness arises from the y i . The number of neighbors k is inversely related
to the model complexity. For small k, the estimate f ˆ k (x) can potentially
adapt itself better to the underlying f (x). As we increase k, the bias—the
squared difference between f (x 0 ) and the average of f (x) at the k-nearest
neighbors—will typically increase, while the variance decreases.

For a linear model fit f ˆ p (x) = x T β̂, where the parameter vector β with
p components is fit by least squares, we have

$$\begin{align} \text{Err}(x\_0) &=
E[(Y - \hat{f}\_p(x\_0))^2 | X = x\_0] \\\\ &=
\sigma^2\_\varepsilon + [f(x\_0) - E\hat{f}\_p(x\_0)]^2 +
  \\|\mathbf{h}(x\_0)\\|^2 \sigma^2\_\varepsilon
\tag{7.11}\end{align}$$

the in-sample error. Here model complexity is directly related to the num-
ber of parameters p.

The test error Err(x 0 ) for a ridge regression fit f ˆ α (x 0 ) is identical in
form to (7.11), except the linear weights in the variance term are different:
h(x 0 ) = X(X T X + αI) T x 0 . The bias term will also be different.

For a linear model family such as ridge regression, we can break down
the bias more finely. Let β ∗ denote the parameters of the best-fitting linear
approximation to f :

$$ \beta\_\* = \underset{\beta}{\text{argmin}}
E \left( f(X) − X^T\beta \right)^2 \tag{7.13}$$

Here the expectation is taken with respect to the distribution of the input
variables X. Then we can write the average squared bias as

$$\begin{align}
&E\_{x\_0} \left[ f(x\_0) - E\hat{f}\_\alpha(x\_0) \right]^2 \\\\ &=
E\_{x\_0} \left[ f(x\_0) - x\_0^T\beta\_\* \right]^2 +
E\_{x\_0} \left[ x\_0^T\beta\_\* - Ex\_0^T\hat{\beta}\_\alpha \right]^2 \\\\ &=
\text{Ave}[\text{Model Bias}]^2 + \text{Ave}[\text{Estimation Bias}]^2
\tag{7.14}\end{align}$$

The first term on the right-hand side is the average squared model bias, the
error between the best-fitting linear approximation and the true function.
The second term is the average squared estimation bias, the error between
the average estimate E(x T 0 β̂) and the best-fitting linear approximation.

For linear models fit by ordinary least squares, the estimation bias is zero.
For restricted fits, such as ridge regression, it is positive, and we trade it off
with the benefits of a reduced variance. The model bias can only be reduced
by enlarging the class of linear models to a richer collection of models, by
including interactions and transformations of the variables in the model.

{{< figure
  src="http://public.guansong.wang/eslii/ch07/eslii_fig_07_02.png"
  title="**图7.2**："
>}}

Figure 7.2 shows the bias–variance tradeoff schematically. In the case
of linear models, the model space is the set of all linear predictions from
p inputs and the black dot labeled “closest fit” is x T β ∗ . The blue-shaded
region indicates the error σ ε with which we see the truth in the training
sample.

Also shown is the variance of the least squares fit, indicated by the large
yellow circle centered at the black dot labeled “closest fit in population,’
Now if we were to fit a model with fewer predictors, or regularize the coef-
ficients by shrinking them toward zero (say), we would get the “shrunken
fit” shown in the figure. This fit has an additional estimation bias, due to
the fact that it is not the closest fit in the model space. On the other hand,
it has smaller variance. If the decrease in variance exceeds the increase in
(squared) bias, then this is worthwhile.

### 7.3.1 Example: Bias–Variance Tradeoff

{{< figure
  src="http://public.guansong.wang/eslii/ch07/eslii_fig_07_03.png"
  title="**图7.3**："
>}}

Figure 7.3 shows the bias–variance tradeoff for two simulated examples.
There are 80 observations and 20 predictors, uniformly distributed in the
hypercube [0, 1] 20 . The situations are as follows:

* Left panels: Y is 0 if X 1 ≤ 1/2 and 1 if X 1 > 1/2, and we apply k-nearest
neighbors.
* Right panels: Y is 1 if j=1 X j is greater than 5 and 0 otherwise, and we
use best subset linear regression of size p.

The top row is regression with squared error loss; the bottom row is classi-
fication with 0–1 loss. The figures show the prediction error (red), squared
bias (green) and variance (blue), all computed for a large test sample.

In the regression problems, bias and variance add to produce the predic-
tion error curves, with minima at about k = 5 for k-nearest neighbors, and
p ≥ 10 for the linear model. For classification loss (bottom figures), some
interesting phenomena can be seen. The bias and variance curves are the
same as in the top figures, and prediction error now refers to misclassifi-
cation rate. We see that prediction error is no longer the sum of squared
bias and variance. For the k-nearest neighbor classifier, prediction error
decreases or stays the same as the number of neighbors is increased to 20,
despite the fact that the squared bias is rising. For the linear model classi-
fier the minimum occurs for p ≥ 10 as in regression, but the improvement
over the p = 1 model is more dramatic. We see that bias and variance seem
to interact in determining prediction error.

Why does this happen? There is a simple explanation for the first phe-
nomenon. Suppose at a given input point, the true probability of class 1 is
0.9 while the expected value of our estimate is 0.6. Then the squared bias—
(0.6 − 0.9) 2 —is considerable, but the prediction error is zero since we make
the correct decision. In other words, estimation errors that leave us on the
right side of the decision boundary don’t hurt. Exercise 7.2 demonstrates
this phenomenon analytically, and also shows the interaction effect between
bias and variance.

The overall point is that the bias–variance tradeoff behaves differently
for 0–1 loss than it does for squared error loss. This in turn means that
the best choices of tuning parameters may differ substantially in the two
settings. One should base the choice of tuning parameter on an estimate of
prediction error, as described in the following sections.