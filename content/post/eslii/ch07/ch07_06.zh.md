+++
title = "ESL-7.6 参数的有效个数The Effective Number of Parameters"
summary = """
统计学习基础（译注）第七章第六节，第 232-233 页。
"""

date = 2018-12-01T17:11:00+08:00
lastmod = 2018-12-01T17:11:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The concept of “number of parameters” can be generalized, especially to
models where regularization is used in the fitting. Suppose we stack the
outcomes y 1 , y 2 , . . . , y N into a vector y, and similarly for the predictions
ŷ. Then a linear fitting method is one for which we can write

$$\hat{y}\mathbf{y} = \mathbf{S}\mathbf{y} \tag{7.31}$$

where S is an N × N matrix depending on the input vectors x i but not on
the y i . Linear fitting methods include linear regression on the original fea-
tures or on a derived basis set, and smoothing methods that use quadratic
shrinkage, such as ridge regression and cubic smoothing splines. Then the
effective number of parameters is defined as

$$\text{df}(\mathbf{S}) = \text{trace}(\mathbf{S}) \tag{7.32}$$

the sum of the diagonal elements of S (also known as the effective degrees-
of-freedom). Note that if S is an orthogonal-projection matrix onto a basis
set spanned by M features, then trace(S) = M . It turns out that trace(S) is
exactly the correct quantity to replace d as the number of parameters in the
C p statistic (7.26). If y arises from an additive-error model Y = f (X) + ε
with Var(ε) = σ ε 2 , then one can show that i=1 Cov(ŷ i , y i ) = trace(S)σ ε 2 ,
which motivates the more general definition

$$\text{df}(\hat{\mathbf{y}}) = \frac
{\sum\_{i=1}^N \text{Cov}(\hat{y}\_i, y\_i)}{\sigma^2\_\varepsilon}
\tag{7.33}$$

(Exercises 7.4 and 7.5). Section 5.4.1 on page 153 gives some more intuition
for the definition df = trace(S) in the context of smoothing splines.

For models like neural networks, in which we minimize an  error function
R(w) with weight decay penalty (regularization) α m w m, the effective
number of parameters has the form

$$\text{df}(\alpha) = \sum\_{m=1}^M
\frac{\theta\_m}{\theta\_m + \alpha)} \tag{7.34}$$

where the θ m are the eigenvalues of the Hessian matrix ∂ 2 R(w)/∂w∂w T .
Expression (7.34) follows from (7.32) if we make a quadratic approximation
to the error function at the solution (Bishop, 1995).