+++
title = "ESL-7.6 有效参数个数"
summary = """
统计学习基础（译注）第七章第六节，第 232-233 页。
将线性回归中参数个数的概念拓展到更一般的模型中，
即有效参数个数或有效自由度。
"""

date = 2018-12-02T14:46:00+08:00
lastmod = 2018-12-02T14:46:00+08:00
draft = false
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

可以将“参数个数”的概念一般化，
特别是在使用了正则化拟合的模型中。
假设 $\mathbf{y}$ 为输出变量 $y\_1$，$y\_2$，……，$y\_N$ 组成的向量，
预测的结果 $\hat{\mathbf{y}}$ 与之类似。
那么线性拟合模型可表达为：

$$\hat{y}\mathbf{y} = \mathbf{S}\mathbf{y} \tag{7.31}$$

其中的 $\mathbf{S}$ 是 $N \times N$ 的矩阵，
依赖于输入向量 $x\_i$ 但不依赖于 y\_i$。
线性拟合方法包括使用原始特征或生成的基函数集合的线性回归，
以及使用了二次收缩的平滑方法，比如岭回归和三次平滑样条。
**有效参数个数**（effective number of parameters）可定义为：

$$\text{df}(\mathbf{S}) = \text{trace}(\mathbf{S}) \tag{7.32}$$

即为 $\mathbf{S}$ 的对角线元素之和，
也被称为**有效自由度**（effective degrees-of-freedom）。
注意如果 $\mathbf{S}$ 为一个到
$M$ 个特征生成的基函数集合的正交投影矩阵，
那么 $\text{trace}(\mathbf{S}) = M$。
这样的话，在，
$\text{trace}(\mathbf{S})$ 的数值恰好等于
等式 7.26 的 $C\_p$ 统计量中的参数个数 $d$。
如果 $\mathbf{y}$ 来自于加性误差模型 $Y=f(X)+\varepsilon$，
其中 $\text{Var}(\varepsilon) = \sigma^2\_\varepsilon$，
则可以证明
$\sum\_{i=1}^N \text{Cov}(\hat{y\_i}, y\_i) =
\text{trace}(\mathbf{S})\sigma^2\_\varepsilon$，
这帮助引入了一个更一般化的定义：

$$\text{df}(\hat{\mathbf{y}}) = \frac
{\sum\_{i=1}^N \text{Cov}(\hat{y}\_i, y\_i)}{\sigma^2\_\varepsilon}
\tag{7.33}$$

（见练习 7.4 和 7.5）。
第 153 页的[第 5.4.1 节]({{< ref "/post/eslii/ch05/ch05_04.zh.md" >}})
在平滑样条的场景中对 $\text{df} = \text{trace}(\mathbf{S})$
做了一些直觉上的说明。

在类似于神经网络的模型中，
会对一个含有权重衰减惩罚（正则）项 $\alpha\sum\_m w\_m^2$
的误差函数 $R(w)$ 最小化，
它的有效参数个数可写为：

$$\text{df}(\alpha) = \sum\_{m=1}^M
\frac{\theta\_m}{\theta\_m + \alpha)} \tag{7.34}$$

其中 $\theta\_m$ 为海森（Hessian）矩阵
$\partial^2 R(w) / \partial w \partial w^T$
的特征值。
如果利用在最优解处误差函数的二次近似，
则可从等式 7.32 得出表达式 7.34（Bishop, 1995）。