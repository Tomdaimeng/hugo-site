+++
title = "ESL-12.7 Mixture Discriminant Analysis"
summary = """
统计学习基础（译注）第十二章第七节，第 449-454 页。
"""

date = 2019-03-24T15:11:00+08:00
lastmod = 2019-03-24T15:11:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Linear discriminant analysis can be viewed as a prototype classifier. Each
class is represented by its centroid, and we classify to the closest using an
appropriate metric. In many situations a single prototype is not sufficient
to represent inhomogeneous classes, and mixture models are more appro-
priate. In this section we review Gaussian mixture models and show how
they can be generalized via the FDA and PDA methods discussed earlier.
A Gaussian mixture model for the kth class has density

$$P(X|G=k) = \sum\_{r=1}^{R\_k}
\pi\_{kr} \phi(X; \mu\_{kr}, \mathbf{\Sigma}) \tag{12.59}$$

where the mixing proportions π kr sum to one. This has R k prototypes for
the kth class, and in our specification, the same covariance matrix Σ is
used as the metric throughout. Given such a model for each class, the class
posterior probabilities are given by

$$P(G=k|X=x) = \frac
{\sum\_{r=1}^{R\_k} \pi\_{kr} \phi(X;\mu\_{kr},\mathbf{\Sigma}) \Pi\_k}
{\sum\_{\ell=1}^K \sum\_{r=1}^{R\_\ell}
\pi\_{\ell r} \phi(X;\mu\_{\ell r},\mathbf{\Sigma}) \Pi\_\ell}
\tag{12.60}$$

where Π k represent the class prior probabilities.

We saw these calculations for the special case of two components in
Chapter 8. As in LDA, we estimate the parameters by maximum likelihood,
using the joint log-likelihood based on P (G, X):

$$\sum\_{k=1}^K \sum\_{g\_i=k} \log \left[
\sum\_{r=1}^{R\_k} \pi\_{kr} \phi(x\_i;\mu\_{kr},\mathbf{\Sigma}) \Pi\_k
\right]\tag{12.61}$$

The sum within the log makes this a rather messy optimization problem
if tackled directly. The classical and natural method for computing the
maximum-likelihood estimates (MLEs) for mixture distributions is the EM
algorithm (Dempster et al., 1977), which is known to possess good conver-
gence properties. EM alternates between the two steps:

- E-step: Given the current parameters, compute the responsibility of sub-
class c kr within class k for each of the class-k observations (g i = k):
$$W(c\_{kr}|x\_i,g\_i) = \frac
{\pi\_{kr}\phi(x\_i;\mu\_{kr},\mathbf{\Sigma})}
{\sum\_{\ell=1}^{R\_k} \pi\_{k\ell}\phi(x\_i;\mu\_{k\ell},\mathbf{\Sigma})}
\tag{12.62}$$
- M-step: Compute the weighted MLEs for the parameters of each of the
component Gaussians within each of the classes, using the weights
from the E-step.

In the E-step, the algorithm apportions the unit weight of an observation
in class k to the various subclasses assigned to that class. If it is close to the
centroid of a particular subclass, and far from the others, it will receive a
mass close to one for that subclass. On the other hand, observations halfway
between two subclasses will get approximately equal weight for both.

In the M-step, an observation in class k is used R k times, to estimate the
parameters in each of the R k component densities, with a different weight
for each. The EM algorithm is studied in detail in Chapter 8. The algorithm
requires initialization, which can have an impact, since mixture likelihoods
are generally multimodal. Our software (referenced in the Computational
Considerations on page 455) allows several strategies; here we describe the
default. The user supplies the number R k of subclasses per class. Within
class k, a k-means clustering model, with multiple random starts, is fitted
to the data. This partitions the observations into R k disjoint groups, from
which an initial weight matrix, consisting of zeros and ones, is created.

Our assumption of an equal component covariance matrix Σ throughout
buys an additional simplicity; we can incorporate rank restrictions in the
mixture formulation just like in LDA. To understand this, we review a little-
known fact about LDA. The rank-L LDA fit (Section 4.3.3) is equivalent to
the maximum-likelihood fit of a Gaussian model,where the different mean
vectors in each class are confined to a rank-L subspace of IR p (Exercise 4.8).
We can inherit this property for the mixture model, and maximize the log-
likelihood (12.61) subject to rank constraints on all the k R k centroids:
rank{μ kl } = L.

Again the EM algorithm is available, and the M-step turns out to be
a weighted version of LDA, with R = k=1 R k “classes.” Furthermore,
we can use optimal scoring as before to solve the weighted LDA problem,
which allows us to use a weighted version of FDA or PDA at this stage.
One would expect, in addition to an increase in the number of “classes,” a
similar increase in the number of “observations” in the kth class by a factor
of R k . It turns out that this is not the case if linear operators are used for
the optimal scoring regression. The enlarged indicator Y matrix collapses
in this case to a blurred response matrix Z, which is intuitively pleasing.
For example, suppose there are K = 3 classes, and R k = 3 subclasses per
class. Then Z might be

$$\begin{matrix}
 & \begin{matrix}c\_{11}&c\_{12}&c\_{13}&c\_{21}&c\_{22}&c\_{23}&c\_{31}&c\_{32}&c\_{33}\end{matrix}
\\\\ \begin{matrix}g\_1=2\\\\g\_2=1\\\\g\_3=1\\\\g\_4=3\\\\g\_5=2\\\\\vdots\\\\g\_N=3\end{matrix}
& \begin{pmatrix}
  0 & 0 & 0 & 0.3 & 0.5 & 0.2 & 0 & 0 & 0
  \\\\ 0.9 & 0.1 & 0.0 & 0 & 0 & 0 & 0 & 0 & 0
  \\\\ 0.1 & 0.8 & 0.1 & 0 & 0 & 0 & 0 & 0 & 0
  \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0.5 & 0.4 & 0.1
  \\\\ 0 & 0 & 0 & 0.7 & 0.1 & 0.2 & 0 & 0 & 0
  \\\\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots
  \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0.1 & 0.1 & 0.8
\end{pmatrix}
\end{matrix}\tag{12.63}$$

where the entries in a class-k row correspond to W (c kr |x, g i ).

The remaining steps are the same:

$$\text{M-step of MDA}$$

These simple modifications add considerable flexibility to the mixture
model:

- The dimension reduction step in LDA, FDA or PDA is limited by
the number of classes; in particular, for K = 2 classes no reduction is
possible. MDA substitutes subclasses for classes, and then allows us
to look at low-dimensional views of the subspace spanned by these
subclass centroids. This subspace will often be an important one for
discrimination.
- By using FDA or PDA in the M-step, we can adapt even more to par-
ticular situations. For example, we can fit MDA models to digitized
analog signals and images, with smoothness constraints built in.

Figure 12.13 compares FDA and MDA on the mixture example.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_13.png"
  title="**图12.13**："
>}}
FDA and MDA on the mixture data. The upper plot uses
FDA with MARS as the regression procedure. The lower plot uses MDA with
five mixture centers per class (indicated). The MDA solution is close to Bayes
optimal, as might be expected given the data arise from mixtures of Gaussians.
The broken purple curve in the background is the Bayes decision boundary.

### 12.7.1 Example: Waveform Data

We now illustrate some of these ideas on a popular simulated example,
taken from Breiman et al. (1984, pages 49–55), and used in Hastie and
Tibshirani (1996b) and elsewhere. It is a three-class problem with 21 vari-
ables, and is considered to be a difficult pattern recognition problem. The
predictors are defined by

$$\begin{align}
X\_j &= U h\_1(j) + (1-U) h\_2(j) + \epsilon\_j  & \text{ Class 1}
\\\\ X\_j &= U h\_1(j) + (1-U) h\_3(j) + \epsilon\_j  & \text{ Class 2}
\\\\ X\_j &= U h\_2(j) + (1-U) h\_3(j) + \epsilon\_j  & \text{ Class 2}
\end{align}\tag{12.64}$$

where j = 1, 2, . . . , 21, U is uniform on (0, 1), ǫ j are standard normal vari-
ates, and the h l are the shifted triangular waveforms: h 1 (j) = max(6 −
|j − 11|, 0), h 2 (j) = h 1 (j − 4) and h 3 (j) = h 1 (j + 4). Figure 12.14 shows
some example waveforms from each class.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_14.png"
  title="**图12.14**："
>}}
Some examples of the waveforms generated from model (12.64)
before the Gaussian noise is added.

|               |  Error Rates | Error Rates |
|---------------|--------------|-------------|
| **Technique** | **Training** | **Test**    |
| LDA                                | 0.121(0.006) | 0.191(0.006) |
| QDA                                | 0.039(0.004) | 0.205(0.006) |
| CART                               | 0.072(0.003) | 0.289(0.004) |
| FDA/MARS (degree = 1)              | 0.100(0.006) | 0.191(0.006) |
| FDA/MARS (degree = 2)              | 0.068(0.004) | 0.215(0.002) |
| MDA (3 subclasses)                 | 0.087(0.005) | 0.169(0.006) |
| MDA (3 subclasses, penalized 4 df) | 0.137(0.006) | 0.157(0.005) |
| PDA (penalized 4 df)               | 0.150(0.005) | 0.171(0.005) |
| Bayes                              |              | 0.140        |
**TABLE 12.4**. Results for waveform data. The values are averages over ten sim-
ulations, with the standard error of the average in parentheses. The five entries
above the line are taken from Hastie et al. (1994). The first model below the line
is MDA with three subclasses per class. The next line is the same, except that the
discriminant coefficients are penalized via a roughness penalty to effectively 4df.
The third is the corresponding penalized LDA or PDA model.


Table 12.4 shows the results of MDA applied to the waveform data, as
well as several other methods from this and other chapters. Each train-
ing sample has 300 observations, and equal priors were used, so there are
roughly 100 observations in each class. We used test samples of size 500.
The two MDA models are described in the caption.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_15.png"
  title="**图12.15**："
>}}
Some two-dimensional views of the MDA model fitted to a
sample of the waveform model. The points are independent test data, projected
on to the leading two canonical coordinates (left panel), and the third and fourth
(right panel). The subclass centers are indicated.

Figure 12.15 shows the leading canonical variates for the penalized MDA
model, evaluated at the test data. As we might have guessed, the classes
appear to lie on the edges of a triangle. This is because the h j (i) are repre-
sented by three points in 21-space, thereby forming vertices of a triangle,
and each class is represented as a convex combination of a pair of vertices,
and hence lie on an edge. Also it is clear visually that all the information
lies in the first two dimensions; the percentage of variance explained by the
first two coordinates is 99.8%, and we would lose nothing by truncating the
solution there. The Bayes risk for this problem has been estimated to be
about 0.14 (Breiman et al., 1984). MDA comes close to the optimal rate,
which is not surprising since the structure of the MDA model is similar to
the generating model.