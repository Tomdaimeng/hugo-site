+++
title = "ESL-12.4 Generalizing Linear Discriminant Analysis"
summary = """
统计学习基础（译注）第十二章第四节，第 438-440 页。
"""

date = 2019-03-22T16:21:00+08:00
lastmod = 2019-03-22T16:21:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In Section 4.3 we discussed linear discriminant analysis (LDA), a funda-
mental tool for classification. For the remainder of this chapter we discuss
a class of techniques that produce better classifiers than LDA by directly
generalizing LDA.

Some of the virtues of LDA are as follows:

- It is a simple prototype classifier. A new observation is classified to the
class with closest centroid. A slight twist is that distance is measured
in the Mahalanobis metric, using a pooled covariance estimate.
- LDA is the estimated Bayes classifier if the observations are multi-
variate Gaussian in each class, with a common covariance matrix.
Since this assumption is unlikely to be true, this might not seem to
be much of a virtue.
- The decision boundaries created by LDA are linear, leading to deci-
sion rules that are simple to describe and implement.
- LDA provides natural low-dimensional views of the data. For exam-
ple, Figure 12.12 is an informative two-dimensional view of data in
256 dimensions with ten classes.
- Often LDA produces the best classification results, because of its
simplicity and low variance. LDA was among the top three classifiers
for 11 of the 22 datasets studied in the STATLOG project (Michie et
al., 1994)[^1].

Unfortunately the simplicity of LDA causes it to fail in a number of situa-
tions as well:

- Often linear decision boundaries do not adequately separate the classes.
When N is large, it is possible to estimate more complex decision
boundaries. Quadratic discriminant analysis (QDA) is often useful
here, and allows for quadratic decision boundaries. More generally
we would like to be able to model irregular decision boundaries.
- The aforementioned shortcoming of LDA can often be paraphrased
by saying that a single prototype per class is insufficient. LDA uses
a single prototype (class centroid) plus a common covariance matrix
to describe the spread of the data in each class. In many situations,
several prototypes are more appropriate.
- At the other end of the spectrum, we may have way too many (corre-
lated) predictors, for example, in the case of digitized analogue signals
and images. In this case LDA uses too many parameters, which are
estimated with high variance, and its performance suffers. In cases
such as this we need to restrict or regularize LDA even further.

In the remainder of this chapter we describe a class of techniques that
attend to all these issues by generalizing the LDA model. This is achieved
largely by three different ideas.

The first idea is to recast the LDA problem as a linear regression problem.
Many techniques exist for generalizing linear regression to more flexible,
nonparametric forms of regression. This in turn leads to more flexible forms
of discriminant analysis, which we call FDA. In most cases of interest, the
regression procedures can be seen to identify an enlarged set of predictors
via basis expansions. FDA amounts to LDA in this enlarged space, the
same paradigm used in SVMs.

In the case of too many predictors, such as the pixels of a digitized image,
we do not want to expand the set: it is already too large. The second idea is
to fit an LDA model, but penalize its coefficients to be smooth or otherwise
coherent in the spatial domain, that is, as an image. We call this procedure
penalized discriminant analysis or PDA. With FDA itself, the expanded
basis set is often so large that regularization is also required (again as in
SVMs). Both of these can be achieved via a suitably regularized regression
in the context of the FDA model.

The third idea is to model each class by a mixture of two or more Gaus-
sians with different centroids, but with every component Gaussian, both
within and between classes, sharing the same covariance matrix. This allows
for more complex decision boundaries, and allows for subspace reduction
as in LDA. We call this extension mixture discriminant analysis or MDA.
All three of these generalizations use a common framework by exploiting
their connection with LDA.

[^1]: This study predated the emergence of SVMs.