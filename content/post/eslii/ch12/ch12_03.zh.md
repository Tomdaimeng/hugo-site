+++
title = "ESL-12.3 Support Vector Machines and Kernels"
summary = """
统计学习基础（译注）第十二章第三节，第 423-438 页。
"""

date = 2019-03-22T16:01:00+08:00
lastmod = 2019-03-22T16:01:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

The support vector classifier described so far finds linear boundaries in the
input feature space. As with other linear methods, we can make the pro-
cedure more flexible by enlarging the feature space using basis expansions
such as polynomials or splines (Chapter 5). Generally linear boundaries
in the enlarged space achieve better training-class separation, and trans-
late to nonlinear boundaries in the original space. Once the basis functions
h m (x), m = 1, . . . , M are selected, the procedure is the same as before. We
fit the SV classifier using input features h(x i ) = (h 1 (x i ), h 2 (x i ), . . . , h M (x i )),
i = 1, . . . , N , and produce the (nonlinear) function f ˆ (x) = h(x) T β̂ + β ˆ 0 .
The classifier is Ĝ(x) = sign( f ˆ (x)) as before.

The support vector machine classifier is an extension of this idea, where
the dimension of the enlarged space is allowed to get very large, infinite
in some cases. It might seem that the computations would become pro-
hibitive. It would also seem that with sufficient basis functions, the data
would be separable, and overfitting would occur. We first show how the
SVM technology deals with these issues. We then see that in fact the SVM
classifier is solving a function-fitting problem using a particular criterion
and form of regularization, and is part of a much bigger class of problems
that includes the smoothing splines of Chapter 5. The reader may wish
to consult Section 5.8, which provides background material and overlaps
somewhat with the next two sections.

### 12.3.1 Computing the SVM for Classification

We can represent the optimization problem (12.9) and its solution in a
special way that only involves the input features via inner products. We do
this directly for the transformed feature vectors h(x i ). We then see that for
particular choices of h, these inner products can be computed very cheaply.

The Lagrange dual function (12.13) has the form

$$L\_D = \sum\_{i=1}^N \alpha\_i -
\frac{1}{2} \sum\_{i=1}^N \sum\_{i'=1}^N
\alpha\_i \alpha\_{i'} y\_i y\_{i'}
\langle h(x\_i), h(x\_{i'}) \rangle \tag{12.19}$$

From (12.10) we see that the solution function f (x) can be written

$$\begin{align} f(x)
&= h(x)^T \beta + \beta\_0
\\\\ &= \sum\_{i=1}^N \alpha\_i y\_i \langle h(x\_i), h(x\_{i'} \rangle
+ \beta\_0 \tag{12.20}\end{align}$$

As before, given α i , β 0 can be determined by solving y i f (x i ) = 1 in (12.20)
for any (or all) x i for which 0 < α i < C.

So both (12.19) and (12.20) involve h(x) only through inner products. In
fact, we need not specify the transformation h(x) at all, but require only
knowledge of the kernel function

$$K(x, x') = \langle h(x\_i), h(x\_{i'}) \rangle \tag{12.21}$$

that computes inner products in the transformed space. K should be a
symmetric positive (semi-) definite function; see Section 5.8.1.

Three popular choices for K in the SVM literature are

$$\begin{align}
\text{dth-Degree polynomial}
&: K(x, x') = (1 + \langle x, x' \rangle)^d
\\\\ \text{Radial basis}
&: K(x, x') = \exp(-\gamma \\|x - x'\\|^2)
\\\\ \text{Neural network}
&: K(x, x') = \tanh(\kappa\_1 \langle x, x' \rangle + \kappa\_2)
\end{align}\tag{12.22}$$

Consider for example a feature space with two inputs X 1 and X 2 , and a
polynomial kernel of degree 2. Then

$$\begin{align} K(X, X')
=& (1 + \langle X, X' \rangle)^2
\\\\ =& (1 + X\_1 X'\_1 + X\_2 X'\_2)^2
\\\\ =& 1 + 2 X\_1 X'\_1 + 2 X\_2 X'\_2 +
\\\\ & (X\_1 X'\_1)^2 + (X\_2 X'\_2)^2 + 2 X\_1 X'\_1 X\_2 X'\_2
\tag{12.23}\end{align}$$

Then M = 6, and if we choose h 1 (X) = 1, h 2 (X) = 2X 1 , h 3 (X) =
√2X 2 , h 4 (X) = X 1 2 , h 5 (X) = X 2 2 , and h 6 (X) = 2X 1 X 2 , then K(X, X ′ ) =
hh(X), h(X ′ )i. From (12.20) we see that the solution can be written

$$\hat{f}(x) = \sum\_{i=1}^N \hat{\alpha}\_i y\_i K(x, x\_i)
+ \hat{\beta}\_0 \tag{12.24}$$

The role of the parameter C is clearer in an enlarged feature space,
since perfect separation is often achievable there. A large value of C will
discourage any positive ξ i , and lead to an overfit wiggly boundary in the
original feature space; a small value of C will encourage a small value of
kβk, which in turn causes f (x) and hence the boundary to be smoother.
Figure 12.3 show two nonlinear support vector machines applied to the
mixture example of Chapter 2. The regularization parameter was chosen
in both cases to achieve good test error. The radial basis kernel produces
a boundary quite similar to the Bayes optimal boundary for this example;
compare Figure 2.5.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_03.png"
  title="**图12.3**："
>}}
Two nonlinear SVMs for the mixture data. The upper plot uses
a 4th degree polynomial kernel, the lower a radial basis kernel (with γ = 1). In
each case C was tuned to approximately achieve the best test error performance,
and C = 1 worked well in both cases. The radial basis kernel performs the best
(close to Bayes optimal), as might be expected given the data arise from mixtures
of Gaussians. The broken purple curve in the background is the Bayes decision
boundary.

In the early literature on support vectors, there were claims that the
kernel property of the support vector machine is unique to it and allows
one to finesse the curse of dimensionality. Neither of these claims is true,
and we go into both of these issues in the next three subsections.

### 12.3.2 The SVM as a Penalization Method

With f (x) = h(x) T β + β 0 , consider the optimization problem

$$\min\_{\beta\_0, \beta} \sum\_{i=1}^N
[1 - y\_i f(x\_i)]\_+ + \frac{\lambda}{2} \\|\beta\\|^2 \tag{12.25}$$

where the subscript “+” indicates positive part. This has the form loss +
penalty, which is a familiar paradigm in function estimation. It is easy to
show (Exercise 12.1) that the solution to (12.25), with λ = 1/C, is the
same as that for (12.8).

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_04.png"
  title="**图12.4**："
>}}
The support vector loss function (hinge loss), compared to the
negative log-likelihood loss (binomial deviance) for logistic regression, squared-er-
ror loss, and a “Huberized” version of the squared hinge loss. All are shown as a
function of yf rather than f , because of the symmetry between the y = +1 and
y = −1 case. The deviance and Huber have the same asymptotes as the SVM
loss, but are rounded in the interior. All are scaled to have the limiting left-tail
slope of −1.

Examination of the “hinge” loss function L(y, f ) = [1 − yf ] + shows that
it is reasonable for two-class classification, when compared to other more
traditional loss functions. Figure 12.4 compares it to the log-likelihood loss
for logistic regression, as well as squared-error loss and a variant thereof.
The (negative) log-likelihood or binomial deviance has similar tails as the
SVM loss, giving zero penalty to points well inside their margin, and a
linear penalty to points on the wrong side and far away. Squared-error, on
the other hand gives a quadratic penalty, and points well inside their own
margin have a strong influence on the model as well. The squared hinge
loss L(y, f ) = [1 − yf ] 2+ is like the quadratic, except it is zero for points
inside their margin. It still rises quadratically in the left tail, and will be
less robust than hinge or deviance to misclassified observations. Recently
Rosset and Zhu (2007) proposed a “Huberized” version of the squared hinge
loss, which converts smoothly to a linear loss at yf = −1.

| Loss Function | $L[y, f(x)]$ | Minimizing Function |
|---------------|--------------|---------------------|
| Binomial Deviance | $\log[1+e^{-yf(x)}]$ | $f(x)=\log\frac{\text{Pr}(Y=+1\|x)}{\text{Pr}(Y=-1\|x)}$ |
| SVM Hinge Loss | $[1-yf(x)]\_+$ | $f(x)=\text{sign}[\text{Pr}(Y=+1\|x) - \frac{1}{2}]$ |
| Squared Error | $[y-f(x)]^2=[1-yf(x)]^2$ | $f(x)=2\text{Pr}(Y=+1\|x)-1$ |
| "Huberised" Squared Hinge Loss | $\begin{cases} -4yf(x) & yf(x)<-1 \\\\ [1-yf(x)]^2\_+ & \text{otherwise}\end{cases}$ | $f(x)=2\text{Pr}(Y=+1\|x)-1$ |
**TABLE 12.1**: The population minimizers for the different loss functions in Fig-
ure 12.4. Logistic regression uses the binomial log-likelihood or deviance. Linear
discriminant analysis (Exercise 4.2) uses squared-error loss. The SVM hinge loss
estimates the mode of the posterior class probabilities, whereas the others estimate
a linear transformation of these probabilities.

We can characterize these loss functions in terms of what they are es-
timating at the population level. We consider minimizing EL(Y, f (X)).
Table 12.1 summarizes the results. Whereas the hinge loss estimates the
classifier G(x) itself, all the others estimate a transformation of the class
posterior probabilities. The “Huberized” square hinge loss shares attractive
properties of logistic regression (smooth loss function, estimates probabili-
ties), as well as the SVM hinge loss (support points).

Formulation (12.25) casts the SVM as a regularized function estimation
problem, where the coefficients of the linear expansion f (x) = β 0 + h(x) T β
are shrunk toward zero (excluding the constant). If h(x) represents a hierar-
chical basis having some ordered structure (such as ordered in roughness),
then the uniform shrinkage makes more sense if the rougher elements h j in
the vector h have smaller norm.

All the loss-function in Table 12.1 except squared-error are so called
“margin maximizing loss-functions” (Rosset et al., 2004b). This means that
if the data are separable, then the limit of β̂ λ in (12.25) as λ → 0 defines
the optimal separating hyperplane[^1].

### 12.3.3 Function Estimation and Reproducing Kernels :scream:

Here we describe SVMs in terms of function estimation in reproducing
kernel Hilbert spaces, where the kernel property abounds. This material is
discussed in some detail in Section 5.8. This provides another view of the
support vector classifier, and helps to clarify how it works.

Suppose the basis h arises from the (possibly finite) eigen-expansion of
a positive definite kernel K,

$$K(x, x')= \sum\_{m=1}^\infty \phi\_m(x)\phi\_m(x')\delta\_m \tag{12.26}$$

and h m (x) = δ m φ m (x). Then with θ m = δ m β m , we can write (12.25)
as

$$\min\_{\beta\_0, \theta} \sum\_{i=1}^N
\left[ 1 - y\_i (\beta\_0 +
\sum\_{m=1}^\infty \theta\_m \phi\_m(x\_i)) \right]\_+
+ \frac{\lambda}{2} \sum\_{m=1}^\infty \frac{\theta\_m^2}{\delta\_m}
\tag{12.27}$$

Now (12.27) is identical in form to (5.49) on page 169 in Section 5.8, and
the theory of reproducing kernel Hilbert spaces described there guarantees
a finite-dimensional solution of the form

$$f(x) = \beta\_0 + \sum\_{i=1}^N \alpha\_i K(x, x\_i) \tag{12.28}$$

In particular we see there an equivalent version of the optimization crite-
rion (12.19) [Equation (5.67) in Section 5.8.2; see also Wahba et al. (2000)],

$$\min\_{\beta\_0, \alpha} \sum\_{i=1}^N (1 - y\_i f(x\_i))\_+
+ \frac{\lambda}{2} \alpha^T \mathbf{K} \alpha \tag{12.29}$$

where K is the N × N matrix of kernel evaluations for all pairs of training
features (Exercise 12.2).

These models are quite general, and include, for example, the entire fam-
ily of smoothing splines, additive and interaction spline models discussed
in Chapters 5 and 9, and in more detail in Wahba (1990) and Hastie and
Tibshirani (1990). They can be expressed more generally as

$$\min\_{f\in\mathcal{H}} \sum\_{i=1}^N [1 - y\_i f(x\_i)]\_+
+ \lambda J(f) \tag{12.30}$$

where H is the structured space of functions, and J(f ) an appropriate reg-
ularizer on that space. For example, suppose H is the space of additive
functions f (x) = j=1 f j (x j ), and J(f ) = j {f ′′ j (x j )} 2 dx j . Then the
solution to (12.30) is an additive cubic spline, and has a kernel representa-
tion (12.28) with K(x, x ′ ) = j=1 K j (x j , x ′ j ). Each of the K j is the kernel
appropriate for the univariate smoothing spline in x j (Wahba, 1990).

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_05.png"
  title="**图12.5**："
>}}
The logistic regression versions of the SVM models in Fig-
ure 12.3, using the identical kernels and hence penalties, but the log-likelihood
loss instead of the SVM loss function. The two broken contours correspond to
posterior probabilities of 0.75 and 0.25 for the +1 class (or vice versa). The bro-
ken purple curve in the background is the Bayes decision boundary.

Conversely this discussion also shows that, for example, any of the kernels
described in (12.22) above can be used with any convex loss function, and
will also lead to a finite-dimensional representation of the form (12.28).
Figure 12.5 uses the same kernel functions as in Figure 12.3, except using
the binomial log-likelihood as a loss function[^2]. The fitted function is hence
an estimate of the log-odds,

$$\begin{align} \hat{f}(x)
&= \log\frac{\hat{\text{Pr}}(Y=+1|x)}{\hat{\text{Pr}}(Y=-1|x)}
\\\\ &=\hat{\beta}\_0 + \sum\_{i=1}^N \hat{\alpha}\_i K(x, x\_i)
\tag{12.31}\end{align}$$

or conversely we get an estimate of the class probabilities

$$\hat{\text{Pr}(Y=+1|x)} = \frac{1}
{1 + e^{- \hat{\beta}\_0 - \sum\_{i=1}^N \hat{\alpha}\_i K(x, x\_i)}}
\tag{12.32}$$

The fitted models are quite similar in shape and performance. Examples
and more details are given in Section 5.8.

It does happen that for SVMs, a sizable fraction of the N values of α i
can be zero (the nonsupport points). In the two examples in Figure 12.3,
these fractions are 42% and 45%, respectively. This is a consequence of the
piecewise linear nature of the first part of the criterion (12.25). The lower
the class overlap (on the training data), the greater this fraction will be.
Reducing λ will generally reduce the overlap (allowing a more flexible f ).
A small number of support points means that f ˆ (x) can be evaluated more
quickly, which is important at lookup time. Of course, reducing the overlap
too much can lead to poor generalization.

### 12.3.4 SVMs and the Curse of Dimensionality

In this section, we address the question of whether SVMs have some edge
on the curse of dimensionality. Notice that in expression (12.23) we are not
allowed a fully general inner product in the space of powers and products.
For example, all terms of the form 2X j X j ′ are given equal weight, and the
kernel cannot adapt itself to concentrate on subspaces. If the number of
features p were large, but the class separation occurred only in the linear
subspace spanned by say X 1 and X 2 , this kernel would not easily find the
structure and would suffer from having many dimensions to search over.
One would have to build knowledge about the subspace into the kernel;
that is, tell it to ignore all but the first two inputs. If such knowledge were
available a priori, much of statistical learning would be made much easier.
A major goal of adaptive methods is to discover such structure.

We support these statements with an illustrative example. We generated
100 observations in each of two classes. The first class has four standard
normal independent features X 1 , X 2 , X 3 , X 4 . The second class also has four
standard normal independent features, but conditioned on 9 ≤ X j 2 ≤ 16.
This is a relatively easy problem. As a second harder problem, we aug-
mented the features with an additional six standard Gaussian noise fea-
tures. Hence the second class almost completely surrounds the first, like the
skin surrounding the orange, in a four-dimensional subspace. The Bayes er-
ror rate for this problem is 0.029 (irrespective of dimension). We generated
1000 test observations to compare different procedures. The average test
errors over 50 simulations, with and without noise features, are shown in
Table 12.2

|   |        | Test Error (SE) | Test Error (SE) |
|---|--------|-----------------|-----------------|
|   | **Method** | **No Noise Features** | **Six Noise Features** |
| 1 | SV Classifier | 0.450 (0.003) | 0.472 (0.003) |
| 2 | SVM/ploy 2    | 0.078 (0.003) | 0.152 (0.004) |
| 3 | SVM/ploy 5    | 0.180 (0.004) | 0.370 (0.004) |
| 4 | SVM/ploy 10   | 0.230 (0.003) | 0.434 (0.002) |
| 5 | BRUTO         | 0.084 (0.003) | 0.090 (0.003) |
| 6 | MARS          | 0.156 (0.004) | 0.173 (0.005) |
|   | Bayes         | 0.029         | 0.029         |
**TABLE 12.2**: Skin of the orange: Shown are mean (standard error of the mean)
of the test error over 50 simulations. BRUTO fits an additive spline model adap-
tively, while MARS fits a low-order interaction model adaptively.

Line 1 uses the support vector classifier in the original feature space.
Lines 2–4 refer to the support vector machine with a 2-, 5- and 10-dimension-
al polynomial kernel. For all support vector procedures, we chose the cost
parameter C to minimize the test error, to be as fair as possible to the
method. Line 5 fits an additive spline model to the (−1, +1) response by
least squares, using the BRUTO algorithm for additive models, described
in Hastie and Tibshirani (1990). Line 6 uses MARS (multivariate adaptive
regression splines) allowing interaction of all orders, as described in Chap-
ter 9; as such it is comparable with the SVM/poly 10. Both BRUTO and
MARS have the ability to ignore redundant variables. Test error was not
used to choose the smoothing parameters in either of lines 5 or 6.

In the original feature space, a hyperplane cannot separate the classes,
and the support vector classifier (line 1) does poorly. The polynomial sup-
port vector machine makes a substantial improvement in test error rate,
but is adversely affected by the six noise features. It is also very sensitive to
the choice of kernel: the second degree polynomial kernel (line 2) does best,
since the true decision boundary is a second-degree polynomial. However,
higher-degree polynomial kernels (lines 3 and 4) do much worse. BRUTO
performs well, since the boundary is additive. BRUTO and MARS adapt
well: their performance does not deteriorate much in the presence of noise.

### 12.3.5 A Path Algorithm for the SVM Classifier :scream:

The regularization parameter for the SVM classifier is the cost parameter
C, or its inverse λ in (12.25). Common usage is to set C high, leading often
to somewhat overfit classifiers.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_06.png"
  title="**图12.6**："
>}}
Test-error curves as a function of the cost parameter C for the
radial-kernel SVM classifier on the mixture data. At the top of each plot is the
scale parameter γ for the radial kernel: K γ (x, y) = exp −γ||x − y|| 2 . The optimal
value for C depends quite strongly on the scale of the kernel. The Bayes error
rate is indicated by the broken horizontal lines.

Figure 12.6 shows the test error on the mixture data as a function of
C, using different radial-kernel parameters γ. When γ = 5 (narrow peaked
kernels), the heaviest regularization (small C) is called for. With γ = 1
(the value used in Figure 12.3), an intermediate value of C is required.
Clearly in situations such as these, we need to determine a good choice
for C, perhaps by cross-validation. Here we describe a path algorithm (in
the spirit of Section 3.8) for efficiently fitting the entire sequence of SVM
models obtained by varying C.

It is convenient to use the loss+penalty formulation (12.25), along with
Figure 12.4. This leads to a solution for β at a given value of λ:

$$\beta\_\lambda = \frac{1}{\lambda}
\sum\_{i=1}^N \alpha\_i y\_i x\_i \tag{12.33}$$

The α i are again Lagrange multipliers, but in this case they all lie in [0, 1].

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_07.png"
  title="**图12.7**："
>}}
A simple example illustrates the SVM path algorithm. (left
panel:) This plot illustrates the state of the model at λ = 0.05. The ‘‘ + 1”
points are orange, the “−1” blue. λ = 1/2, and the width of the soft margin
is 2/||β|| = 2 × 0.587. Two blue points {3, 5} are misclassified, while the two or-
ange points {10, 12} are correctly classified, but on the wrong side of their margin
f (x) = +1; each of these has y i f (x i ) < 1. The three square shaped points {2, 6, 7}
are exactly on their margins. (right panel:) This plot shows the piecewise linear
profiles α i (λ). The horizontal broken line at λ = 1/2 indicates the state of the α i
for the model in the left plot.

Figure 12.7 illustrates the setup. It can be shown that the KKT optimal-
ity conditions imply that the labeled points (x i , y i ) fall into three distinct
groups:

- Observations correctly classified and outside their margins. They have
y i f (x i ) > 1, and Lagrange multipliers α i = 0. Examples are the
orange points 8, 9 and 11, and the blue points 1 and 4.
- Observations sitting on their margins with y i f (x i ) = 1, with Lagrange
multipliers α i ∈ [0, 1]. Examples are the orange 7 and the blue 2 and
8.
- Observations inside their margins have y i f (x i ) < 1, with α i = 1.
Examples are the blue 3 and 5, and the orange 10 and 12.

The idea for the path algorithm is as follows. Initially λ is large, the
margin 1/||β λ || is wide, and all points are inside their margin and have
α i = 1. As λ decreases, 1/||β λ || decreases, and the margin gets narrower.
Some points will move from inside their margins to outside their margins,
and their α i will change from 1 to 0. By continuity of the α i (λ), these points
will linger on the margin during this transition. From (12.33) we see that
the points with α i = 1 make fixed contributions to β(λ), and those with
α i = 0 make no contribution. So all that changes as λ decreases are the
α i ∈ [0, 1] of those (small number) of points on the margin. Since all these
points have y i f (x i ) = 1, this results in a small set of linear equations that
prescribe how α i (λ) and hence β λ changes during these transitions. This
results in piecewise linear paths for each of the α i (λ). The breaks occur
when points cross the margin. Figure 12.7 (right panel) shows the α i (λ)
profiles for the small example in the left panel.

Although we have described this for linear SVMs, exactly the same idea
works for nonlinear models, in which (12.33) is replaced by

$$f\_\lambda(x) = \frac{1}{\lambda}
\sum\_{i=1}^N \alpha\_i y\_i K(x, x\_i) \tag{12.34}$$

Details can be found in Hastie et al. (2004). An R package svmpath is
available on CRAN for fitting these models.

### 12.3.6 Support Vector Machines for Regression

In this section we show how SVMs can be adapted for regression with a
quantitative response, in ways that inherit some of the properties of the
SVM classifier. We first discuss the linear regression model

$$f(x) = x^T\beta + \beta\_0 \tag{12.35}$$

and then handle nonlinear generalizations. To estimate β, we consider min-
imization of

$$H(\beta, \beta\_0)= \sum\_{i=1}^N V(y\_i - f(x\_i)) +
\frac{\lambda}{2} \\|\beta\\|^2 \tag{12.36}$$

where

$$V\_\epsilon =
\begin{cases}
0 & \text{if } \|r\| < \epsilon
\\\\ \|r\| - \epsilon & \text{otherwise}
\end{cases}
\tag{12.37}$$

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_08.png"
  title="**图12.8**："
>}}
The left panel shows the ǫ-insensitive error function used by the
support vector regression machine. The right panel shows the error function used
in Huber’s robust regression (blue curve). Beyond |c|, the function changes from
quadratic to linear.

This is an “ǫ-insensitive” error measure, ignoring errors of size less than
ǫ (left panel of Figure 12.8). There is a rough analogy with the support
vector classification setup, where points on the correct side of the deci-
sion boundary and far away from it, are ignored in the optimization. In
regression, these “low error” points are the ones with small residuals.

It is interesting to contrast this with error measures used in robust re-
gression in statistics. The most popular, due to Huber (1964), has the form

$$V\_H( r ) =
\begin{cases}
r^2 / 2 & \text{if } \|r\| \leq c
\\\\ c\|r\| - c^2 / 2 & \|r\| > c
\end{cases}
\tag{12.38}$$

shown in the right panel of Figure 12.8. This function reduces from quadratic
to linear the contributions of observations with absolute residual greater
than a prechosen constant c. This makes the fitting less sensitive to out-
liers. The support vector error measure (12.37) also has linear tails (beyond
ǫ), but in addition it flattens the contributions of those cases with small
residuals.

If β̂, β̂ 0 are the minimizers of H, the solution function can be shown to
have the form

$$\begin{align}
\hat{\beta} &=
\sum\_{i=1}^N (\hat{\alpha}^\*\_i - \hat{\alpha}\_i) x\_i \tag{12.39}
\\\\ \hat{f}(x) &=
\sum\_{i=1}^N (\hat{\alpha}^\*\_i - \hat{\alpha}\_i) \langle x,x\_i \rangle
+ \beta\_0 \tag{12.40}\end{align}$$

where α̂ i , α̂ i ∗ are positive and solve the quadratic programming problem

$$
\begin{align}
\min\_{\alpha\_i, \alpha\_i^\*}
& \epsilon \sum\_{i=1}^N (\hat{\alpha}\_i^\* + \hat{\alpha}\_i) -
\sum\_{i=1}^N y\_i (\hat{\alpha}\_i^\* - \hat{\alpha}\_i) +
\\\\ & \frac{1}{2} \sum\_{i,i'=1}^N
(\alpha\_i^\* - \alpha\_i)
(\alpha\_{i'}^\* - \alpha\_{i'})
\langle x\_i, x\_{i'} \rangle
\end{align}$$

subject to the constraints

$$\begin{gather}
0 \leq \alpha\_i, \alpha\_i^\* \leq 1 / \lambda
\\\\ \sum\_{i=1}^N (\alpha\_i^\* - \alpha\_i) = 0
\\\\ \alpha\_i \alpha\_i^\* = 0
\end{gather}\tag{12.41}$$

Due to the nature of these constraints, typically only a subset of the solution
values (α̂ i ∗ − α̂ i ) are nonzero, and the associated data values are called the
support vectors. As was the case in the classification setting, the solution
depends on the input values only through the inner products hx i , x i ′ i. Thus
we can generalize the methods to richer spaces by defining an appropriate
inner product, for example, one of those defined in (12.22).

Note that there are parameters, ǫ and λ, associated with the criterion
(12.36). These seem to play different roles. ǫ is a parameter of the loss
function V ǫ , just like c is for V H . Note that both V ǫ and V H depend on the
scale of y and hence r. If we scale our response (and hence use V H (r/σ) and
V ǫ (r/σ) instead), then we might consider using preset values for c and ǫ (the
value c = 1.345 achieves 95% efficiency for the Gaussian). The quantity λ
is a more traditional regularization parameter, and can be estimated for
example by cross-validation.

### 12.3.7 Regression and Kernels

As discussed in Section 12.3.3, this kernel property is not unique to sup-
port vector machines. Suppose we consider approximation of the regression
function in terms of a set of basis functions {h m (x)}, m = 1, 2, . . . , M :

$$f(x) = \sum\_{m=1}^M \beta\_m h\_m(x) + \beta\_0 \tag{12.42}$$

To estimate β and β 0 we minimize

$$H(\beta, \beta\_0)= \sum\_{i=1}^N V(y\_i - f(x\_i)) +
\frac{\lambda}{2} \sum \beta\_m^2 \tag{12.43}$$

for some general error measure V (r). For any choice of V (r), the solution
f ˆ (x) =   β̂ m h m (x) + β̂ 0 has the form

$$\hat{f}(x) = \sum\_{i=1}^N \hat{\alpha}\_i K(x, x\_i) \tag{12.44}$$

with K(x, y) =  m=1 h m (x)h m (y). Notice that this has the same form
as both the radial basis function expansion and a regularization estimate,
discussed in Chapters 5 and 6.

For concreteness, let’s work out the case V (r) = r 2 . Let H be the N × M
basis matrix with imth element h m (x i ), and suppose that M > N is large.
For simplicity we assume that β 0 = 0, or that the constant is absorbed in
h; see Exercise 12.3 for an alternative.

We estimate β by minimizing the penalized least squares criterion

$$H(\beta) =
(\mathbf{y} - \mathbf{H}\beta)^T(\mathbf{y} - \mathbf{H}\beta) +
\lambda \\|\beta\\|^2 \tag{12.45}$$

The solution is

$$\hat{\mathbf{y}} = \mathbf{H} \hat{\beta} \tag{12.46}$$

with β̂ determined by

$$- \mathbf{H}^T (\mathbf{y} - \mathbf{H}\hat{\beta}) +
\lambda \hat{\beta} = 0 \tag{12.47}$$

From this it appears that we need to evaluate the M × M matrix of inner
products in the transformed space. However, we can premultiply by H to
give

$$\mathbf{H} \hat{\beta} =
(\mathbf{H} \mathbf{H}^T + \lambda \mathbf{I})^{-1}
\mathbf{H} \mathbf{H}^T \mathbf{y}\tag{12.48}$$

The N × N matrix HH T consists of inner products between pairs of obser-
vations i, i ′ ; that is, the evaluation of an inner product kernel {HH T } i,i ′ =
K(x i , x i ′ ). It is easy to show (12.44) directly in this case, that the predicted
values at an arbitrary x satisfy

$$\begin{align} \hat{f}(x)
&= h(x)^T \hat{\beta}
\\\\ &= \sum\_{i=1}^N \hat{\alpha}\_i K(x, x\_i)
\tag{12.49}\end{align}$$

where α̂ = (HH T + λI) −1 y. As in the support vector machine, we need not
specify or evaluate the large set of functions h 1 (x), h 2 (x), . . . , h M (x). Only
the inner product kernel K(x i , x i ′ ) need be evaluated, at the N training
points for each i, i ′ and at points x for predictions there. Careful choice
of h m (such as the eigenfunctions of particular, easy-to-evaluate kernels
K) means, for example, that HH T can be computed at a cost of N 2 /2
evaluations of K, rather than the direct cost N 2 M .

Note, however, that this property depends on the choice of squared norm
kβk 2 in the penalty. It does not hold, for example, for the L 1 norm |β|,
which may lead to a superior model.

### 12.3.8 Discussion

The support vector machine can be extended to multiclass problems, es-
sentially by solving many two-class problems. A classifier is built for each
pair of classes, and the final classifier is the one that dominates the most
(Kressel, 1999; Friedman, 1996; Hastie and Tibshirani, 1998). Alternatively,
one could use the multinomial loss function along with a suitable kernel,
as in Section 12.3.3. SVMs have applications in many other supervised
and unsupervised learning problems. At the time of this writing, empirical
evidence suggests that it performs well in many real learning problems.

Finally, we mention the connection of the support vector machine and
structural risk minimization (7.9). Suppose the training points (or their
basis expansion) are contained in a sphere of radius R, and let G(x) =
sign[f (x)] = sign[β T x + β 0 ] as in (12.2). Then one can show that the class
of functions {G(x), kβk ≤ A} has VC-dimension h satisfying

$$h \leq R^2 A^2\tag{12.50}$$

If f (x) separates the training data, optimally for kβk ≤ A, then with
probability at least 1 − η over training sets (Vapnik, 1996, page 139):

$$\text{Error}\_\text{Test} \leq 4
\frac{h[\log(2N / h) + 1] - \log(\eta / 4)}{N} \tag{12.51}$$

The support vector classifier was one of the first practical learning pro-
cedures for which useful bounds on the VC dimension could be obtained,
and hence the SRM program could be carried out. However in the deriva-
tion, balls are put around the data points—a process that depends on the
observed values of the features. Hence in a strict sense, the VC complexity
of the class is not fixed a priori, before seeing the features.

The regularization parameter C controls an upper bound on the VC
dimension of the classifier. Following the SRM paradigm, we could choose C
by minimizing the upper bound on the test error, given in (12.51). However,
it is not clear that this has any advantage over the use of cross-validation
for choice of C.

[^1]: For logistic regression with separable data, β̂ diverges, but β̂ /|| β̂ converges to the optimal separating direction.
[^2]: Ji Zhu assisted in the preparation of these examples.
