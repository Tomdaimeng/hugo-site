+++
title = "ESL-12.2 The Support Vector Classifier"
summary = """
统计学习基础（译注）第十二章第二节，第 417-422 页。
"""

date = 2019-03-04T19:55:00+08:00
lastmod = 2019-03-04T19:55:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In Chapter 4 we discussed a technique for constructing an optimal separat-
ing hyperplane between two perfectly separated classes. We review this and
generalize to the nonseparable case, where the classes may not be separable
by a linear boundary.

Our training data consists of N pairs (x 1 , y 1 ), (x 2 , y 2 ), . . . , (x N , y N ), with
x i ∈ IR p and y i ∈ {−1, 1}. Define a hyperplane by

$$\\{x : f(x) = x^T\beta + \beta\_0 = 0\\} \tag{12.1}$$

where β is a unit vector: kβk = 1. A classification rule induced by f (x) is

$$G(x) = \text{sign}[x^T\beta + \beta\_0] \tag{12.2}$$

The geometry of hyperplanes is reviewed in Section 4.5, where we show that
f (x) in (12.1) gives the signed distance from a point x to the hyperplane
f (x) = x T β +β 0 = 0. Since the classes are separable, we can find a function
f (x) = x T β + β 0 with y i f (x i ) > 0 ∀i. Hence we are able to find the
hyperplane that creates the biggest margin between the training points for
class 1 and −1 (see Figure 12.1). The optimization problem

$$\begin{gather}
\max\_{\beta, \beta\_0, \\|\beta\\| = 1} M \\\\ \text{subject to }
y\_i(x\_i^T\beta + \beta\_0) \geq M, i = 1,\dots,M
\end{gather}\tag{12.3}$$

captures this concept. The band in the figure is M units away from the
hyperplane on either side, and hence 2M units wide. It is called the margin.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_01.png"
  title="**图12.1**："
>}}
Support vector classifiers. The left panel shows the separable
case. The decision boundary is the solid line, while broken lines bound the shaded
maximal margin of width 2M = 2/kβk. The right panel shows the nonseparable
(overlap) case. The points labeled ξ j ∗ are on the wrong side of their margin by
an amount ξ j ∗ = M ξ j ; points on the P correct side have ξ j ∗ = 0. The margin is
maximized subject to a total budget ξ i ≤ constant. Hence ξ j ∗ is the total
distance of points on the wrong side of their margin.

We showed that this problem can be more conveniently rephrased as

$$\begin{gather}
\min\_{\beta, \beta\_0} \\|\beta\\| \\\\ \text{subject to }
y\_i(x\_i^T\beta + \beta\_0) \geq 1, i = 1,\dots,M
\end{gather}\tag{12.4}$$

where we have dropped the norm constraint on β. Note that M = 1/kβk.
Expression (12.4) is the usual way of writing the support vector criterion
for separated data. This is a convex optimization problem (quadratic cri-
terion, linear inequality constraints), and the solution is characterized in
Section 4.5.2.

Suppose now that the classes overlap in feature space. One way to deal
with the overlap is to still maximize M , but allow for some points to be on
the wrong side of the margin. Define the slack variables ξ = (ξ 1 , ξ 2 , . . . , ξ N ).
There are two natural ways to modify the constraint in (12.3):

$$\begin{align}
y\_i(x\_i^T\beta + \beta\_0) &\geq M - \xi\_i \tag{12.5}
\\\\ &\text{or}
\\\\ y\_i(x\_i^T\beta + \beta\_0) &\geq M(1 - \xi\_i) \tag{12.6}
\end{align}

∀i, ξ i ≥ 0, i=1 ξ i ≤ constant. The two choices lead to different solutions.
The first choice seems more natural, since it measures overlap in actual
distance from the margin; the second choice measures the overlap in relative
distance, which changes with the width of the margin M . However, the first
choice results in a nonconvex optimization problem, while the second is
convex; thus (12.6) leads to the “standard” support vector classifier, which
we use from here on.

Here is the idea of the formulation. The value ξ i in the constraint y i (x Ti β+
β 0 ) ≥ M (1 − ξ i ) is the proportional amount by which the prediction
f (x i ) = x Ti β +β 0 is on the wrong side of its margin. Hence by bounding the
sum ξ i , we bound the total proportional amount by which predictions
fall on the wrong side of their margin. Misclassifications occur when ξ i > 1,
so bounding ξ i at a value K say, bounds the total number of training
misclassifications at K.

As in (4.48) in Section 4.5.2, we can drop the norm constraint on β,
define M = 1/kβk, and write (12.4) in the equivalent form

$$\min\\|\beta\\| \text{ subject to }
\begin{cases}
y\_i(x\_i^T\beta + \beta\_0) \geq 1 - \xi\_i \,\forall i
\\\\ \xi\_i \geq 0,  \sum \xi\_i \leq \text{constant}.
\end{cases}\tag{12.7}$$

This is the usual way the support vector classifier is defined for the non-
separable case. However we find confusing the presence of the fixed scale
“1” in the constraint y i (x Ti β + β 0 ) ≥ 1 − ξ i , and prefer to start with (12.6).
The right panel of Figure 12.1 illustrates this overlapping case.

By the nature of the criterion (12.7), we see that points well inside their
class boundary do not play a big role in shaping the boundary. This seems
like an attractive property, and one that differentiates it from linear dis-
criminant analysis (Section 4.3). In LDA, the decision boundary is deter-
mined by the covariance of the class distributions and the positions of the
class centroids. We will see in Section 12.3.3 that logistic regression is more
similar to the support vector classifier in this regard.

### 12.2.1 Computing the Support Vector Classifier :scream:

The problem (12.7) is quadratic with linear inequality constraints, hence it
is a convex optimization problem. We describe a quadratic programming
solution using Lagrange multipliers. Computationally it is convenient to
re-express (12.7) in the equivalent form

$$\begin{align} & \min\_{\beta, \beta\_0}
\frac{1}{2} \\|\beta\\|^2 + C \sum\_{i=1}^N \xi\_i
\\\\ & \text{subject to }
\xi \geq 0, y\_i(x\_i^T\beta + \beta\_0) \geq 1 - \xi\_i \,\forall i
\end{align}\tag{12.8}$$

where the “cost” parameter C replaces the constant in (12.7); the separable
case corresponds to C = ∞.

The Lagrange (primal) function is

$$L\_p = \frac{1}{2} \\|\beta\\|^2 + C \sum\_{i=1}^N \xi\_i -
\sum\_{i=1}^N \alpha\_i[y\_i(x\_i^T\beta + \beta\_0) - (1-\xi\_i)] -
\sum\_{i=1}^N \mu\_i \xi\_i
\tag{12.9}$$

which we minimize w.r.t β, β 0 and ξ i . Setting the respective derivatives to
zero, we get

$$\begin{align}
\beta &= \sum\_{i=1}^N \alpha\_i y\_i x\_i \tag{12.10}
\\\\ 0 &= \sum\_{i=1} \alpha\_i y\_i \tag{12.11}
\\\\ \alpha\_i &= C - \mu\_i, \forall i\tag{12.12}
\end{align}$$

as well as the positivity constraints α i , μ i , ξ i ≥ 0 ∀i. By substituting
(12.10)–(12.12) into (12.9), we obtain the Lagrangian (Wolfe) dual objec-
tive function

$$L\_D = \sum\_{i=1}^N \alpha\_i - \frac{1}{2}\sum\_{i=1}^N\sum\_{i'=1}^N
\alpha\_i \alpha\_{i'} y\_i y\_{i'} x\_i^T x\_{i'} \tag{12.13}$$

which gives a lower bound on the objective function (12.8) for any feasible
point. We maximize L D subject to 0 ≤ α i ≤ C and i=1 α i y i = 0. In
addition to (12.10)–(12.12), the Karush–Kuhn–Tucker conditions include
the constraints

$$\begin{align}
a\_i[y\_i(x\_i^T\beta + \beta\_0) - (1 - \xi\_i)] &= 0 \tag{12.14}
\\\\ \mu\_i \xi\_i &= 0 \tag{12.15}
\\\\ y\_i(x\_i^T\beta + \beta\_0) - (1 - \xi\_i) &\geq 0 \tag{12.16}
\end{align}$$

for i = 1, . . . , N . Together these equations (12.10)–(12.16) uniquely char-
acterize the solution to the primal and dual problem.

From (12.10) we see that the solution for β has the form

$$\hat{\beta} = \sum\_{i=1}^N \hat{\alpha}\_i y\_i x\_i
\tag{12.17}$$

with nonzero coefficients α̂ i only for those observations i for which the
constraints in (12.16) are exactly met (due to (12.14)). These observations
are called the support vectors, since β̂ is represented in terms of them
alone. Among these support points, some will lie on the edge of the margin
( ξ ˆ i = 0), and hence from (12.15) and (12.12) will be characterized by
0 < α̂ i < C; the remainder ( ξ ˆ i > 0) have α̂ i = C. From (12.14) we can
see that any of these margin points (0 < α̂ i , ξ ˆ i = 0) can be used to solve
for β 0 , and we typically use an average of all the solutions for numerical
stability.

Maximizing the dual (12.13) is a simpler convex quadratic programming
problem than the primal (12.9), and can be solved with standard techniques
(Murray et al., 1981, for example).

Given the solutions β̂ 0 and β̂, the decision function can be written as

$$\begin{align}
\hat{G}(x) &= \text{sign}[\hat{f}(x)] \\\\ &=
\text{sign}[x^T\hat{\beta} + \hat{beta}\_0] \tag{12.18}
\end{align}$$

The tuning parameter of this procedure is the cost parameter C.

### 12.2.2 Mixture Example (Continued)

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_02.png"
  title="**图12.2**："
>}}
The linear support vector boundary for the mixture data exam-
ple with two overlapping classes, for two different values of C. The broken lines
indicate the margins, where f (x) = ±1. The support points (α i > 0) are all the
points on the wrong side of their margin. The black solid dots are those support
points falling exactly on the margin (ξ i = 0, α i > 0). In the upper panel 62% of
the observations are support points, while in the lower panel 85% are. The broken
purple curve in the background is the Bayes decision boundary.

Figure 12.2 shows the support vector boundary for the mixture example
of Figure 2.5 on page 21, with two overlapping classes, for two different
values of the cost parameter C. The classifiers are rather similar in their
performance. Points on the wrong side of the boundary are support vectors.
In addition, points on the correct side of the boundary but close to it (in
the margin), are also support vectors. The margin is larger for C = 0.01
than it is for C = 10, 000. Hence larger values of C focus attention more
on (correctly classified) points near the decision boundary, while smaller
values involve data further away. Either way, misclassified points are given
weight, no matter how far away. In this example the procedure is not very
sensitive to choices of C, because of the rigidity of a linear boundary.

The optimal value for C can be estimated by cross-validation, as dis-
cussed in Chapter 7. Interestingly, the leave-one-out cross-validation error
can be bounded above by the proportion of support points in the data. The
reason is that leaving out an observation that is not a support vector will
not change the solution. Hence these observations, being classified correctly
by the original boundary, will be classified correctly in the cross-validation
process. However this bound tends to be too high, and not generally useful
for choosing C (62% and 85%, respectively, in our examples).