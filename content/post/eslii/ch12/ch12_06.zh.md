+++
title = "ESL-12.6 Penalized Discriminant Analysis"
summary = """
统计学习基础（译注）第十二章第六节，第 446-449 页。
"""

date = 2019-03-24T14:58:00+08:00
lastmod = 2019-03-24T14:58:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

Although FDA is motivated by generalizing optimal scoring, it can also be
viewed directly as a form of regularized discriminant analysis. Suppose the
regression procedure used in FDA amounts to a linear regression onto a
basis expansion h(X), with a quadratic penalty on the coefficients:

$$\text{ASR}(\\{\beta\_\ell, \beta\_\ell\\}\_{\ell=1}^N) = \frac{1}{N}
\sum\_{\ell=1}^L \left[
  \sum\_{i=1}^N (\theta\_\ell(g\_i) - h^T(x\_i)\beta\_\ell)^2 +
  \lambda \beta\_\ell^T \mathbf{\Omega} \beta\_\ell
\right]\tag{12.57}$$

The choice of Ω depends on the problem. If η l (x) = h(x)β l is an expansion
on spline basis functions, Ω might constrain η l to be smooth over IR p . In
the case of additive splines, there are N spline basis functions for each
coordinate, resulting in a total of N p basis functions in h(x); Ω in this case
is N p × N p and block diagonal.

The steps in FDA can then be viewed as a generalized form of LDA,
which we call penalized discriminant analysis, or PDA:

- Enlarge the set of predictors X via a basis expansion h(X).
- Use (penalized) LDA in the enlarged space, where the penalized
Mahalanobis distance is given by
$$D(x,\mu) = (h(x) - h(\mu))^T
(\mathbf{\Sigma}\_W +\lambda\mathbf{\Omega})^{-1}
(h(x) - h(\mu))^T \tag{12.58}$$
where Σ W is the within-class covariance matrix of the derived vari-
ables h(x i ).
- Decompose the classification subspace using a penalized metric:
$$\max u^T\mathbf{\Omega}\_\text{Bet}u \text{ subject to }
u^T(\mathbf{\Sigma}\_W +\lambda\mathbf{\Omega})u = 1$$

Loosely speaking, the penalized Mahalanobis distance tends to give less
weight to “rough” coordinates, and more weight to “smooth” ones; since
the penalty is not diagonal, the same applies to linear combinations that
are rough or smooth.

For some classes of problems, the first step, involving the basis expansion,
is not needed; we already have far too many (correlated) predictors. A
leading example is when the objects to be classified are digitized analog
signals:

- the log-periodogram of a fragment of spoken speech, sampled at a set
of 256 frequencies; see Figure 5.5 on page 149.
- the grayscale pixel values in a digitized image of a handwritten digit.

It is also intuitively clear in these cases why regularization is needed.
Take the digitized image as an example. Neighboring pixel values will tend
to be correlated, being often almost the same. This implies that the pair
of corresponding LDA coefficients for these pixels can be wildly different
and opposite in sign, and thus cancel when applied to similar pixel values.
Positively correlated predictors lead to noisy, negatively correlated coeffi-
cient estimates, and this noise results in unwanted sampling variance. A
reasonable strategy is to regularize the coefficients to be smooth over the
spatial domain, as with images. This is what PDA does. The computations
proceed just as for FDA, except that an appropriate penalized regression
method is used. Here h T (X)β l = Xβ l , and Ω is chosen so that β l T Ωβ l
penalizes roughness in β l when viewed as an image. Figure 1.2 on page 4
shows some examples of handwritten digits. Figure 12.11 shows the dis-
criminant variates using LDA and PDA. Those produced by LDA appear
as salt-and-pepper images, while those produced by PDA are smooth im-
ages. The first smooth image can be seen as the coefficients of a linear
contrast functional for separating images with a dark central vertical strip
(ones, possibly sevens) from images that are hollow in the middle (zeros,
some fours). Figure 12.12 supports this interpretation, and with more dif-
ficulty allows an interpretation of the second coordinate. This and other
examples are discussed in more detail in Hastie et al. (1995), who also show
that the regularization improves the classification performance of LDA on
independent test data by a factor of around 25% in the cases they tried.


{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_11.png"
  title="**图12.11**："
>}}
The images appear in pairs, and represent the nine discrim-
inant coefficient functions for the digit recognition problem. The left member of
each pair is the LDA coefficient, while the right member is the PDA coefficient,
regularized to enforce spatial smoothness.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_12.png"
  title="**图12.12**："
>}}
The first two penalized canonical variates, evaluated for the
test data. The circles indicate the class centroids. The first coordinate contrasts
mainly 0’s and 1’s, while the second contrasts 6’s and 7/9’s.