+++
title = "ESL-12.5 Generalizing Linear Discriminant Analysis"
summary = """
统计学习基础（译注）第十二章第五节，第 440-445 页。
"""

date = 2019-03-22T16:24:00+08:00
lastmod = 2019-03-22T16:24:00+08:00
draft = true 
math = true

authors = ["Butters"]
tags = ["译文"]
categories = ["统计学习基础（译注）"]

[header]
image = ""
caption = ""
preview = true
+++

In this section we describe a method for performing LDA using linear re-
gression on derived responses. This in turn leads to nonparametric and flex-
ible alternatives to LDA. As in Chapter 4, we assume we have observations
with a quantitative response G falling into one of K classes G = {1, . . . , K},
each having measured features X. Suppose θ : G 7→ IR 1 is a function that
assigns scores to the classes, such that the transformed class labels are op-
timally predicted by linear regression on X: If our training sample has the
form (g i , x i ), i = 1, 2, . . . , N , then we solve

$$\min\_{\beta,\theta} \sum\_{i=1}^N  (\theta(g\_i) - x\_i^T\beta)^2
\tag{12.52}$$

with restrictions on θ to avoid a trivial solution (mean zero and unit vari-
ance over the training data). This produces a one-dimensional separation
between the classes.

More generally, we can find up to L ≤ K − 1 sets of independent scorings
for the class labels, θ 1 , θ 2 , . . . , θ L , and L corresponding linear maps η l (X) =
X T β l , l = 1, . . . , L, chosen to be optimal for multiple regression in IR p . The
scores θ l (g) and the maps β l are chosen to minimize the average squared
residual,

$$\text{ASR} = \frac{1}{N}\sum\_{\ell=1}^L \left[
\sum\_{i=1}^N (\theta\_\ell(g\_i)) - x\_i^T \beta\_\ell)^2
\right] \tag{12.53}$$

The set of scores are assumed to be mutually orthogonal and normalized
with respect to an appropriate inner product to prevent trivial zero
solutions.

Why are we going down this road? It can be shown that the sequence
of discriminant (canonical) vectors ν l derived in Section 4.3.3 are identical
to the sequence β l up to a constant (Mardia et al., 1979; Hastie et al.,
1995). Moreover, the Mahalanobis distance of a test point x to the kth
class centroid μ̂ k is given by

$$\delta\_J(x, \hat{\mu}\_k) = \sum\_{\ell=1}^{K-1}
w\_\ell(\hat{\eta}\_\ell(x) - \bar{\eta}\_\ell^k)^2 + D(x) \tag{12.54}$$

where η̄ l k is the mean of the η̂ l (x i ) in the kth class, and D(x) does not
depend on k. Here w l are coordinate weights that are defined in terms of
the mean squared residual r l 2 of the lth optimally scored fit

$$w\_\ell = \frac{1}{r\_\ell^2(1 - r\_\ell^2)} \tag{12.55}$$

In Section 4.3.2 we saw that these canonical distances are all that is needed
for classification in the Gaussian setup, with equal covariances in each class.
To summarize:

> LDA can be performed by a sequence of linear regressions, fol-
> lowed by classification to the closest class centroid in the space
> of fits. The analogy applies both to the reduced rank version,
> or the full rank case when L = K − 1.

The real power of this result is in the generalizations that it invites. We
can replace the linear regression fits η l (x) = x T β l by far more flexible,
nonparametric fits, and by analogy achieve a more flexible classifier than
LDA. We have in mind generalized additive fits, spline functions, MARS
models and the like. In this more general form the regression problems are
defined via the criterion

$$\text{ASR}(\\{\theta\_\ell, \eta\_\ell\\}\_{\ell=1}^L) = \frac{1}{N}
\sum\_{\ell=1}^L \left[
\sum\_{i=1}^N (\theta\_\ell(g\_i) - \eta\_\ell(x\_i))^2 +
\lambda J(\eta\_\ell)
\right]$$

where J is a regularizer appropriate for some forms of nonparametric regres-
sion, such as smoothing splines, additive splines and lower-order ANOVA
spline models. Also included are the classes of functions and associated
penalties generated by kernels, as in Section 12.3.3.

Before we describe the computations involved in this generalization, let
us consider a very simple example. Suppose we use degree-2 polynomial
regression for each η l . The decision boundaries implied by the (12.54) will
be quadratic surfaces, since each of the fitted functions is quadratic, and as
in LDA their squares cancel out when comparing distances. We could have
achieved identical quadratic boundaries in a more conventional way, by
augmenting our original predictors with their squares and cross-products.
In the enlarged space one performs an LDA, and the linear boundaries in
the enlarged space map down to quadratic boundaries in the original space.
A classic example is a pair of multivariate Gaussians centered at the origin,
one having covariance matrix I, and the other cI for c > 1; Figure 12.9
illustrates. The Bayes decision boundary is the sphere kxk = pc2(c−1) , which
is a linear boundary in the enlarged space.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_09.png"
  title="**图12.09**："
>}}
The data consist of 50 points generated from each of N (0, I) and
N (0, 94 I). The solid black ellipse is the decision boundary found by FDA using
degree-two polynomial regression. The dashed purple circle is the Bayes decision
boundary.

Many nonparametric regression procedures operate by generating a basis
expansion of derived variables, and then performing a linear regression in
the enlarged space. The MARS procedure (Chapter 9) is exactly of this
form. Smoothing splines and additive spline models generate an extremely
large basis set (N ×p basis functions for additive splines), but then perform
a penalized regression fit in the enlarged space. SVMs do as well; see also
the kernel-based regression example in Section 12.3.7. FDA in this case can
be shown to perform a penalized linear discriminant analysis in the enlarged
space. We elaborate in Section 12.6. Linear boundaries in the enlarged space
map down to nonlinear boundaries in the reduced space. This is exactly the
same paradigm that is used with support vector machines (Section 12.3).

We illustrate FDA on the speech recognition example used in Chapter
4.), with K = 11 classes and p = 10 predictors. The classes correspond to
11 vowel sounds, each contained in 11 different words. Here are the words,
preceded by the symbols that represent them:

| Vowel | Word | Vowel | Word | Vowel | Word | Vowel | Word |
|-------|------|-------|------|-------|------|-------|------|
| i:    | heed | O     | hod  | I     | hid  | C:    | hoard |
| E     | head | U     | hood | A     | had  | u:    | who'd |
| a:    | hard | 3:    | heard | Y     | hud  |     |   |

Each of eight speakers spoke each word six times in the training set, and
likewise seven speakers in the test set. The ten predictors are derived from
the digitized speech in a rather complicated way, but standard in the speech
recognition world. There are thus 528 training observations, and 462 test
observations. Figure 12.10 shows two-dimensional projections produced by
LDA and FDA. The FDA model used adaptive additive-spline regression
functions to model the η l (x), and the points plotted in the right plot have
coordinates η̂ 1 (x i ) and η̂ 2 (x i ). The routine used in S-PLUS is called bruto,
hence the heading on the plot and in Table 12.3. We see that flexible model-
ing has helped to separate the classes in this case. Table 12.3 shows training
and test error rates for a number of classification techniques. FDA/MARS
refers to Friedman’s multivariate adaptive regression splines; degree = 2
means pairwise products are permitted. Notice that for FDA/MARS, the
best classification results are obtained in a reduced-rank subspace.

{{< figure
  src="http://public.guansong.wang/eslii/ch12/eslii_fig_12_10.png"
  title="**图12.10**："
>}}
The left plot shows the first two LDA canonical variates for
the vowel training data. The right plot shows the corresponding projection when
FDA/BRUTO is used to fit the model; plotted are the fitted regression functions
η̂ 1 (x i ) and η̂ 2 (x i ). Notice the improved separation. The colors represent the eleven
different vowel sounds.

|   |   | Error Rates | Error Rates |
|---|---|-------------|-------------|
|   | **Technique** | **Training** | **Test** |
| (1) | LDA                                       | 0.32 | 0.56 |
|     | Softmax                                   | 0.48 | 0.67 |
| (2) | QDA                                       | 0.01 | 0.53 |
| (3) | CART                                      | 0.05 | 0.56 |
| (4) | CART (linear combination splits)          | 0.05 | 0.54 |
| (5) | Single-layer perceptron                   |      | 0.67 |
| (6) | Multi-layer perceptron (88 hidden units)  |      | 0.49 |
| (7) | Gaussian node network (528 hidden units)  |      | 0.45 |
| (8) | Nearest neighbor                          |      | 0.44 |
| (9) | FDA/BRUTO                                 | 0.06 | 0.44 |
|     | Softmax                                   | 0.11 | 0.50 |
| (10) | FDA/MARS (degree = 1)                    | 0.09 | 0.45 |
|      | Best reduced dimension (=2)              | 0.18 | 0.42 |
|      | Softmax                                  | 0.14 | 0.48 |
| (11) | FDA/MARS (degree = 2)                    | 0.02 | 0.42 |
|      | Best reduced dimension (=6)              | 0.13 | 0.39 |
|      | Softmax                                  | 0.10 | 0.50 |
**TABLE 12.3** Vowel recognition data performance results. The results for neural
networks are the best among a much larger set, taken from a neural network
archive. The notation FDA/BRUTO refers to the regression method used with
FDA.

### 12.5.1 Computing the FDA Estimates

The computations for the FDA coordinates can be simplified in many im-
portant cases, in particular when the nonparametric regression procedure
can be represented as a linear operator. We will denote this operator by
S λ ; that is, ŷ = S λ y, where y is the vector of responses and ŷ the vector
of fits. Additive splines have this property, if the smoothing parameters are
fixed, as does MARS once the basis functions are selected. The subscript λ
denotes the entire set of smoothing parameters. In this case optimal scoring
is equivalent to a canonical correlation problem, and the solution can be
computed by a single eigen-decomposition. This is pursued in Exercise 12.6,
and the resulting algorithm is presented here.

We create an N × K indicator response matrix Y from the responses g i ,
such that y ik = 1 if g i = k, otherwise y ik = 0. For a five-class problem Y
might look like the following:

$$\begin{matrix}
 & \begin{matrix}C\_1&C\_2&C\_3&C\_4&C\_5\end{matrix}
\\\\ \begin{matrix}g\_1=2\\\\g\_2=1\\\\g\_3=1\\\\g\_4=5\\\\g\_5=4\\\\\vdots\\\\g\_N=3\end{matrix}
& \begin{pmatrix}
  0 & 1 & 0 & 0 & 0
  \\\\ 1 & 0 & 0 & 0 & 0
  \\\\ 1 & 0 & 0 & 0 & 0
  \\\\ 0 & 0 & 0 & 0 & 1
  \\\\ 0 & 0 & 0 & 1 & 0
  \\\\ \vdots & \vdots & \vdots & \vdots & \vdots
  \\\\ 0 & 0 & 1 & 0 & 0
\end{pmatrix}
\end{matrix}$$

Here are the computational steps:
1. Multivariate nonparametric regression. Fit a multiresponse, adaptive
nonparametric regression of Y on X, giving fitted values Ŷ. Let S λ
be the linear operator that fits the final chosen model, and η ∗ (x) be
the vector of fitted regression functions.
2. Optimal scores. Compute the eigen-decomposition of Y T Ŷ = Y T S λ Y,
where the eigenvectors Θ are normalized: Θ T D π Θ = I. Here D π =
Y T Y/N is a diagonal matrix of the estimated class prior
probabilities.
3. Update the model from step 1 using the optimal scores: η(x) = Θ T η ∗ (x).

The first of the K functions in η(x) is the constant function— a trivial
solution; the remaining K −1 functions are the discriminant functions. The
constant function, along with the normalization, causes all the remaining
functions to be centered.

Again S λ can correspond to any regression method. When S λ = H X , the
linear regression projection operator, then FDA is linear discriminant anal-
ysis. The software that we reference in the Computational Considerations
section on page 455 makes good use of this modularity; the fda function
has a method= argument that allows one to supply any regression function,
as long as it follows some natural conventions. The regression functions
we provide allow for polynomial regression, adaptive additive models and
MARS. They all efficiently handle multiple responses, so step (1) is a single
call to a regression routine. The eigen-decomposition in step (2) simulta-
neously computes all the optimal scoring functions.

In Section 4.2 we discussed the pitfalls of using linear regression on an
indicator response matrix as a method for classification. In particular, se-
vere masking can occur with three or more classes. FDA uses the fits from
such a regression in step (1), but then transforms them further to produce
useful discriminant functions that are devoid of these pitfalls. Exercise 12.9
takes another view of this phenomenon.
